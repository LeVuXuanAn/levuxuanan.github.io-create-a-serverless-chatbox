[
{
	"uri": "http://Handoo464.github.io/7-lgme/7.6/7.6.1/",
	"title": "Add an inverted index",
	"tags": [],
	"description": "",
	"content": "In this step, you add an inverted index to the table. An inverted index is created like any other global secondary index (GSI).\nIn the code you downloaded, a add_inverted_index.py script is in the scripts/ directory. This Python script adds an inverted index to your table.\nimport boto3 dynamodb = boto3.client(\u0026#39;dynamodb\u0026#39;) try: dynamodb.update_table( TableName=\u0026#39;battle-royale\u0026#39;, AttributeDefinitions=[ { \u0026#34;AttributeName\u0026#34;: \u0026#34;PK\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34; }, { \u0026#34;AttributeName\u0026#34;: \u0026#34;SK\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34; } ], GlobalSecondaryIndexUpdates=[ { \u0026#34;Create\u0026#34;: { \u0026#34;IndexName\u0026#34;: \u0026#34;InvertedIndex\u0026#34;, \u0026#34;KeySchema\u0026#34;: [ { \u0026#34;AttributeName\u0026#34;: \u0026#34;SK\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;HASH\u0026#34; }, { \u0026#34;AttributeName\u0026#34;: \u0026#34;PK\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;RANGE\u0026#34; } ], \u0026#34;Projection\u0026#34;: { \u0026#34;ProjectionType\u0026#34;: \u0026#34;ALL\u0026#34; }, \u0026#34;ProvisionedThroughput\u0026#34;: { \u0026#34;ReadCapacityUnits\u0026#34;: 1, \u0026#34;WriteCapacityUnits\u0026#34;: 1 } } } ], ) print(\u0026#34;Table \u0026#39;battle-royale\u0026#39; updated successfully.\u0026#34;) except Exception as e: print(\u0026#34;Could not update table. Error:\u0026#34;) print(e) Edit scripts/add_inverted_index.py, set both ReadCapacityUnits and WriteCapacityUnits to 100 for InvertedIndex.\nIn this script, you call an update_table() method on the DynamoDB client. In the method, you pass details about the secondary index you want to create, including the key schema for the index, the provisioned throughput, and the attributes to project into the index.\nYou can choose to run either the add_inverted_index.py python script or the AWS CLI command below. Both are provided to show different methods of interacting with DynamoDB.\nRun the script by typing the following command in your terminal:\npython scripts/add_inverted_index.py Your terminal will display output that your index was created successfully.\nTable \u0026#39;battle-royale\u0026#39; updated successfully. Alternatively, you can create the InvertedIndex GSI by running the AWS CLI command below:\naws dynamodb update-table \\ --table-name battle-royale \\ --attribute-definitions AttributeName=PK,AttributeType=S AttributeName=SK,AttributeType=S \\ --global-secondary-index-updates \\ \u0026#34;[ { \\\u0026#34;Create\\\u0026#34;: { \\\u0026#34;IndexName\\\u0026#34;: \\\u0026#34;InvertedIndex\\\u0026#34;, \\\u0026#34;KeySchema\\\u0026#34;: [ { \\\u0026#34;AttributeName\\\u0026#34;: \\\u0026#34;SK\\\u0026#34;, \\\u0026#34;KeyType\\\u0026#34;: \\\u0026#34;HASH\\\u0026#34; }, { \\\u0026#34;AttributeName\\\u0026#34;: \\\u0026#34;PK\\\u0026#34;, \\\u0026#34;KeyType\\\u0026#34;: \\\u0026#34;RANGE\\\u0026#34; } ], \\\u0026#34;Projection\\\u0026#34;: { \\\u0026#34;ProjectionType\\\u0026#34;: \\\u0026#34;ALL\\\u0026#34; }, \\\u0026#34;ProvisionedThroughput\\\u0026#34;: { \\\u0026#34;ReadCapacityUnits\\\u0026#34;: 100, \\\u0026#34;WriteCapacityUnits\\\u0026#34;: 100 } } } ]\u0026#34; If you chose to run the AWS CLI command, the output will contain a full description of the battle-royale table including existing and newly creating indexes. You will notice the IndexStatus for the index InvertedIndex will show as CREATING.\nIt will take a few minutes for the new secondary index to get populated. You need to wait until the secondary index is active.\nYou can find out the current status of the table and its indexes by either way:\nChecking under Services, Database, DynamoDB in the AWS console.\nRunning the command below in the Cloud9 Terminal:\naws dynamodb describe-table --table-name battle-royale --query \u0026#34;Table.GlobalSecondaryIndexes[].IndexStatus\u0026#34; You also can script the command to run every 5 seconds using watch.\n# Watch checks every 5 seconds by default watch -n 5 \u0026#34;aws dynamodb describe-table --table-name battle-royale --query \\\u0026#34;Table.GlobalSecondaryIndexes[].IndexStatus\\\u0026#34;\u0026#34; Press Ctrl + C to end watch after the global secondary index has been created.\n"
},
{
	"uri": "http://Handoo464.github.io/7-lgme/7.5/7.5.1/",
	"title": "Add users to a game",
	"tags": [],
	"description": "",
	"content": "The first access pattern you address in this module is adding new users to a game.\nWhen adding a new user to a game, you need to:\nConfirm that there are not already 50 players in the game (each game can have a maximum of 50 players). Confirm that the user is not already in the game. Create a new UserGameMapping entity to add the user to the game. Increment the people attribute on the Game entity to track how many players are in the game. Note that accomplishing all of these things requires write actions across the existing Game entity and the new UserGameMapping entity as well as conditional logic for each of the entities. This is the kind of operation that is a perfect fit for DynamoDB transactions because you need to work on multiple entities in the same request, and you want the entire request to succeed or fail together.\nIn the code you downloaded, a join_game.py script is in the scripts/ directory. The function in that script uses a DynamoDB transaction to add a user to a game.\nimport boto3 dynamodb = boto3.client(\u0026#39;dynamodb\u0026#39;) GAME_ID = \u0026#34;c6f38a6a-d1c5-4bdf-8468-24692ccc4646\u0026#34; USERNAME = \u0026#39;vlopez\u0026#39; def join_game_for_user(game_id, username): try: resp = dynamodb.transact_write_items( TransactItems=[ { \u0026#34;Put\u0026#34;: { \u0026#34;TableName\u0026#34;: \u0026#34;battle-royale\u0026#34;, \u0026#34;Item\u0026#34;: { \u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: f\u0026#34;GAME#{game_id}\u0026#34; }, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: f\u0026#34;USER#{username}\u0026#34; }, \u0026#34;game_id\u0026#34;: {\u0026#34;S\u0026#34;: game_id }, \u0026#34;username\u0026#34;: {\u0026#34;S\u0026#34;: username } }, \u0026#34;ConditionExpression\u0026#34;: \u0026#34;attribute_not_exists(SK)\u0026#34;, \u0026#34;ReturnValuesOnConditionCheckFailure\u0026#34;: \u0026#34;ALL_OLD\u0026#34; }, }, { \u0026#34;Update\u0026#34;: { \u0026#34;TableName\u0026#34;: \u0026#34;battle-royale\u0026#34;, \u0026#34;Key\u0026#34;: { \u0026#34;PK\u0026#34;: { \u0026#34;S\u0026#34;: f\u0026#34;GAME#{game_id}\u0026#34; }, \u0026#34;SK\u0026#34;: { \u0026#34;S\u0026#34;: f\u0026#34;#METADATA#{game_id}\u0026#34; }, }, \u0026#34;UpdateExpression\u0026#34;: \u0026#34;SET people = people + :p\u0026#34;, \u0026#34;ConditionExpression\u0026#34;: \u0026#34;people \u0026lt; :limit\u0026#34;, \u0026#34;ExpressionAttributeValues\u0026#34;: { \u0026#34;:p\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;1\u0026#34; }, \u0026#34;:limit\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;50\u0026#34; } }, \u0026#34;ReturnValuesOnConditionCheckFailure\u0026#34;: \u0026#34;ALL_OLD\u0026#34; } } ] ) print(f\u0026#34;Added user: {username} to game: {game_id}\u0026#34;) return True except Exception as e: print(\u0026#34;Could not add user to game\u0026#34;) join_game_for_user(GAME_ID, USERNAME) In this script’s join_game_for_user function, the transact_write_items() method performs a write transaction. This transaction has two operations.\nIn the transaction’s first operation, you use a Put operation to insert a new UserGameMapping entity. As part of that operation, you specify a condition that the SK attribute should not exist for this entity. This ensures that an entity with this PK and SK doesn’t already exist. If such an entity did already exist, that would mean this user already joined the game.\nThe second operation is an Update operation on the Game entity to increment the people attribute by one. As part of this operation, you add a conditional check that the current value of people is not greater than 50. As soon as 50 people join a game, the game is full and ready to begin.\nBefore we add vlopez to the game, we can verify the current number of users already in the game by querying the sparse GSI we made. In the AWS DynamoDB console choose Explore items on the left and filter for the table named Battle Royale. Choose Query and then select the GSI named OpenGamesIndex from the dropdown. Specify Urban Underground as the value for the map (Partition Key) and click the orange Run button. You should see a single item returned with a vale of 49 for the people attribute.\nYou can choose to run either the join_game.py python script or the AWS CLI command below. Both are provided to show different methods of interacting with DynamoDB.\nRun this script with the following command in your terminal:\npython scripts/join_game.py The output in your terminal should indicate that the user was added to the game.\nAdded user: vlopez to game: c6f38a6a-d1c5-4bdf-8468-24692ccc4646 You can return to the DynamoDB console and click Run again to query the GSI and you will see that the people attribute now shows 50\nNote that if you try to run the script again, the function fails. User vlopez has been added to the game already, so trying to add the user again does not satisfy the conditions you specified.\nAlternatively, you can also submit transactions via the AWS CLI.\nRun the following command to add user ebarton to a game using the Juicy Jungle map:\naws dynamodb transact-write-items \\ --transact-items \\ \u0026#34;[ { \\\u0026#34;Put\\\u0026#34;: { \\\u0026#34;TableName\\\u0026#34;: \\\u0026#34;battle-royale\\\u0026#34;, \\\u0026#34;Item\\\u0026#34;: { \\\u0026#34;PK\\\u0026#34;: {\\\u0026#34;S\\\u0026#34;: \\\u0026#34;GAME#248dd9ef-6b17-42f0-9567-2cbd3dd63174\\\u0026#34; }, \\\u0026#34;SK\\\u0026#34;: {\\\u0026#34;S\\\u0026#34;: \\\u0026#34;USER#ebarton\\\u0026#34; }, \\\u0026#34;game_id\\\u0026#34;: {\\\u0026#34;S\\\u0026#34;: \\\u0026#34;248dd9ef-6b17-42f0-9567-2cbd3dd63174\\\u0026#34; }, \\\u0026#34;username\\\u0026#34;: {\\\u0026#34;S\\\u0026#34;: \\\u0026#34;ebarton\\\u0026#34; } }, \\\u0026#34;ConditionExpression\\\u0026#34;: \\\u0026#34;attribute_not_exists(SK)\\\u0026#34;, \\\u0026#34;ReturnValuesOnConditionCheckFailure\\\u0026#34;: \\\u0026#34;ALL_OLD\\\u0026#34; } }, { \\\u0026#34;Update\\\u0026#34;: { \\\u0026#34;TableName\\\u0026#34;: \\\u0026#34;battle-royale\\\u0026#34;, \\\u0026#34;Key\\\u0026#34;: { \\\u0026#34;PK\\\u0026#34;: { \\\u0026#34;S\\\u0026#34;: \\\u0026#34;GAME#248dd9ef-6b17-42f0-9567-2cbd3dd63174\\\u0026#34; }, \\\u0026#34;SK\\\u0026#34;: { \\\u0026#34;S\\\u0026#34;: \\\u0026#34;#METADATA#248dd9ef-6b17-42f0-9567-2cbd3dd63174\\\u0026#34; } }, \\\u0026#34;UpdateExpression\\\u0026#34;: \\\u0026#34;SET people = people + :p\\\u0026#34;, \\\u0026#34;ConditionExpression\\\u0026#34;: \\\u0026#34;people \u0026lt; :limit\\\u0026#34;, \\\u0026#34;ExpressionAttributeValues\\\u0026#34;: { \\\u0026#34;:p\\\u0026#34;: { \\\u0026#34;N\\\u0026#34;: \\\u0026#34;1\\\u0026#34; }, \\\u0026#34;:limit\\\u0026#34;: { \\\u0026#34;N\\\u0026#34;: \\\u0026#34;50\\\u0026#34; } }, \\\u0026#34;ReturnValuesOnConditionCheckFailure\\\u0026#34;: \\\u0026#34;ALL_OLD\\\u0026#34; } } ]\u0026#34; The addition of DynamoDB transactions greatly simplifies the workflow around complex operations like these. Without transactions, this would have required multiple API calls with complex conditions and manual rollbacks in the event of conflicts. Now, you can implement such complex operations with fewer than 50 lines of code.\n"
},
{
	"uri": "http://Handoo464.github.io/",
	"title": "Amazon DynamoDB Immersion Day",
	"tags": [],
	"description": "",
	"content": "Amazon DynamoDB Immersion Day Welcome to the AWS Workshop and Lab Content Portal for Amazon DynamoDB, a key-value and document database that delivers single-digit millisecond performance at any scale. Here you will find a collection of workshops and hands-on content aimed at helping you gain an understanding of DynamoDB features and NoSQL data modeling best practices.\nThe 200-level hands on labs (LHOL) include exercises designed to familarize you with DynamoDB using the CLI and the AWS Management Console. This site also includes a workshop (LADV) that is a collection of easy-to-follow instructions, scripts, and tutorial data. In addition the site includes a collection of data model design challenge scenarios (LDC) to help you understand the decisions and tradeoffs made while building efficient data models. If you\u0026rsquo;re already comfortable with these topics and you would like to learn more about DynamoDB global tables, the site includes a multi-region workshop with a fun video-streaming use case (LMR).\nPrior expertise with AWS and NoSQL databases is beneficial but not required to complete this workshop. If you\u0026rsquo;re brand new to DynamoDB with no experience, you may want to begin with Hands-on Labs for Amazon DynamoDB. If you want to learn the design patterns for DynamoDB, check out Advanced Design Patterns for DynamoDB and the Design Challenges scenarios.\nLooking for a larger challenge? The DynamoDB Immersion Day has a series of workshops designed to cover advanced topics. If you want to dig deep into streaming aggregations with AWS Lambda and DynamoDB Streams, consider LEDA. Or if you want an easier introduction CDC you can consider LCDC. Do you want to integrate Generative AI to create a context-aware reasoning application? If so consider LBED, a lab that takes a product catalog from DynamoDB and contiously indexes it into OpenSearch Service for natural language queries supported by Amazon Bedrock.\nDive into the content:\nLHOL: Hands-on Labs for Amazon DynamoDB LBED: Generative AI with DynamoDB zero-ETL to OpenSearch integration and Amazon Bedrock LADV: Advanced Design Patterns for Amazon DynamoDB LCDC: Change Data Capture for Amazon DynamoDB LMR: Build and Deploy a Global Serverless Application with Amazon DynamoDB LEDA: Build a Serverless Event Driven Architecture with DynamoDB LGME: Modeling Game Player Data with Amazon DynamoDB LDC: Design Challenges "
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.4/1.4.1/",
	"title": "AWS Backup Recap",
	"tags": [],
	"description": "",
	"content": "AWS Backup is designed to help you to centralize and automate data protection across AWS services. AWS Backup offers a cost-effective, fully managed, policy-based service that further simplifies data protection at scale. AWS Backup enables you to centrally deploy backup policies to configure, manage, and govern your backup activity across your organization’s AWS accounts and resources, including Amazon EC2 instances, Amazon EBS volumes, Amazon RDS databases, Amazon DynamoDB tables, Amazon EFS , Amazon FSx for Lustre , Amazon FSx for Windows File Server , and AWS Storage Gateway volumes .\nLet’s understand some AWS Backup terminology:\nBackup vault: a container that you organize your backups in.\nBackup plan: a policy expression that defines when and how you want to back up your AWS resources. The backup plan is attached to a backup vault.\nResource assignment: defines which resources should be backed up. You can select resources by tags or by resource ARN.\nRecovery point: a snapshot/backup of a resource backed up by AWS Backup. Each recovery point can be restored with AWS Backup.\n"
},
{
	"uri": "http://Handoo464.github.io/7-lgme/7.2/7.2.1/",
	"title": "Best Practices",
	"tags": [],
	"description": "",
	"content": "Use the following best practices when modeling data with DynamoDB Focus on access patterns When doing any kind of data modeling, you will start with an entity-relationship diagram that describes the different objects (or entities) in your application and how they are connected (or the relationships between your entities).\nIn relational databases, you will put your entities directly into tables and specify relationships using foreign keys . After you have defined your data tables, a relational database provides a flexible query language to return data in the shape you need.\nIn DynamoDB, you think about access patterns before modeling your table. NoSQL databases are focused on speed, not flexibility. You first ask how you will access your data, and then model your data in the shape it will be accessed.\nBefore designing your DynamoDB table, document every need you have for reading and writing data in your application. Be thorough and think about all the flows in your application because you are going to optimize your table for your access patterns.\nOptimize for the number of requests to DynamoDB After you have documented your application’s access pattern needs, you are ready to design your table. You should design your table to minimize the number of requests to DynamoDB for each access pattern. Ideally, each access pattern should require only a single request to DynamoDB, so you want to reduce the number of round trips from the application to the table.\nTo optimize for the number of requests to DynamoDB, you need to understand some core concepts: Primary Keys Secondary Indexes Transactions Don’t fake a relational model People new to DynamoDB often try to implement a relational model on top of nonrelational DynamoDB. If you try to do this, you will lose most of the benefits of DynamoDB.\nThe most common anti-patterns (ineffective responses to recurring problems) that people try with DynamoDB are: Normalization\nIn a relational database, you normalize your data to reduce data redundancy and storage space, and then use joins to combine multiple different tables. However, joins at scale are slow and expensive. DynamoDB does not allow for joins because they slow down as your table grows.\nOne entity type per table\nYour DynamoDB table will often include different types of data in a single table. In the example, you have User, Game, and UserGameMapping entities in a single table. In a relational database, this would be modeled as three different tables.\nToo many secondary indexes\nPeople often try to create a secondary index for each additional access pattern they need. DynamoDB is schemaless, and this applies to your indexes, too. Use the flexibility in your attributes to reuse a single secondary index across multiple data types in your table. This is called index overloading .\nIn the next step, we will build the entity-relationship diagram and map out the access patterns up front. These should always be your first steps when using DynamoDB. Then, in the modules that follow, you implement these access patterns in the table design.\n"
},
{
	"uri": "http://Handoo464.github.io/2-lbed/2.3/2.3.1/",
	"title": "Configure Integrations",
	"tags": [],
	"description": "",
	"content": "In this section you\u0026rsquo;ll configure ML and Pipeline connectors in OpenSearch Service. These configurations are set up by a series of POST and PUT requests that are authenticated with AWS Signature Version 4 (sig-v4). Sigv4 is the standard authentication mechanism used by AWS services. While in most cases an SDK abstracts away sig-v4 but in this case we will be building the requests ourselves with curl.\nBuilding a sig-v4 signed request requires a session token, access key, and secret access key. You\u0026rsquo;ll first retrieve these from your Cloud9 Instance metadata with the provided \u0026ldquo;credentials.sh\u0026rdquo; script which exports required values to environmental variables. In the following steps, you\u0026rsquo;ll also export other values to environmental variables to allow for easy substitution into listed commands.\nEdit the credentials.sh You can make edits on Cloud9/Session Manager/VSCode.\nOn VSCode, you need to log in using the command sudo su, then execute the command vim credentials.sh with the following content:\n```bash # Lấy token từ metadata service TOKEN=$(curl -s -X PUT \u0026quot;http://169.254.169.254/latest/api/token\u0026quot; -H \u0026quot;X-aws-ec2-metadata-token-ttl-seconds: 21600\u0026quot;) # Lấy IAM role INSTANCE_ROLE=$(curl -s -H \u0026quot;X-aws-ec2-metadata-token: $TOKEN\u0026quot; http://169.254.169.254/latest/meta-data/iam/security-credentials/) # Lấy thông tin chi tiết về IAM role ROLE_DETAILS=$(curl -s -H \u0026quot;X-aws-ec2-metadata-token: $TOKEN\u0026quot; http://169.254.169.254/latest/meta-data/iam/security-credentials/${INSTANCE_ROLE}) # Trích xuất thông tin từ JSON AccessKeyId=$(echo $ROLE_DETAILS | jq -r '.AccessKeyId') SecretAccessKey=$(echo $ROLE_DETAILS | jq -r '.SecretAccessKey') Token=$(echo $ROLE_DETAILS | jq -r '.Token') Expiration=$(echo $ROLE_DETAILS | jq -r '.Expiration') Region=$(curl -s -H \u0026quot;X-aws-ec2-metadata-token: $TOKEN\u0026quot; http://169.254.169.254/latest/meta-data/placement/region) Role=$(aws sts get-caller-identity | jq -r '.Arn | sub(\u0026quot;sts\u0026quot;;\u0026quot;iam\u0026quot;) | sub(\u0026quot;assumed-role\u0026quot;;\u0026quot;role\u0026quot;) | sub(\u0026quot;/i-[a-zA-Z0-9]+$\u0026quot;;\u0026quot;\u0026quot;)') # Xuất các biến môi trường export METADATA_AWS_ACCESS_KEY_ID=${AccessKeyId} export METADATA_AWS_SECRET_ACCESS_KEY=${SecretAccessKey} export METADATA_AWS_SESSION_TOKEN=${Token} export METADATA_AWS_REGION=${Region} export METADATA_AWS_ROLE=${Role} export METADATA_AWS_EXPIRATION=${Expiration} # Hiển thị thông tin echo \u0026quot;METADATA_AWS_ACCESS_KEY_ID: $AccessKeyId\u0026quot; echo \u0026quot;METADATA_AWS_SECRET_ACCESS_KEY: $SecretAccessKey\u0026quot; echo \u0026quot;METADATA_AWS_SESSION_TOKEN: $Token\u0026quot; echo \u0026quot;METADATA_AWS_REGION: $Region\u0026quot; echo \u0026quot;METADATA_AWS_ROLE: $Role\u0026quot; echo \u0026quot;METADATA_AWS_EXPIRATION: $Expiration\u0026quot; ``` Run the credentials.sh script to retrieve and export credentials. These credentials will be used to sign API requests to the OpenSearch cluster. Note the leading \u0026ldquo;.\u0026rdquo; before \u0026ldquo;./credentials.sh\u0026rdquo;, this must be included to ensure that the exported credentials are available in the currently running shell.\n. ./credentials.sh Next, export an environmental variable with the OpenSearch endpoint URL. This URL is listed in the CloudFormation Stack Outputs tab as \u0026ldquo;OSDomainEndpoint\u0026rdquo;. This variable will be used in subsequent commands.\nexport OPENSEARCH_ENDPOINT=\u0026#34;https://search-ddb-os-xxxx-xxxxxxxxxxxxx.us-west-2.es.amazonaws.com\u0026#34; Execute the following curl command to create the OpenSearch ML model connector.\ncurl --request POST \\ ${OPENSEARCH_ENDPOINT}\u0026#39;/_plugins/_ml/connectors/_create\u0026#39; \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --header \u0026#39;Accept: application/json\u0026#39; \\ --header \u0026#34;x-amz-security-token: ${METADATA_AWS_SESSION_TOKEN}\u0026#34; \\ --aws-sigv4 aws:amz:${METADATA_AWS_REGION}:es \\ --user \u0026#34;${METADATA_AWS_ACCESS_KEY_ID}:${METADATA_AWS_SECRET_ACCESS_KEY}\u0026#34; \\ --data-raw \u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;Amazon Bedrock Connector: embedding\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;The connector to bedrock Titan embedding model\u0026#34;, \u0026#34;version\u0026#34;: 1, \u0026#34;protocol\u0026#34;: \u0026#34;aws_sigv4\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;region\u0026#34;: \u0026#34;\u0026#39;${METADATA_AWS_REGION}\u0026#39;\u0026#34;, \u0026#34;service_name\u0026#34;: \u0026#34;bedrock\u0026#34; }, \u0026#34;credential\u0026#34;: { \u0026#34;roleArn\u0026#34;: \u0026#34;\u0026#39;${METADATA_AWS_ROLE}\u0026#39;\u0026#34; }, \u0026#34;actions\u0026#34;: [ { \u0026#34;action_type\u0026#34;: \u0026#34;predict\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;POST\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://bedrock-runtime.\u0026#39;${METADATA_AWS_REGION}\u0026#39;.amazonaws.com/model/amazon.titan-embed-text-v1/invoke\u0026#34;, \u0026#34;headers\u0026#34;: { \u0026#34;content-type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;x-amz-content-sha256\u0026#34;: \u0026#34;required\u0026#34; }, \u0026#34;request_body\u0026#34;: \u0026#34;{ \\\u0026#34;inputText\\\u0026#34;: \\\u0026#34;${parameters.inputText}\\\u0026#34; }\u0026#34;, \u0026#34;pre_process_function\u0026#34;: \u0026#34;\\n StringBuilder builder = new StringBuilder();\\n builder.append(\\\u0026#34;\\\\\\\u0026#34;\\\u0026#34;);\\n String first = params.text_docs[0];\\n builder.append(first);\\n builder.append(\\\u0026#34;\\\\\\\u0026#34;\\\u0026#34;);\\n def parameters = \\\u0026#34;{\\\u0026#34; +\\\u0026#34;\\\\\\\u0026#34;inputText\\\\\\\u0026#34;:\\\u0026#34; + builder + \\\u0026#34;}\\\u0026#34;;\\n return \\\u0026#34;{\\\u0026#34; +\\\u0026#34;\\\\\\\u0026#34;parameters\\\\\\\u0026#34;:\\\u0026#34; + parameters + \\\u0026#34;}\\\u0026#34;;\u0026#34;, \u0026#34;post_process_function\u0026#34;: \u0026#34;\\n def name = \\\u0026#34;sentence_embedding\\\u0026#34;;\\n def dataType = \\\u0026#34;FLOAT32\\\u0026#34;;\\n if (params.embedding == null || params.embedding.length == 0) {\\n return params.message;\\n }\\n def shape = [params.embedding.length];\\n def json = \\\u0026#34;{\\\u0026#34; +\\n \\\u0026#34;\\\\\\\u0026#34;name\\\\\\\u0026#34;:\\\\\\\u0026#34;\\\u0026#34; + name + \\\u0026#34;\\\\\\\u0026#34;,\\\u0026#34; +\\n \\\u0026#34;\\\\\\\u0026#34;data_type\\\\\\\u0026#34;:\\\\\\\u0026#34;\\\u0026#34; + dataType + \\\u0026#34;\\\\\\\u0026#34;,\\\u0026#34; +\\n \\\u0026#34;\\\\\\\u0026#34;shape\\\\\\\u0026#34;:\\\u0026#34; + shape + \\\u0026#34;,\\\u0026#34; +\\n \\\u0026#34;\\\\\\\u0026#34;data\\\\\\\u0026#34;:\\\u0026#34; + params.embedding +\\n \\\u0026#34;}\\\u0026#34;;\\n return json;\\n \u0026#34; } ] }\u0026#39; Note the \u0026ldquo;connector_id\u0026rdquo; returned in the previous command. Export it to an environmental variable for convenient substitution in future commands.\nexport CONNECTOR_ID=\u0026#39;xxxxxxxxxxxxxx\u0026#39; Run the next curl command to create the model group.\ncurl --request POST \\ ${OPENSEARCH_ENDPOINT}\u0026#39;/_plugins/_ml/model_groups/_register\u0026#39; \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --header \u0026#39;Accept: application/json\u0026#39; \\ --header \u0026#34;x-amz-security-token: ${METADATA_AWS_SESSION_TOKEN}\u0026#34; \\ --aws-sigv4 aws:amz:${METADATA_AWS_REGION}:es \\ --user \u0026#34;${METADATA_AWS_ACCESS_KEY_ID}:${METADATA_AWS_SECRET_ACCESS_KEY}\u0026#34; \\ --data-raw \u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;remote_model_group\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;This is an example description\u0026#34; }\u0026#39; Note the \u0026ldquo;model_group_id\u0026rdquo; returned in the previous command. Export it to an environmental variable for later substitution.\n1 export MODEL_GROUP_ID=\u0026#39;xxxxxxxxxxxxx\u0026#39; The next curl command registers the connector with the model group.\ncurl --request POST \\ ${OPENSEARCH_ENDPOINT}\u0026#39;/_plugins/_ml/models/_register\u0026#39; \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --header \u0026#39;Accept: application/json\u0026#39; \\ --header \u0026#34;x-amz-security-token: ${METADATA_AWS_SESSION_TOKEN}\u0026#34; \\ --aws-sigv4 aws:amz:${METADATA_AWS_REGION}:es \\ --user \u0026#34;${METADATA_AWS_ACCESS_KEY_ID}:${METADATA_AWS_SECRET_ACCESS_KEY}\u0026#34; \\ --data-raw \u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;Bedrock embedding model\u0026#34;, \u0026#34;function_name\u0026#34;: \u0026#34;remote\u0026#34;, \u0026#34;model_group_id\u0026#34;: \u0026#34;\u0026#39;${MODEL_GROUP_ID}\u0026#39;\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;embedding model\u0026#34;, \u0026#34;connector_id\u0026#34;: \u0026#34;\u0026#39;${CONNECTOR_ID}\u0026#39;\u0026#34; }\u0026#39; Note the \u0026ldquo;model_id\u0026rdquo; and export it.\n1 export MODEL_ID=\u0026#39;xxxxxxxxxxxxx\u0026#39; Run the following command to verify that you have successfully exported the connector, model group, and model id.\n1 echo -e \u0026#34;CONNECTOR_ID=${CONNECTOR_ID}\\nMODEL_GROUP_ID=${MODEL_GROUP_ID}\\nMODEL_ID=${MODEL_ID}\u0026#34; Next, we\u0026rsquo;ll deploy the model with the following curl.\ncurl --request POST \\ ${OPENSEARCH_ENDPOINT}\u0026#39;/_plugins/_ml/models/\u0026#39;${MODEL_ID}\u0026#39;/_deploy\u0026#39; \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --header \u0026#39;Accept: application/json\u0026#39; \\ --header \u0026#34;x-amz-security-token: ${METADATA_AWS_SESSION_TOKEN}\u0026#34; \\ --aws-sigv4 aws:amz:${METADATA_AWS_REGION}:es \\ --user \u0026#34;${METADATA_AWS_ACCESS_KEY_ID}:${METADATA_AWS_SECRET_ACCESS_KEY}\u0026#34; With the model created, OpenSearch can now use Bedrock\u0026rsquo;s Titan embedding model for processing text. An embeddings model is a type of machine learning model that transforms high-dimensional data (like text or images) into lower-dimensional vectors, known as embeddings. These vectors capture the semantic or contextual relationships between the data points in a more compact, dense representation.\nThe embeddings represent the semantic meaning of the input data, in this case product descriptions. Words with similar meanings are represented by vectors that are close to each other in the vector space. For example, the vectors for \u0026ldquo;sturdy\u0026rdquo; and \u0026ldquo;strong\u0026rdquo; would be closer to each other than to \u0026ldquo;warm\u0026rdquo;.\nNow we can test the model. If you recieve results back with a \u0026ldquo;200\u0026rdquo; status code, everything is working properly.\ncurl --request POST \\ ${OPENSEARCH_ENDPOINT}\u0026#39;/_plugins/_ml/models/\u0026#39;${MODEL_ID}\u0026#39;/_predict\u0026#39; \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --header \u0026#39;Accept: application/json\u0026#39; \\ --header \u0026#34;x-amz-security-token: ${METADATA_AWS_SESSION_TOKEN}\u0026#34; \\ --aws-sigv4 aws:amz:${METADATA_AWS_REGION}:es \\ --user \u0026#34;${METADATA_AWS_ACCESS_KEY_ID}:${METADATA_AWS_SECRET_ACCESS_KEY}\u0026#34; \\ --data-raw \u0026#39;{ \u0026#34;parameters\u0026#34;: { \u0026#34;inputText\u0026#34;: \u0026#34;What is the meaning of life?\u0026#34; } }\u0026#39; Next, we\u0026rsquo;ll create the Details table mapping pipeline.\ncurl --request PUT \\ ${OPENSEARCH_ENDPOINT}\u0026#39;/_ingest/pipeline/product-en-nlp-ingest-pipeline\u0026#39; \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --header \u0026#39;Accept: application/json\u0026#39; \\ --header \u0026#34;x-amz-security-token: ${METADATA_AWS_SESSION_TOKEN}\u0026#34; \\ --aws-sigv4 aws:amz:${METADATA_AWS_REGION}:es \\ --user \u0026#34;${METADATA_AWS_ACCESS_KEY_ID}:${METADATA_AWS_SECRET_ACCESS_KEY}\u0026#34; \\ --data-raw \u0026#39;{ \u0026#34;description\u0026#34;: \u0026#34;A text embedding pipeline\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;def combined_field = \\\u0026#34;ProductID: \\\u0026#34; + ctx.ProductID + \\\u0026#34;, Description: \\\u0026#34; + ctx.Description + \\\u0026#34;, ProductName: \\\u0026#34; + ctx.ProductName + \\\u0026#34;, Category: \\\u0026#34; + ctx.Category; ctx.combined_field = combined_field;\u0026#34; } }, { \u0026#34;text_embedding\u0026#34;: { \u0026#34;model_id\u0026#34;: \u0026#34;\u0026#39;${MODEL_ID}\u0026#39;\u0026#34;, \u0026#34;field_map\u0026#34;: { \u0026#34;combined_field\u0026#34;: \u0026#34;product_embedding\u0026#34; } } } ] }\u0026#39; Followed by the Reviews table mapping pipeline. We won\u0026rsquo;t use this in this version of the lab, but in a real system you will want to keep your embeddings indexes separate for different queries.\ncurl --request PUT \\ ${OPENSEARCH_ENDPOINT}\u0026#39;/_ingest/pipeline/product-reviews-nlp-ingest-pipeline\u0026#39; \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --header \u0026#39;Accept: application/json\u0026#39; \\ --header \u0026#34;x-amz-security-token: ${METADATA_AWS_SESSION_TOKEN}\u0026#34; \\ --aws-sigv4 aws:amz:${METADATA_AWS_REGION}:es \\ --user \u0026#34;${METADATA_AWS_ACCESS_KEY_ID}:${METADATA_AWS_SECRET_ACCESS_KEY}\u0026#34; \\ --data-raw \u0026#39;{ \u0026#34;description\u0026#34;: \u0026#34;A text embedding pipeline\u0026#34;, \u0026#34;processors\u0026#34;: [ { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;def combined_field = \\\u0026#34;ProductID: \\\u0026#34; + ctx.ProductID + \\\u0026#34;, ProductName: \\\u0026#34; + ctx.ProductName + \\\u0026#34;, Comment: \\\u0026#34; + ctx.Comment + \\\u0026#34;, Timestamp: \\\u0026#34; + ctx.Timestamp; ctx.combined_field = combined_field;\u0026#34; } }, { \u0026#34;text_embedding\u0026#34;: { \u0026#34;model_id\u0026#34;: \u0026#34;m6jIgowBXLzE-9O0CcNs\u0026#34;, \u0026#34;field_map\u0026#34;: { \u0026#34;combined_field\u0026#34;: \u0026#34;product_reviews_embedding\u0026#34; } } } ] }\u0026#39; These pipelines allow OpenSearch to preprocess and enrich data as it is written to the index by adding embeddings through the Bedrock connector.\n"
},
{
	"uri": "http://Handoo464.github.io/2-lbed/2.2/2.2.1/",
	"title": "Configure OpenSearch Service Permissions",
	"tags": [],
	"description": "",
	"content": "The OpenSearch Service Domain deployed by the CloudFormation Template uses Fine-grained access control. Fine-grained access control offers additional ways of controlling access to your data on Amazon OpenSearch Service. In order to configure integrations between OpenSearch Service, DynamoDB, and Bedrock certain OpenSearch Service permissions will need to be mapped to the IAM Role being used.\nLinks to the OpenSearch Dashboards, credentials, and necessary values are provided in the Outputs of the DynamoDBzETL CloudFormation Template. It is recommended that you leave Outputs open in one browser tab to easily refer to while following through the lab.\nIn a production environment as a best practice, you would configure roles with the least privilege required. For simplicity in this lab, we will use the \u0026ldquo;all_access\u0026rdquo; OpenSearch Service role.\nDo not continue unless the CloudFormation Template has finished deploying.\nOpen the \u0026ldquo;Outputs\u0026rdquo; tab of the stack named dynamodb-opensearch-setup in the CloudFormation Console.\nOpen the link for SecretConsoleLink in a new tab. This will take you to the AWS Secrets Manager secret which contains the login information for OpenSearch. Click on the Retrieve secret value button to see the username and password for the OpenSearch Cluster.\nReturn to the CloudFormation Console \u0026ldquo;Outputs\u0026rdquo; and open the link for OSDashboardsURL in a new tab.\nLogin to Dashboards with the username and password provided in Secrets Manager.\nWhen prompted to select your tenant, choose Global and click Confirm. Dismiss any pop ups.\nOpen the top left menu and select Security under the Management section.\nOpen the \u0026ldquo;Roles\u0026rdquo; tab, then click on the \u0026ldquo;all_access\u0026rdquo; role.\nOpen the \u0026ldquo;Mapped users\u0026rdquo; tab, then select \u0026ldquo;Manage mapping\u0026rdquo;.\nIn the \u0026ldquo;Backend roles\u0026rdquo; field, enter the Arn provided in the CloudFormation Stack Outputs. The attribute named \u0026ldquo;Role\u0026rdquo; provides the correct Arn.\nBe absolutely sure you have removed any white space characters from the start and end of the ARN to ensure you do not have permissions issues later. Click \u0026ldquo;Map\u0026rdquo;.\nVerify that the \u0026ldquo;all_access\u0026rdquo; Role now has a \u0026ldquo;Backend role\u0026rdquo; listed.\n"
},
{
	"uri": "http://Handoo464.github.io/4-lcdc/4.2/4.2.1/",
	"title": "Create The DynamoDB Tables",
	"tags": [],
	"description": "",
	"content": "In this section you create the DynamoDB tables you will use during the labs for this workshop.\nIn the commands below, the create-table AWS CLI command is used to create two new tables called Orders and OrdersHistory.\nIt will create the Orders table in provisioned capacity mode to have 5 read capacity units (RCU), 5 write capacity uints (WCU) and a partition key named id.\nIt will also create the OrdersHistory table in provisioned capacity mode to have 5 RCU, 5 WCU, a partition key named pk and a sort key named sk.\nCopy the create-table commands below and paste them into your command terminal. Execute the commands to to create two tables named Orders and OrdersHistory. aws dynamodb create-table \\ --table-name Orders \\ --attribute-definitions \\ AttributeName=id,AttributeType=S \\ --key-schema \\ AttributeName=id,KeyType=HASH \\ --provisioned-throughput \\ ReadCapacityUnits=5,WriteCapacityUnits=5 \\ --query \u0026#34;TableDescription.TableStatus\u0026#34; aws dynamodb create-table \\ --table-name OrdersHistory \\ --attribute-definitions \\ AttributeName=pk,AttributeType=S \\ AttributeName=sk,AttributeType=S \\ --key-schema \\ AttributeName=pk,KeyType=HASH \\ AttributeName=sk,KeyType=RANGE \\ --provisioned-throughput \\ ReadCapacityUnits=5,WriteCapacityUnits=5 \\ --query \u0026#34;TableDescription.TableStatus\u0026#34; Run the command below to confirm that both tables have been created.\naws dynamodb wait table-exists --table-name Orders aws dynamodb wait table-exists --table-name OrdersHistory "
},
{
	"uri": "http://Handoo464.github.io/7-lgme/7.3/7.3.1/",
	"title": "Design the primary key",
	"tags": [],
	"description": "",
	"content": "Let’s consider the different entities, as suggested in the preceding introduction. In the gaming application, you have the following entities:\nUser\nGame\nUserGameMapping\nA UserGameMapping is a record that indicates a user joined a game. There is a many-to-many relationship between User and Game.\nHaving a many-to-many mapping is usually an indication that you want to satisfy two Query patterns, and this gaming application is no exception. You have an access pattern that needs to find all users that have joined a game as well as another pattern to find all games that a user has played.\nIf your data model has multiple entities with relationships among them, you generally use a composite primary key with both partition key and sort key values. The composite primary key gives you the Query ability on the partition key to satisfy one of the query patterns you need. In the DynamoDB documentation, the partition key is also called HASH and the sort key is called RANGE.\nThe other two data entities — User and Game — don’t have a natural property for the sort key value because the access patterns on a User or Game are a key-value lookup. Because a sort key value is required, you can provide a filler value for the sort key.\nWith this in mind, let’s use the following pattern for partition key and sort key values for each entity type.\nEntity Partition Key Sort Key User USER# #METADATA# Game GAME#\u0026lt;GAME_ID\u0026gt; #METADATA#\u0026lt;GAME_ID\u0026gt; UserGameMapping GAME#\u0026lt;GAME_ID\u0026gt; USER# Let’s walk through the preceding table.\nFor the User entity, the partition key value is USER#\u0026lt;USERNAME\u0026gt;. Notice that a prefix is used to identify the entity and prevent any possible collisions across entity types.\nFor the sort key value on the User entity, a static prefix of #METADATA# followed by the USERNAME value is used. For the sort key value, it’s important that you have a value that is known, such as the USERNAME. This allows for single-item actions such as GetItem, PutItem, and DeleteItem.\nHowever, you also want a sort key value with different values across different User entities to enable even partitioning if you use this attribute as a partition key for an index. For that reason, you append the USERNAME.\nThe Game entity has a primary key design that is similar to the User entity’s design. It uses a different prefix (GAME#) and a GAME_ID instead of a USERNAME, but the principles are the same.\nFinally, the UserGameMapping uses the same partition key as the Game entity. This allows you to fetch not only the metadata for a Game but also all the users in a Game in a single query. You then use the User entity for the sort key on the UserGameMapping to identify which user has joined a specific game.\nIn the next step, you create a table with this primary key design.\n"
},
{
	"uri": "http://Handoo464.github.io/4-lcdc/4.3/4.3.1/",
	"title": "Enable DynamoDB Streams",
	"tags": [],
	"description": "",
	"content": "When enabling DynamoDB streams on a table, you can choose to record one of the following.\nKey attributes only - the key attributes of the item being updated New image - the new image of an item after it is updated Old image - the original image of an item before it was updated New and old images - the before and after image of an item following an update. Since the requirement for this scenario is to retain only the current view of orders on the Orders table, you will set the DynamoDB stream for the table to only hold the old image of items when updates are performed. The new image does not need to be written to the stream since it is not required.\nEnable DynamoDB streams on the Orders table by running the AWS CLI command below.\naws dynamodb update-table \\ --table-name Orders \\ --stream-specification \\ StreamEnabled=true,StreamViewType=OLD_IMAGE \\ --query \u0026#34;TableDescription.LatestStreamArn\u0026#34; Confirm that DynamoDB streams has been enabled using the AWS CLI commands below.\naws dynamodb describe-table \\ --table-name Orders \\ --query \u0026#34;Table.StreamSpecification.StreamEnabled\u0026#34; The output should return a boolean as shown below.\ntrue "
},
{
	"uri": "http://Handoo464.github.io/4-lcdc/4.4/4.4.1/",
	"title": "Enable Kinesis Data Streams",
	"tags": [],
	"description": "",
	"content": "Disable DynamoDB streams for the Orders table using the AWS CLI commands below.\naws dynamodb update-table \\ --table-name Orders \\ --stream-specification \\ StreamEnabled=false \\ --query \u0026#34;Table.StreamSpecification.StreamEnabled\u0026#34; Confirm that DynamoDB streams has been disabled using the AWS CLI commands below.\naws dynamodb describe-table \\ --table-name Orders \\ --query \u0026#34;Table.StreamSpecification.StreamEnabled\u0026#34; The output should return a boolean as shown below.\nnull Create a Kinesis data stream named Orders using the following command.\naws kinesis create-stream --stream-name Orders --shard-count 2 Confirm that the stream is active using the following command.\naws kinesis describe-stream \\ --stream-name Orders \\ --query \u0026#34;StreamDescription.[StreamStatus, StreamARN]\u0026#34; Sample output:\n[ \u0026#34;ACTIVE\u0026#34;, \u0026#34;arn:aws:kinesis:${REGION}:${ACCOUNT_ID}:stream/Orders\u0026#34; ] Enable Kinesis streaming for the Orders DynamoDB table using following command. Copy the ARN from the previous command into the \u0026ndash;stream-arn parameter.\naws dynamodb enable-kinesis-streaming-destination \\ --table-name Orders \\ --stream-arn arn:aws:kinesis:${REGION}:${ACCOUNT_ID}:stream/Orders Sample output:\n{ \u0026#34;TableName\u0026#34;: \u0026#34;Orders\u0026#34;, \u0026#34;StreamArn\u0026#34;: \u0026#34;arn:aws:kinesis:${REGION}:${ACCOUNT_ID}:stream/Orders\u0026#34;, \u0026#34;DestinationStatus\u0026#34;: \u0026#34;ENABLING\u0026#34;, \u0026#34;EnableKinesisStreamingConfiguration\u0026#34;: {} } Confirm that Kinesis streaming is active on the Orders table using the following command.\naws dynamodb describe-kinesis-streaming-destination \\ --table-name Orders \\ --query \u0026#34;KinesisDataStreamDestinations[0].DestinationStatus\u0026#34; The sample output will include the table name, the Kinesis data stream ARN and the streaming status.\n\u0026#34;ACTIVE\u0026#34; "
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.5/1.5.1/",
	"title": "Exercise Overview",
	"tags": [],
	"description": "",
	"content": "In this module, you will create an environment to host the MySQL database on Amazon EC2. This instance will be used to host source database and simulate on-premise side of migration architecture. All the resources to configure source infrastructure are deployed via Amazon CloudFormation template. There are two CloudFormation templates used in this exercise which will deploy following resources.\nCloudFormation MySQL Template Resources:\nOnPrem VPC: Source VPC will represent an on-premise source environment in the N. Virginia region. This VPC will host source MySQL database on Amazon EC2 Amazon EC2 MySQL Database: Amazon EC2 Amazon Linux 2 AMI with MySQL installed and running Load IMDb dataset: The template will create IMDb database on MySQL and load IMDb public dataset files into database. You can learn more about IMDb dataset inside Explore Source Model CloudFormation DMS Instance Resources:\nDMS VPC: Migration VPC on in the N. Virginia region. This VPC will host DMS replication instance. Replication Instance: DMS Replication instance that will facilitate database migration from source MySQL server on EC2 to Amazon DynamoDB "
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.1/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": "In this chapter, we\u0026rsquo;ll cover the prerequisites needed to get started with Amazon DynamoDB . You\u0026rsquo;ll create DynamoDB tables and use a AWS Cloud9 envrironment to query these tables.\nThe deployment architecture that you will be building in this lab will look like the below.\n"
},
{
	"uri": "http://Handoo464.github.io/2-lbed/2.1/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": "Background Imagine you are building an online store. Most of your access patterns fit very well with DynamoDB\u0026rsquo;s strengths. Looking up items by SKU, finding shopping cart contents, and viewing customer order history are all relatively straight forward key value queries.\nAs your application grows, though, you may want to add some additional access patterns. Search? Filtering? What about this whole \u0026ldquo;Generative AI\u0026rdquo; thing you\u0026rsquo;ve been hearing so much about. That might be an interesting differentiator for your app.\nIn this lab, we will learn how to integrate DynamoDB with Amazon OpenSearch Service to support those new access patterns.\nThe first half of these setup instructions are identitical for LADV, LHOL, LMR, LBED, and LGME - all of which use the same Cloud9 template. Only complete this section once, and only if you\u0026rsquo;re running it on your own account. If you have already launched the Cloud9 stack in a different lab, skip to the Launch the zETL CloudFormation stack section\nOnly complete this section if you are running the workshop on your own. If you are at an AWS hosted event (such as re:Invent, Immersion Day, etc), go to At an AWS hosted Event\nLaunch the Cloud9 CloudFormation stack During the course of the lab, you will create resources that will incur a cost that could approach tens of dollars per day. Ensure you delete the CloudFormation stack as soon as the lab is complete and verify all resources are deleted.\nLaunch the CloudFormation template in US West 2 to deploy the resources in your account: Optionally, download the YAML template and launch it your own way\nClick Next on the first dialog.\nIn the Parameters section, note the Timeout is set to zero. This means the Cloud9 instance will not sleep; you may want to change this manually to a value such as 60 to protect against unexpected charges if you forget to delete the stack at the end.\nLeave the WorkshopZIP parameter unchanged and click Next\nScroll to the bottom and click Next, and then review the Template and Parameters. When you are ready to create the stack, scroll to the bottom, check the box acknowledging the creation of IAM resources, and click Submit. The stack will create a Cloud9 lab instance, a role for the instance, and a role for the AWS Lambda function used later on in the lab. It will use Systems Manager to configure the Cloud9 instance.\nAfter the CloudFormation stack is CREATE_COMPLETE, continue with the next stack. Launch the zETL CloudFormation stack Do not continue unless the Cloud9 CloudFormation Template has finished deploying.\nLaunch the CloudFormation template in US West 2 to deploy the resources in your account: Optionally, download the YAML template and launch it your own way\nClick Next on the first dialog.\nIn the Parameters section, optionally change the value of OpenSearchClusterName to set a specific name for your OpenSearch cluster or leave the parameter unchanged. Click Next\nScroll to the bottom and click Next, and then review the Template and Parameters. When you are ready to create the stack, scroll to the bottom and click Submit.\nAfter the CloudFormation stack is CREATE_COMPLETE, continue onto connecting to Cloud9.\n"
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.1/3.1.1/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": " These setup instructions are identitical for LADV, LHOL, LMR, LBED, and LGME - all of which use the same Cloud9 template. Only complete this section once, and only if you\u0026rsquo;re running it on your own account.\nOnly complete this section if you are running the workshop on your own. If you are at an AWS hosted event (such as re:Invent, Immersion Day, etc), go to At an AWS hosted Event\nLaunch the CloudFormation stack During the course of the lab, you will make DynamoDB tables that will incur a cost that could approach tens or hundreds of dollars per day. Ensure you delete the DynamoDB tables using the DynamoDB console, and make sure you delete the CloudFormation stack as soon as the lab is complete.\nLaunch the CloudFormation template in US West 2 to deploy the resources in your account: Optionally, download the YAML template and launch it your own way Click Next on the first dialog.\nIn the Parameters section, note the Timeout is set to zero. This means the Cloud9 instance will not sleep; you may want to change this manually to a value such as 60 to protect against unexpected charges if you forget to delete the stack at the end.\nLeave the WorkshopZIP parameter unchanged and click Next Scroll to the bottom and click Next, and then review the Template and Parameters. When you are ready to create the stack, scroll to the bottom, check the box acknowledging the creation of IAM resources, and click Create stack. The stack will create a Cloud9 lab instance, a role for the instance, and a role for the AWS Lambda function used later on in the lab. It will use Systems Manager to configure the Cloud9 instance.\nAfter the CloudFormation stack is CREATE_COMPLETE, continue onto Step 1.\n"
},
{
	"uri": "http://Handoo464.github.io/4-lcdc/4.1/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": "You will have 2 options to start with:\nStart with EC2 Instance Start with Cloud9 "
},
{
	"uri": "http://Handoo464.github.io/5-lmr/5.1/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": " The first half of these setup instructions are identitical for LADV, LHOL, LMR, LBED, and LGME - all of which use the same Cloud9 template. Only complete this section once, and only if you\u0026rsquo;re running it on your own account. If you have already launched the Cloud9 stack in a different lab, skip to the Launch the zETL CloudFormation stack section\nOnly complete this section if you are running the workshop on your own. If you are at an AWS hosted event (such as re:Invent, Immersion Day, etc), go to At an AWS hosted Event\nLaunch the Cloud9 CloudFormation stack During the course of the lab, you will create resources that will incur a cost that could approach tens of dollars per day. Ensure you delete the CloudFormation stack as soon as the lab is complete and verify all resources are deleted.\nLaunch the CloudFormation template in US West 2 to deploy the resources in your account: Optionally, download the YAML template and launch it your own way\nClick Next on the first dialog.\nIn the Parameters section, note the Timeout is set to zero. This means the Cloud9 instance will not sleep; you may want to change this manually to a value such as 60 to protect against unexpected charges if you forget to delete the stack at the end.\nLeave the WorkshopZIP parameter unchanged and click Next\nScroll to the bottom and click Next, and then review the Template and Parameters. When you are ready to create the stack, scroll to the bottom, check the box acknowledging the creation of IAM resources, and click Submit. The stack will create a Cloud9 lab instance, a role for the instance, and a role for the AWS Lambda function used later on in the lab. It will use Systems Manager to configure the Cloud9 instance.\nAfter the CloudFormation stack is CREATE_COMPLETE, continue with the next stack. Obtain \u0026amp; Review Code In this lab, you use Python scripts to interact with the DynamoDB API. Run the following commands in your AWS Cloud9 terminal to download and unpack this lab’s code.\ncd ~/environment curl -sL https://amazon-dynamodb-labs.com/assets/battle-royale.zip -o battle-royal.zip \u0026amp;\u0026amp; unzip -oq battle-royal.zip \u0026amp;\u0026amp; rm battle-royal.zip You should see two directories in the AWS Cloud9 file explorer:\napplication: The application directory contains example code for reading and writing data in your table. This code is similar to code you would have in your real gaming application.\nscripts: The scripts directory contains administrator-level scripts, such as for creating a table, adding a secondary index, or deleting a table.\nYou are now ready to start the lab. With DynamoDB, it is important to plan your data model up front so that you have fast, consistent performance in your application. In the next module, you will learn about planning your data model.\n"
},
{
	"uri": "http://Handoo464.github.io/6-leda/6.1/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": " Only complete this section if you are running the workshop on your own. If you are at an AWS hosted event (such as re:Invent, Immersion Day, etc), go to At an AWS hosted Event\nLaunch the CloudFormation stack During the course of the lab, you will make DynamoDB tables that will incur a cost that could approach tens or hundreds of dollars per day. Ensure you delete the DynamoDB tables using the DynamoDB console, and make sure you delete the CloudFormation stack as soon as the lab is complete.\nLaunch the CloudFormation template in US West 2 to deploy the resources in your account: Optionally, download the YAML template and launch it your own way Click Next on the first dialog.\nScroll to the bottom and click Next, and then review the Template. When you are ready to create the stack, scroll to the bottom, check the box acknowledging the creation of IAM resources, and click Create stack. The stack will create DynamoDB tables, Lambda functions, Kinesis streams, and IAM roles and policies which will be used later on in the lab.\nAfter the CloudFormation stack is CREATE_COMPLETE.\n"
},
{
	"uri": "http://Handoo464.github.io/7-lgme/7.1/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": " The first half of these setup instructions are identitical for LADV, LHOL, LMR, LBED, and LGME - all of which use the same Cloud9 template. Only complete this section once, and only if you\u0026rsquo;re running it on your own account. If you have already launched the Cloud9 stack in a different lab, skip to the Launch the zETL CloudFormation stack section\nOnly complete this section if you are running the workshop on your own. If you are at an AWS hosted event (such as re:Invent, Immersion Day, etc), go to At an AWS hosted Event\nLaunch the Cloud9 CloudFormation stack During the course of the lab, you will create resources that will incur a cost that could approach tens of dollars per day. Ensure you delete the CloudFormation stack as soon as the lab is complete and verify all resources are deleted.\nLaunch the CloudFormation template in US West 2 to deploy the resources in your account: Optionally, download the YAML template and launch it your own way\nClick Next on the first dialog.\nIn the Parameters section, note the Timeout is set to zero. This means the Cloud9 instance will not sleep; you may want to change this manually to a value such as 60 to protect against unexpected charges if you forget to delete the stack at the end.\nLeave the WorkshopZIP parameter unchanged and click Next\nScroll to the bottom and click Next, and then review the Template and Parameters. When you are ready to create the stack, scroll to the bottom, check the box acknowledging the creation of IAM resources, and click Submit. The stack will create a Cloud9 lab instance, a role for the instance, and a role for the AWS Lambda function used later on in the lab. It will use Systems Manager to configure the Cloud9 instance.\nAfter the CloudFormation stack is CREATE_COMPLETE, continue with the next stack. "
},
{
	"uri": "http://Handoo464.github.io/1-lhol/",
	"title": "LHOL: Hands-on Labs for Amazon DynamoDB",
	"tags": [],
	"description": "",
	"content": "In this workshop, you will learn to create and work with Amazon DynamoDB.\nHere\u0026rsquo;s what this workshop includes:\nGetting Started Explore DynamoDB with the CLI Explore the DynamoDB Console Backups LMIG: Relational Modeling \u0026amp; Migration Target audience This workshop is designed for developers, engineers, and database administrators who are involved in designing and maintaining DynamoDB applications.\nRequirements Basic knowledge of AWS services Among other services this lab will guide you through the use of AWS Cloud9 and AWS Lambda. Basic understanding of DynamoDB If you\u0026rsquo;re not familiar with DynamoDB or are not participating in this lab as part of an AWS event, consider reviewing the documentation on \u0026ldquo;What is Amazon DynamoDB?\u0026rdquo; Recommended study before taking the lab If you\u0026rsquo;re not part of an AWS event and you haven\u0026rsquo;t recently reviewed DynamoDB design concepts, we suggest you watch this video on Advanced Design Patterns for DynamoDB, which is about an hour in duration.\n"
},
{
	"uri": "http://Handoo464.github.io/7-lgme/7.4/7.4.1/",
	"title": "Model a sparse GSI",
	"tags": [],
	"description": "",
	"content": "Secondary indexes are crucial data modeling tools in DynamoDB. They allow you to reshape your data to allow for alternate query patterns. To create a secondary index, you specify the primary key of the index, just like when you previously created a table. Note that the primary key for a global secondary index does not have to be unique for each item. DynamoDB then copies items into the index based on the attributes specified, and you can query it just like you do the table.\nUsing sparse secondary indexes is an advanced strategy in DynamoDB. With secondary indexes, DynamoDB copies items from the original table only if they have the elements of the primary key in the secondary index. Items that don’t have the primary key elements are not copied, which is why these secondary indexes are called “sparse.”\nLet’s see how this plays out for us. You might remember that you have two access patterns for finding open games:\nFind open games (Read) Find open games by map (Read) You can create a global secondary index (GSI) using a composite primary key where the partition key is the map attribute for the game and the sort key is the open_timestamp attribute for the game, indicating the time the game was opened.\nThe important part is that when a game becomes full, the open_timestamp attribute is deleted. When the attribute is deleted, the filled game is removed from the GSI because it doesn’t have a value for the sort key attribute. This is what keeps the index sparse: It includes only open games that have the open_timestamp attribute.\n"
},
{
	"uri": "http://Handoo464.github.io/2-lbed/2.1/2.1.1/",
	"title": "Obtain &amp; Review Code",
	"tags": [],
	"description": "",
	"content": " You can use cloud9 or session manager.\nIn this lab, you use Bash and Python scripts to interact with AWS services. Run the following commands in your terminal to download and unpack this lab’s code.\ncd ~/environment curl -sL https://amazon-dynamodb-labs.com/assets/OpenSearchPipeline.zip -o OpenSearchPipeline.zip \u0026amp;\u0026amp; unzip -oq OpenSearchPipeline.zip \u0026amp;\u0026amp; rm OpenSearchPipeline.zip You should see a directory in the AWS Cloud9 file explorer OpenSearchPipeline:\nThe OpenSearchPipeline directory contains example items that will be loaded into a DynamoDB table, as Bash script to simplify managing credentials when signing requests for OpenSearch, and a python script for executing a query to Bedrock.\nYou are now ready to start the lab. In the next module, you will complete setup for each of the three services used in this lab before moving on to integrate them.\n"
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.1/1.1.1/",
	"title": "Prerequisites and Start",
	"tags": [],
	"description": "",
	"content": "Prerequisites To run this lab, you\u0026rsquo;ll need an AWS account, and a user identity with access to the following services:\nAmazon DynamoDB AWS Cloud9 Environment You can use your own account, or an account provided through Workshop Studio Event Delivery as part of an AWS organized workshop. Using an account provided by Workshop Studio is the easier path, as you will have full access to all AWS services, and the account will terminate automatically when the event is over.\nAccount setup Using an account provided to you by your lab instructor If you are running this workshop using a link provided to you by your AWS instructor, please use that link and enter the access-code provided to you as part of the workshop. In the lab AWS account, the Cloud9 instance should already be provisioned. Please open the \u0026ldquo;AWS Cloud9\u0026rdquo; section of the AWS Management Console in the correct region and look for a lab instance called DynamoDBC9.\nUsing your own AWS account If you are using your own AWS account, be sure you have access to create and manage resources in Amazon DynamoDB and AWS Cloud9 environment\nAfter completing the workshop, remember to complete the cleanup section to remove any unnecessary AWS resources.\nLaunch the CloudFormation stack During the course of the lab, you will make DynamoDB tables that will incur a cost that could approach tens or hundreds of dollars per day. Ensure you delete the DynamoDB tables using the DynamoDB console, and make sure you delete the CloudFormation stack as soon as the lab is complete\nLaunch the CloudFormation template in US West 2 to deploy the resources in your account: Optionally, download the YAML template and launch it your own way Click Next on the first dialog.\nIn the Parameters section, note the Timeout is set to zero. This means the Cloud9 instance will not sleep; you may want to change this manually to a value such as 60 to protect against unexpected charges if you forget to delete the stack at the end.\nLeave the WorkshopZIP parameter unchanged and click Next. Scroll to the bottom and click Next, and then review the Template and Parameters. When you are ready to create the stack, scroll to the bottom, check the box acknowledging the creation of IAM resources, and click Create stack. The stack will create a Cloud9 lab instance, a role for the instance, and a role for the AWS Lambda function used later on in the lab. It will use Systems Manager to configure the Cloud9 instance.\nAfter the CloudFormation stack is CREATE_COMPLETE, continue to the next step.\nAccess EC2 to search for the Instance that was created from the stack. Execute \u0026ldquo;Connect to instance.\u0026rdquo; Run the command aws sts get-caller-identity just to verify that your AWS credentials have been configured correctly. "
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.2/1.2.1/",
	"title": "Read Sample Data",
	"tags": [],
	"description": "",
	"content": "Before we can do anything we have to learn what our data looks like.\nDynamoDB provides the Scan API which can be invoked using the scan CLI command . Scan will do a full table scan and return the items in 1MB chunks. Scanning is the slowest and most expensive way to get data out of DynamoDB; Scanning this on a large table from the CLI might be unwieldy but we know there are only a few items in our sample data so its OK to do here. Try running a scan on the ProductCatalog table:\naws dynamodb scan --table-name ProductCatalog Use the arrow keys to move up and down through the Scan response. Type :q and hit enter to exit the viewer once you\u0026rsquo;re done reviewing the response.\nData input and output in the CLI utilizes the DynamoDB JSON format, which is described in the DynamoDB Low-Level API section of the Developer Guide.\nWe can see from our data that this ProductCatalog table has two types of products: Book and Bicycle items.\nIf we wanted to read just a single item, we would use the GetItem API which can be invoked using the get-item CLI command . GetItem is the fastest and cheapest way to get data out of DynamoDB as you must specify the full Primary Key so the command is guaranteed to match at most one item in the table.\naws dynamodb get-item \\ --table-name ProductCatalog \\ --key \u0026#39;{\u0026#34;Id\u0026#34;:{\u0026#34;N\u0026#34;:\u0026#34;101\u0026#34;}}\u0026#39; By default a read from DynamoDB will use eventual consistency because eventually consistent reads in DynamoDB are half the price of a strongly consistent read. See Read Consistency in the DynamoDB Developer Guide for more information.\nThere are many useful options to the get-item command but a few that get used regularly are:\n\u0026ndash;consistent-read : Specifying that you want a strongly consistent read \u0026ndash;projection-expression : Specifying that you only want certain attributes returned in the request \u0026ndash;return-consume-capacity : Tell us how much capacity was consumed by the request Let\u0026rsquo;s run the previous command and add some of these options to the command line:\naws dynamodb get-item \\ --table-name ProductCatalog \\ --key \u0026#39;{\u0026#34;Id\u0026#34;:{\u0026#34;N\u0026#34;:\u0026#34;101\u0026#34;}}\u0026#39; \\ --consistent-read \\ --projection-expression \u0026#34;ProductCategory, Price, Title\u0026#34; \\ --return-consumed-capacity TOTAL We can see from the returned values:\n{ \u0026#34;Item\u0026#34;: { \u0026#34;Price\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;2\u0026#34; }, \u0026#34;Title\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Book 101 Title\u0026#34; }, \u0026#34;ProductCategory\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Book\u0026#34; } }, \u0026#34;ConsumedCapacity\u0026#34;: { \u0026#34;TableName\u0026#34;: \u0026#34;ProductCatalog\u0026#34;, \u0026#34;CapacityUnits\u0026#34;: 1.0 } } That performing this request consume 1.0 RCU, because this item is less than 4KB. If we run the command again but remove the \u0026ndash;consistent-read option, we will see that eventually consistent reads consume half as much capacity:\naws dynamodb get-item \\ --table-name ProductCatalog \\ --key \u0026#39;{\u0026#34;Id\u0026#34;:{\u0026#34;N\u0026#34;:\u0026#34;101\u0026#34;}}\u0026#39; \\ --projection-expression \u0026#34;ProductCategory, Price, Title\u0026#34; \\ --return-consumed-capacity TOTAL We will see this output:\n{ \u0026#34;Item\u0026#34;: { \u0026#34;Price\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;2\u0026#34; }, \u0026#34;Title\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Book 101 Title\u0026#34; }, \u0026#34;ProductCategory\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Book\u0026#34; } }, \u0026#34;ConsumedCapacity\u0026#34;: { \u0026#34;TableName\u0026#34;: \u0026#34;ProductCatalog\u0026#34;, \u0026#34;CapacityUnits\u0026#34;: 0.5 } } "
},
{
	"uri": "http://Handoo464.github.io/8-ldc/8.1/8.1.1/",
	"title": "Retail Cart References",
	"tags": [],
	"description": "",
	"content": "How to tackle this challenge What are the access patterns?\nThe access patterns in the scenario are:\nInsert and update items placed in a cart by users. Return items related to a user (AccountID), sorted by CreateTimestamp, and scoped to a specific Status. Return items across user by ItemSKU, sorted by CreateTimestamp, and scoped to a specific Status. Offline ad hoc queries for Business Intelligence team. Identify possible partition keys to fulfill the primary access pattern:\nWhat item attribute scales in volume along with higher access? What is a natural organization for the related data items (so as to return collected items relative to the access patterns above)? Consider the dimension of access: both reads and writes. When determining how to organize items related to the primary access pattern:\nWhat organization needs to be written to return items in sorted order (sort by)? What is the hierarchy of the relationships (most general to more specific)? Fulfilling the second, third, and fourth access patterns:\nThe second and third access patterns are OLTP access patterns and can be modeled directly in DynamoDB. The fourth access pattern is OLAP and does not need to be fulfilled directly on DynamoDB, or in your solution for that matter. Helpful references Given the above, see below for some helpful references.\nBest Practices for Using Sort Keys to Organize Data Working with Queries in DynamoDB Using Global Secondary Indexes in DynamoDB How to perform advanced analytics and build visualizations of your Amazon DynamoDB data by using Amazon Athena "
},
{
	"uri": "http://Handoo464.github.io/8-ldc/8.2/8.2.1/",
	"title": "Retail Cart References",
	"tags": [],
	"description": "",
	"content": "How to tackle this challenge What are the access patterns?\nThe access patterns in the scenario are outlined as:\nInsert scheduled payments. Return scheduled payments by user for the next 90 days. Return payments across users for specific date by status (SCHEDULED or PENDING). Identify possible partitions keys to fulfill the primary access pattern:\nWhat item attribute (AccountID, ScheduledTime, Status, DataBlob) scales with access patterns? What is a natural organization for the related payment items (so as to return collected items relative to the access patterns above)? Consider the dimension of access: both reads and writes. When determining how to organize items related to the primary access pattern:\nWhat organization do items need to be written with to return items by user for a date range (sort by)? What is the hierarchy of the relationships and when does it apply (most general to more specific)? Fulfilling the third access patterns:\nThe third access pattern is OLTP and can be fulfilled directly on DynamoDB Helpful references Given the above, see below for some helpful references.\nBest Practices for Using Sort Keys to Organize Data Working with Queries Using Global Secondary Indexes in DynamoDB Write Shard a GSI for Selective Queries in DynamoDB "
},
{
	"uri": "http://Handoo464.github.io/8-ldc/8.1/",
	"title": "Retail Cart Scenario",
	"tags": [],
	"description": "",
	"content": "Retail Cart Challenge An online retail store has asked you to design their data storage layer and NoSQL table(s). The website serves customers and the products they view, save, and purchase. The website traffic is currently low, but they want to be able to serve millions of concurrent customers.\nCustomers interact with products that can be ACTIVE, SAVED, or PURCHASED. Once they are PURCHASED they will be assigned an OrderId. Products have the following attributes: AccountID, Status (ACTIVE, SAVED, or PURCHASED), CreateTimestamp, and ItemSKU (Total item size is \u0026lt;= 1 KB). When a customer opens the retail store\u0026rsquo;s application, they view the active products in their cart, which are organized by most recently added. Users can view products that they have saved for later, which are organized by most recently saved. Users can view products that they have purchased, which are organized by most recently purchased. Product teams have the ability to regularly query across all customers to identify the people who have a specific product in their account that is either ACTIVE, SAVED, or PURCHASED. The Business Intelligence team needs to run a number of complex ad hoc queries against the dataset to create weekly and monthly reports. Build a NoSQL Data Model to fulfill the OLTP component of the workload. How you would fulfill the BI team’s requirement?\n"
},
{
	"uri": "http://Handoo464.github.io/7-lgme/7.3/7.3.4/",
	"title": "Retrieve Item collections",
	"tags": [],
	"description": "",
	"content": "An Item collection is a set of items with the same partition key value but different sort key values where the items of different entity types have relationships with the partition key.\nAs you read in the previous module, you should optimize DynamoDB tables for the number of requests it receives. It was also mentioned that DynamoDB does not have joins that a relational database has. Instead, you design your table to allow for join-like behavior in your requests.\nIn this step, you retrieve multiple entity types in a single request. In the gaming application, you may want to fetch details about a game. These details include information about the game itself such as the time it started, time it ended, and details about the users that played in the game.\nThis request spans two entity types: the Game entity and the UserGameMapping entity. However, this doesn’t mean you need to make multiple requests.\nIn the code you downloaded, a fetch_game_and_players.py script is in the scripts/ directory. This script shows how you can structure your code to retrieve both the Game entity and the UserGameMapping entity for the game in a single request.\nThe following code composes the fetch_game_and_players.py script:\nimport boto3 from entities import Game, UserGameMapping dynamodb = boto3.client(\u0026#39;dynamodb\u0026#39;) GAME_ID = \u0026#34;3d4285f0-e52b-401a-a59b-112b38c4a26b\u0026#34; def fetch_game_and_users(game_id): resp = dynamodb.query( TableName=\u0026#39;battle-royale\u0026#39;, KeyConditionExpression=\u0026#34;PK = :pk AND SK BETWEEN :metadata AND :users\u0026#34;, ExpressionAttributeValues={ \u0026#34;:pk\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;GAME#{}\u0026#34;.format(game_id) }, \u0026#34;:metadata\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;#METADATA#{}\u0026#34;.format(game_id) }, \u0026#34;:users\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;USER$\u0026#34; }, }, ScanIndexForward=True ) game = Game(resp[\u0026#39;Items\u0026#39;][0]) game.users = [UserGameMapping(item) for item in resp[\u0026#39;Items\u0026#39;][1:]] return game game = fetch_game_and_users(GAME_ID) print(game) for user in game.users: print(user) At the beginning of this script, you import the Boto 3 library and some simple classes to represent the objects in the application code. You can see the definitions for those entities in the scripts/entities.py file.\nThe real work of the script happens in the fetch_game_and_users function that’s defined in the module. This is similar to a function you would define in your application to be used by any endpoints that need this data.\nThe fetch_game_and_users function does a few things. First, it makes a Query request to DynamoDB. This Query uses a PK of GAME#\u0026lt;GameId\u0026gt;. Then, it requests any entities where the sort key is between #METADATA#\u0026lt;GameId\u0026gt; and USER$. This grabs the Game entity, whose sort key is #METADATA#\u0026lt;GameId\u0026gt;, and all UserGameMapping entities, whose keys start with USER#. Sort keys of the string type are sorted by ASCII character codes. The dollar sign ($) comes directly after the pound sign (#) in ASCII , so this ensures that you will get all mappings in the UserGameMapping entity.\nWhen you receive a response, you assemble the data entities into objects known by the application. You know that the first entity returned is the Game entity, so you create a Game object from the entity. For the remaining entities, you create a UserGameMapping object for each entity and then attach the array of users to the Game object.\nThe end of the script shows the usage of the function and prints out the resulting objects.\nYou can run the script in the Cloud9 Terminal with the following command:\npython scripts/fetch_game_and_players.py The script should print the Game object and all UserGameMapping objects to the console.\nGame: 3d4285f0-e52b-401a-a59b-112b38c4a26b Map: Green Grasslands UserGameMapping: 3d4285f0-e52b-401a-a59b-112b38c4a26b Username: branchmichael UserGameMapping: 3d4285f0-e52b-401a-a59b-112b38c4a26b Username: deanmcclure UserGameMapping: 3d4285f0-e52b-401a-a59b-112b38c4a26b Username: emccoy UserGameMapping: 3d4285f0-e52b-401a-a59b-112b38c4a26b Username: emma83 UserGameMapping: 3d4285f0-e52b-401a-a59b-112b38c4a26b Username: iherrera UserGameMapping: 3d4285f0-e52b-401a-a59b-112b38c4a26b Username: jeremyjohnson UserGameMapping: 3d4285f0-e52b-401a-a59b-112b38c4a26b Username: lisabaker UserGameMapping: 3d4285f0-e52b-401a-a59b-112b38c4a26b Username: maryharris UserGameMapping: 3d4285f0-e52b-401a-a59b-112b38c4a26b Username: mayrebecca UserGameMapping: 3d4285f0-e52b-401a-a59b-112b38c4a26b Username: meghanhernandez UserGameMapping: 3d4285f0-e52b-401a-a59b-112b38c4a26b Username: nruiz UserGameMapping: 3d4285f0-e52b-401a-a59b-112b38c4a26b Username: pboyd UserGameMapping: 3d4285f0-e52b-401a-a59b-112b38c4a26b Username: richardbowman UserGameMapping: 3d4285f0-e52b-401a-a59b-112b38c4a26b Username: roberthill UserGameMapping: 3d4285f0-e52b-401a-a59b-112b38c4a26b Username: robertwood UserGameMapping: 3d4285f0-e52b-401a-a59b-112b38c4a26b Username: victoriapatrick UserGameMapping: 3d4285f0-e52b-401a-a59b-112b38c4a26b Username: waltervargas This script shows how you can model your table and write your queries to retrieve multiple entity types in a single DynamoDB request. In a relational database, you use joins to retrieve multiple entity types from different tables in a single request. With DynamoDB, you specifically model your data, so that entities you should access together are located next to each other in a single table. This approach replaces the need for joins in a typical relational database and keeps your application high-performing as you scale up.\nReview In this module, you designed a primary key and created a table. Then, you bulk-loaded data into the table and saw how to query for multiple entity types in a single request.\nWith the current primary key design, you can satisfy the following access patterns:\nCreate user profile (Write)\nUpdate user profile (Write)\nGet user profile (Read)\nCreate game (Write)\nView game (Read)\nJoin game for a user (Write)\nStart game (Write)\nUpdate game for a user (Write)\nUpdate game (Write)\n"
},
{
	"uri": "http://Handoo464.github.io/6-leda/6.5/6.5.1/",
	"title": "Solutions",
	"tags": [],
	"description": "",
	"content": "Lab 1 Solutions ReduceLambdaPolicy { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;ReadFromDynamoDBStream\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:DescribeStream\u0026#34;, \u0026#34;dynamodb:GetRecords\u0026#34;, \u0026#34;dynamodb:GetShardIterator\u0026#34;, \u0026#34;dynamodb:ListStreams\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:*:*:table/ReduceTable/stream/*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CreateCloudwatchLogGroup\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;WriteToCloudwatchLogGroup\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:log-group:/aws/lambda/ReduceLambda:*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;WriteToDynamoDBTable\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:UpdateItem\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:*:*:table/AggregateTable\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;ReadFromParameterTable\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:GetItem\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:*:*:table/ParameterTable\u0026#34; } ] } Lab 2 Solutions Lab 2 StateLambda function complete code # Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved. # SPDX-License-Identifier: MIT-0 # -------------------------------------------------------------------------------------------------- # Imports # -------------------------------------------------------------------------------------------------- # General Imports import json import time import base64 import random from decimal import Decimal # AWS Imports import boto3 from botocore.exceptions import ClientError # Project Imports import functions import constants # -------------------------------------------------------------------------------------------------- # Lambda Function # -------------------------------------------------------------------------------------------------- def lambda_handler(event, context): # Print Status at Start records = event[\u0026#39;Records\u0026#39;] print(\u0026#39;Invoked StateLambda with \u0026#39; + str(len(records)) + \u0026#39; record(s).\u0026#39;) # Initialize DynamoDB ddb_ressource = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = ddb_ressource.Table(constants.STATE_TABLE_NAME) # Get Failure PCT FAILURE_STATE_LAMBDA_PCT = functions.get_parameter(ddb_ressource, \u0026#34;FAILURE_STATE_LAMBDA_PCT\u0026#34;, 0) # Loop over records for record in records: # Manually Introduced Random Failure if random.uniform(0,100) \u0026lt; FAILURE_STATE_LAMBDA_PCT / len(records): # Raise exception raise Exception(\u0026#39;Manually Introduced Random Failure!\u0026#39;) # Load Record data = json.loads(base64.b64decode(record[\u0026#39;kinesis\u0026#39;][\u0026#39;data\u0026#39;]).decode(\u0026#39;utf-8\u0026#39;)) # Get Entries record_id = data[constants.ID_COLUMN_NAME] record_hierarchy = data[constants.HIERARCHY_COLUMN_NAME] record_value = data[constants.VALUE_COLUMN_NAME] record_version = data[constants.VERSION_COLUMN_NAME] record_time = data[constants.TIMESTAMP_COLUMN_NAME] # If Record is older than 1 Minute -\u0026gt; Ignore it if (time.time() - record_time) \u0026gt; 60: continue # Write to DDB try: table.update_item( Key = { constants.STATE_TABLE_KEY: record_id }, ConditionExpression = \u0026#39;attribute_not_exists(\u0026#39; + constants.STATE_TABLE_KEY + \u0026#39;) OR \u0026#39; + constants.VERSION_COLUMN_NAME + \u0026#39;\u0026lt; :new_version\u0026#39;, UpdateExpression = \u0026#39;SET #VALUE = :new_value,\u0026#39; + \\ \u0026#39;#VERSION = :new_version,\u0026#39; + \\ \u0026#39;#HIERARCHY = :new_hierarchy,\u0026#39; + \\ \u0026#39;#TIMESTAMP = :new_time\u0026#39;, ExpressionAttributeNames={ \u0026#39;#VALUE\u0026#39;: constants.VALUE_COLUMN_NAME, \u0026#39;#VERSION\u0026#39;: constants.VERSION_COLUMN_NAME, \u0026#39;#HIERARCHY\u0026#39;: constants.HIERARCHY_COLUMN_NAME, \u0026#39;#TIMESTAMP\u0026#39;: constants.TIMESTAMP_COLUMN_NAME }, ExpressionAttributeValues={ \u0026#39;:new_version\u0026#39;: record_version, \u0026#39;:new_value\u0026#39;: Decimal(str(record_value)), \u0026#39;:new_hierarchy\u0026#39;: json.dumps(record_hierarchy, sort_keys = True), \u0026#39;:new_time\u0026#39;: Decimal(str(record_time)) } ) except ClientError as e: if e.response[\u0026#39;Error\u0026#39;][\u0026#39;Code\u0026#39;]==\u0026#39;ConditionalCheckFailedException\u0026#39;: print(\u0026#39;Conditional put failed.\u0026#39; + \\ \u0026#39; This is either a duplicate or a more recent version already arrived.\u0026#39;) print(\u0026#39;Id: \u0026#39;, record_id) print(\u0026#39;Hierarchy: \u0026#39;, record_hierarchy) print(\u0026#39;Value: \u0026#39;, record_value) print(\u0026#39;Version: \u0026#39;, record_version) print(\u0026#39;Timestamp: \u0026#39;, record_time) else: raise e # Print Status at End print(\u0026#39;StateLambda successfully processed \u0026#39; + str(len(records)) + \u0026#39; record(s).\u0026#39;) return {\u0026#39;statusCode\u0026#39;: 200} Lab 2 ReduceLambda function complete code # Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved. # SPDX-License-Identifier: MIT-0 # -------------------------------------------------------------------------------------------------- # Imports # -------------------------------------------------------------------------------------------------- # General Imports import json import hashlib import random import time # AWS Imports import boto3 from botocore.exceptions import ClientError # Project Imports import functions import constants # -------------------------------------------------------------------------------------------------- # Lambda Function # -------------------------------------------------------------------------------------------------- def lambda_handler(event, context): # Print Status at Start records = event[\u0026#39;Records\u0026#39;] print(\u0026#39;Invoked ReduceLambda with \u0026#39; + str(len(records)) + \u0026#39; Delta message(s).\u0026#39;) # Initialize Dict for Total Delta totals = dict() # Initialize DDB Ressource ddb_ressource = boto3.resource(\u0026#39;dynamodb\u0026#39;) # Keep track of number of batches for timestamp mean batch_count = 0 # Iterate over Messages for record in event[\u0026#39;Records\u0026#39;]: # Aggregate over Batch of Messages the Lambda was invoked with if \u0026#39;NewImage\u0026#39; in record[\u0026#39;dynamodb\u0026#39;]: # Load Message to Dict message = record[\u0026#39;dynamodb\u0026#39;][\u0026#39;NewImage\u0026#39;][\u0026#39;Message\u0026#39;][\u0026#39;S\u0026#39;].replace(\u0026#34;\u0026#39;\u0026#34;,\u0026#39;\u0026#34;\u0026#39;) data = json.loads(message) # Get Batch Count (To Calculate Mean of Timestamp) batch_count += 1 # Iterate over Entries in Message for entry in data: if (entry == constants.TIMESTAMP_GENERATOR_FIRST or entry == constants.TIMESTAMP_GENERATOR_MEAN): continue else: functions.dict_entry_add(totals, entry, data[entry]) # If this batch contains only deletes: Done if not totals: print(\u0026#39;Skipped batch - no new entries.\u0026#39;) return {\u0026#39;statusCode\u0026#39;: 200} # Total Count of New Messages (for Printing) total_new_message_count = totals[constants.MESSAGE_COUNT_NAME] # Update all Values within one single transaction ddb_client = boto3.client(\u0026#39;dynamodb\u0026#39;) # Batch of Items batch = [ { \u0026#39;Update\u0026#39;: { \u0026#39;TableName\u0026#39; : constants.AGGREGATE_TABLE_NAME, \u0026#39;Key\u0026#39; : {constants.AGGREGATE_TABLE_KEY : {\u0026#39;S\u0026#39; : entry}}, \u0026#39;UpdateExpression\u0026#39; : \u0026#34;ADD #val :val \u0026#34;, \u0026#39;ExpressionAttributeValues\u0026#39; : { \u0026#39;:val\u0026#39;: {\u0026#39;N\u0026#39; : str(totals[entry])} }, \u0026#39;ExpressionAttributeNames\u0026#39;: { \u0026#34;#val\u0026#34; : \u0026#34;Value\u0026#34; } } } for entry in totals.keys()] # Calculate hash to ensure this batch hasn\u0026#39;t been processed already: record_list_hash = hashlib.md5(str(records).encode()).hexdigest() response = ddb_client.transact_write_items( TransactItems = batch, ClientRequestToken = record_list_hash ) # Manually Introduced Random Failure if random.uniform(0,100) \u0026lt; functions.get_parameter(ddb_ressource, \u0026#34;FAILURE_REDUCE_LAMBDA_PCT\u0026#34;, 0): # Raise Exception raise Exception(\u0026#39;Manually Introduced Random Failure!\u0026#39;) # Print Status at End print(\u0026#39;ReduceLambda finished. Updates aggregates with \u0026#39; + str(total_new_message_count) + \u0026#39; new message(s) in total.\u0026#39;) return {\u0026#39;statusCode\u0026#39;: 200} "
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.1/",
	"title": "Start here: Getting Started",
	"tags": [],
	"description": "",
	"content": "Once you have completed with either setup, continue on to:\nExercise 1: DynamoDB Capacity Units and Partitioning "
},
{
	"uri": "http://Handoo464.github.io/4-lcdc/4.1/4.1.1/",
	"title": "Start with Cloud9",
	"tags": [],
	"description": "",
	"content": "Create a Cloud9 Environment To complete the steps in these labs, you need an IAM role that has the privileges to create, update and delete AWS Cloud9 environments, Lambda functions, DynamoDB tables, IAM roles, Kinesis Data Streams and DynamoDB Streams\nLog into the AWS Management Console, go to the AWS Cloud9 service dashboard then select Create environment. Give your new environment a name - DynamoDB Labs then provide an optional description for the environment. Select t2.small as your instance type, leave all other fields as the default values then select Create. Wait for creation of your Cloud9 environment to complete then select Open to launch your Cloud9 evironment. Start a command line terminal in Cloud9 and set up the Region and Account ID environment variables.\nexport REGION={your aws region} \u0026amp;\u0026amp; export ACCOUNT_ID={your aws account ID} Install jq on your AWS Cloud9 environment using the command below.\nsudo yum install jq -y After completing the workshop, remember to complete the Clean Up section to remove AWS resources that you no longer require.\nNow that your environment is set up, continue on to the 2. Scenario Overview.\n"
},
{
	"uri": "http://Handoo464.github.io/4-lcdc/4.1/4.1.2/",
	"title": "Start with EC2 Instance",
	"tags": [],
	"description": "",
	"content": "Open the EC2 Instance interface and click Lauch Instance\nName it as you wish; here, the EC2 Instance is named DynamoDB Labs\nSelect the instance type as t3.small.\nCreate a new key pair as you wish.\nConfigure the network settings for the EC2 Instance with a network that can connect to the session manager. If you are not experienced with this, you can refer to this article: After completing, you can connect the EC2 Instance with the session manager and proceed to the next part.\n"
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.6/3.6.1/",
	"title": "Step 1 - Add a new global secondary index to the employees table",
	"tags": [],
	"description": "",
	"content": "Add a new global secondary index that uses the is_manager attribute as the global secondary index partition key, which is stored under the attribute named GSI_2_PK. Employee job titles are stored under GSI_2_SK.\nRun the following AWS CLI command.\naws dynamodb update-table --table-name employees \\ --attribute-definitions AttributeName=GSI_2_PK,AttributeType=S AttributeName=GSI_2_SK,AttributeType=S \\ --global-secondary-index-updates file://gsi_manager.json Wait until the global secondary index has been created. This takes approximately 5 minutes.\nCheck output of the following command. If \u0026ldquo;IndexStatus\u0026rdquo;: is \u0026ldquo;CREATING\u0026rdquo; you will have to wait until \u0026ldquo;IndexStatus\u0026rdquo; is \u0026ldquo;ACTIVE\u0026rdquo; before continuing to the next step.\naws dynamodb describe-table --table-name employees --query \u0026#34;Table.GlobalSecondaryIndexes[].IndexStatus\u0026#34; The output will initially look like the following.\n[ \u0026#34;CREATING\u0026#34;, \u0026#34;ACTIVE\u0026#34; ] You also can script the command to run every 2 seconds using watch.\n# Watch checks every 2 seconds by default watch -n 2 \u0026#34;aws dynamodb describe-table --table-name employees --query \\\u0026#34;Table.GlobalSecondaryIndexes[].IndexStatus\\\u0026#34;\u0026#34; Press Ctrl + C to end watch after the global secondary index has been created.\nWait until the new index is ACTIVE before proceeding.\n[ \u0026#34;ACTIVE\u0026#34;, \u0026#34;ACTIVE\u0026#34; ] Do not continue until the IndexStatus is ACTIVE on both indexes. Querying the index before it is ACTIVE will result in a failed query.\n"
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.7/3.7.1/",
	"title": "Step 1 - Create a new global secondary index for City-Department",
	"tags": [],
	"description": "",
	"content": "Run the following command to create a new global secondary index called GSI_3:\naws dynamodb update-table --table-name employees \\ --attribute-definitions AttributeName=GSI_3_PK,AttributeType=S AttributeName=GSI_3_SK,AttributeType=S \\ --global-secondary-index-updates file://gsi_city_dept.json You have to wait until the index has been created before proceeding.\nNotice that the following command shows the global secondary index\u0026rsquo;s status as CREATING.\naws dynamodb describe-table --table-name employees --query \u0026#34;Table.GlobalSecondaryIndexes[].IndexStatus\u0026#34; The output looks similar to the following. The order of the table statuses may be different.\n[ \u0026#34;ACTIVE\u0026#34;, \u0026#34;ACTIVE\u0026#34;, \u0026#34;CREATING\u0026#34; ] You can script this command to run every two seconds by using watch, so that you can more easily see when the table status has changed to ACTIVE.\n# Watch checks every two seconds by default watch -n 2 \u0026#34;aws dynamodb describe-table --table-name employees --query \\\u0026#34;Table.GlobalSecondaryIndexes[].IndexStatus\\\u0026#34;\u0026#34; Use Ctrl + C to end watch after the global secondary index has been created.\nWait until the new index is ACTIVE before proceeding.\n[ \u0026#34;ACTIVE\u0026#34;, \u0026#34;ACTIVE\u0026#34;, \u0026#34;ACTIVE\u0026#34; ] Now you can use the new global secondary index to query the table. You must to use the partition key, and you can use the sort key (but it is optional).\nFor the sort key, you can use the begins_with expression to query starting with the left-most attribute of the composite key. As a result, you can query all employees in a city or in a specific department in a city.\nThe KeyConditionExpression looks like the following.\nKey(\u0026#39;GSI_3_PK\u0026#39;).eq(\u0026#34;state#{}\u0026#34;.format(\u0026#39;TX\u0026#39;)) \u0026amp; Key(\u0026#39;GSI_3_SK\u0026#39;).begins_with(\u0026#39;Austin\u0026#39;) Wait until the IndexStatus is ACTIVE on all indexes before continuing. If you try to query a GSI but it is not finished creating, you will receive an error.\n"
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.8/3.8.1/",
	"title": "Step 1 - Create and load the the InvoiceandBilling table",
	"tags": [],
	"description": "",
	"content": "Run the AWS CLI command to create the table named InvoiceAndBilling and create a GSI named GSI_1 which is partitioned on SK attribute of the parent table:\naws dynamodb create-table --table-name InvoiceAndBills \\ --attribute-definitions AttributeName=PK,AttributeType=S AttributeName=SK,AttributeType=S \\ --key-schema AttributeName=PK,KeyType=HASH AttributeName=SK,KeyType=RANGE \\ --provisioned-throughput ReadCapacityUnits=100,WriteCapacityUnits=100 \\ --tags Key=workshop-design-patterns,Value=targeted-for-cleanup \\ --global-secondary-indexes \u0026#34;IndexName=GSI_1,\\ KeySchema=[{AttributeName=SK,KeyType=HASH}],\\ Projection={ProjectionType=ALL},\\ ProvisionedThroughput={ReadCapacityUnits=100,WriteCapacityUnits=100}\u0026#34; Wait until the table becomes active:\naws dynamodb wait table-exists --table-name InvoiceAndBills Then, load data into the InvoiceAndBills table:\npython load_invoice.py InvoiceAndBills ./data/invoice-data.csv "
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.2/3.2.1/",
	"title": "Step 1 - Create the DynamoDB table",
	"tags": [],
	"description": "",
	"content": "Run the following AWS CLI command to create the first DynamoDB table called logfile:\naws dynamodb create-table --table-name logfile \\ --attribute-definitions AttributeName=PK,AttributeType=S AttributeName=GSI_1_PK,AttributeType=S \\ --key-schema AttributeName=PK,KeyType=HASH \\ --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5 \\ --tags Key=workshop-design-patterns,Value=targeted-for-cleanup \\ --global-secondary-indexes \u0026#34;IndexName=GSI_1,\\ KeySchema=[{AttributeName=GSI_1_PK,KeyType=HASH}],\\ Projection={ProjectionType=INCLUDE,NonKeyAttributes=[\u0026#39;bytessent\u0026#39;]},\\ ProvisionedThroughput={ReadCapacityUnits=5,WriteCapacityUnits=5}\u0026#34; The table you just created will have the following structure.\nTable logfile: Key schema: HASH (partition key) Table read capacity units (RCUs) = 5 Table write capacity units (WCUs) = 5 Global secondary index (GSI): GSI_1 (5 RCUs, 5 WCUs) - Allows for querying by host IP address. Attribute Name (Type) Special Attribute? Attribute Use Case Sample Attribute Value PK (STRING) Partition key Holds the request id for the access log request#104009 GSI_1_PK (STRING) GSI 1 partition key The host for the request, an IPv4 address host#66.249.67.3 Special attributes include named attributes that define the primary key of a DynamoDB table or a global secondary index (GSI). Global secondary indexes have primary keys just like DynamoDB tables. In DynamoDB, the partition key is the same as the hash key, and the sort key is the same as the range key. The DynamoDB APIs use the terms hash and range, while the AWS documentation uses the terms partition and range. No matter which terms you use, these two keys together form the primary key. To learn more, please review our AWS Developer Guide for Amazon DynamoDB section on the primary key .\nRun the following AWS CLI command to wait until the table becomes ACTIVE\naws dynamodb wait table-exists --table-name logfile You also can run the following AWS CLI command to get only the table status.\naws dynamodb describe-table --table-name logfile --query \u0026#34;Table.TableStatus\u0026#34; "
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.5/3.5.1/",
	"title": "Step 1 - Create the employees table for global secondary index key overloading",
	"tags": [],
	"description": "",
	"content": "Run the following AWS CLI command to create the employees table.\naws dynamodb create-table --table-name employees \\ --attribute-definitions AttributeName=PK,AttributeType=S AttributeName=SK,AttributeType=S \\ AttributeName=GSI_1_PK,AttributeType=S AttributeName=GSI_1_SK,AttributeType=S \\ --key-schema AttributeName=PK,KeyType=HASH AttributeName=SK,KeyType=RANGE \\ --provisioned-throughput ReadCapacityUnits=100,WriteCapacityUnits=100 \\ --tags Key=workshop-design-patterns,Value=targeted-for-cleanup \\ --global-secondary-indexes \u0026#34;IndexName=GSI_1,\\ KeySchema=[{AttributeName=GSI_1_PK,KeyType=HASH},{AttributeName=GSI_1_SK,KeyType=RANGE}],\\ Projection={ProjectionType=ALL},\\ ProvisionedThroughput={ReadCapacityUnits=100,WriteCapacityUnits=100}\u0026#34; Run the following command to wait until the table becomes active.\naws dynamodb wait table-exists --table-name employees Let\u0026rsquo;s take a closer look at the create-table command. You are creating a table named employees. The partition key on the table is PK and it holds the employee ID. The sort key is SK, which contains a derived value that you choose in the Python script. (We\u0026rsquo;ll revisit this shortly.) You will create a global secondary index on this table and name it GSI_1; This will be an overloaded global secondary index. The partition key on the global secondary index is GSI_1_PK, and holds the same value as the sort key SK on the base table. The GSI_1 sort key value is name, and its attribute name is GSI_1_SK.\nTable: employees Key schema: HASH, RANGE (partition and sort key) Table read capacity units (RCUs) = 100 Table write capacity units (WCUs) = 100 Global secondary index: GSI_1 (100 RCUs, 100 WCUs) - Allows for querying by host IP address. Attribute Name (Type) Special Attribute? Attribute Use Case Sample Attribute Value PK (STRING) Partition Key Employee ID e#129 SK (STRING) Sort key Derived value root, state#MI GSI_1_PK (STRING) GSI_1 partition key Derived value root, state#MI GSI_1_SK (STRING) GSI_1 sort key Employee name Christine Milsted "
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.9/3.9.1/",
	"title": "Step 1 - Create the replica table",
	"tags": [],
	"description": "",
	"content": "\nLet\u0026rsquo;s create a table named logfile_replica to hold the replicated rows. This create-table command is based on the command to create the logfile table. As a result, it creates a table that can hold the same items as its upstream table.\naws dynamodb create-table --table-name logfile_replica \\ --attribute-definitions AttributeName=PK,AttributeType=S AttributeName=GSI_1_PK,AttributeType=S \\ --key-schema AttributeName=PK,KeyType=HASH --provisioned-throughput ReadCapacityUnits=10,WriteCapacityUnits=5 \\ --tags Key=workshop-design-patterns,Value=targeted-for-cleanup \\ --global-secondary-indexes \u0026#34;IndexName=GSI_1,KeySchema=[{AttributeName=GSI_1_PK,KeyType=HASH}],\\ Projection={ProjectionType=INCLUDE,NonKeyAttributes=[\u0026#39;bytessent\u0026#39;, \u0026#39;requestid\u0026#39;, \u0026#39;host\u0026#39;]},\\ ProvisionedThroughput={ReadCapacityUnits=10,WriteCapacityUnits=5}\u0026#34; This command creates a new table and an associated global secondary index with the following definition.\nTable: logfile_replica Key schema: HASH (partition key) Table read capacity units (RCUs) = 10 Table write capacity units (WCUs) = 5 • Global secondary index: GSI_1 (10 RCUs, 5 WCUs) - Allows for querying by host IP address. Attribute Name (Type) Special Attribute? Attribute Use Case Sample Attribute Value PK (STRING) Partition key Holds the request id request#104009 GSI_1_PK (STRING) GSI 1 partition key Host host#66.249.67.3 Run the following command to wait until the table becomes active.\naws dynamodb wait table-exists --table-name logfile_replica "
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.4/3.4.1/",
	"title": "Step 1 - Creating the GSI",
	"tags": [],
	"description": "",
	"content": "The global secondary index for this exercise was created during the setup stage of this workshop. You can see the description of the global secondary index by executing the following AWS CLI command.\naws dynamodb describe-table --table-name logfile_scan --query \u0026#34;Table.GlobalSecondaryIndexes\u0026#34; The description of the global secondary indexes should look like the following.\n{ \u0026#34;GlobalSecondaryIndexes\u0026#34;: [ { \u0026#34;IndexName\u0026#34;: \u0026#34;GSI_1\u0026#34;, \u0026#34;KeySchema\u0026#34;: [ { \u0026#34;AttributeName\u0026#34;: \u0026#34;GSI_1_PK\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;HASH\u0026#34; }, { \u0026#34;AttributeName\u0026#34;: \u0026#34;GSI_1_SK\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;RANGE\u0026#34; } ], \u0026#34;Projection\u0026#34;: { \u0026#34;ProjectionType\u0026#34;: \u0026#34;KEYS_ONLY\u0026#34; }, \u0026#34;IndexStatus\u0026#34;: \u0026#34;ACTIVE\u0026#34;, \u0026#34;ProvisionedThroughput\u0026#34;: { \u0026#34;NumberOfDecreasesToday\u0026#34;: 0, \u0026#34;ReadCapacityUnits\u0026#34;: 3000, \u0026#34;WriteCapacityUnits\u0026#34;: 5000 }, \u0026#34;IndexSizeBytes\u0026#34;: 0, \u0026#34;ItemCount\u0026#34;: 0, \u0026#34;IndexArn\u0026#34;: \u0026#34;arn:aws:dynamodb:(region):(accountid):table/logfile_scan/index/GSI_1\u0026#34; } ] } The DynamoDB ItemCount in this example is zero. DynamoDB calculates the total item count seen in this API many times throughout the day, and it may be possible that you see a different number than what the preceding output shows. This is normal.\n"
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.3/3.3.1/",
	"title": "Step 1 - Execute a sequential Scan",
	"tags": [],
	"description": "",
	"content": "First, execute a simple (sequential) Scan to calculate the total bytes sent for all records with response code \u0026lt;\u0026gt; 200. You can run a Scan with a filter expression to filter out unrelated records. The application worker will then sum the values of all the returned records where response code \u0026lt;\u0026gt; 200.\nThe Python file, scan_logfile_simple.py, includes the command to run a Scan with a filter expression and then calculate the sum of bytes sent.\nThe following code block scans the table.\nfe = \u0026#34;responsecode \u0026lt;\u0026gt; :f\u0026#34; eav = {\u0026#34;:f\u0026#34;: 200} response = table.scan( FilterExpression=fe, ExpressionAttributeValues=eav, Limit=pageSize, ProjectionExpression=\u0026#39;bytessent\u0026#39;) You can review the file on your own with vim ~/workshop/scan_logfile_simple.py. Type :q and hit enter to exit vim.\nNotice that there is a Limit parameter set in the Scan command. A single Scan operation will read up to the maximum number of items set (if using the Limit parameter) or a maximum of 1 MB of data, and then apply any filtering to the results by using FilterExpression. If the total number of scanned items exceeds the maximum set by the limit parameter or the data set size limit of 1 MB, the scan stops and results are returned to the user as a LastEvaluatedKey value. This value can be used in a subsequent operation so that you can pick up where you left off.\nIn the following code, the LastEvaluatedKey value in the response is passed to the Scan method via the ExclusiveStartKey parameter.\nwhile \u0026#39;LastEvaluatedKey\u0026#39; in response: response = table.scan( FilterExpression=fe, ExpressionAttributeValues=eav, Limit=pageSize, ExclusiveStartKey=response[\u0026#39;LastEvaluatedKey\u0026#39;], ProjectionExpression=\u0026#39;bytessent\u0026#39;) for i in response[\u0026#39;Items\u0026#39;]: totalbytessent += i[\u0026#39;bytessent\u0026#39;] When the last page is returned, LastEvaluatedKey is not part of the response, so you know that the scan is complete.\nNow, execute this code.\npython scan_logfile_simple.py logfile_scan Parameters: Tablename: logfile_scan\nThe output will look like the following.\nScanning 1 million rows of table logfile_scan to get the total of bytes sent Total bytessent 6054250 in 16.325594425201416 seconds Make a note of the time it took for the scan to complete. With this exercise\u0026rsquo;s dataset, a parallel scan will complete faster than the sequential scan.\n"
},
{
	"uri": "http://Handoo464.github.io/6-leda/6.3/6.3.1/",
	"title": "Step 1: Connect StateLambda",
	"tags": [],
	"description": "",
	"content": "The objective of this step is to connect the StateLambda function with the IncomingDataStream Kinesis stream as shown in the diagram below. When new messages become available in the Kinesis stream, the StateLambda function will be invoked to process the stream records. Each stream record contains a single trade.\nConnect the StateLambda function with a Kinesis Data Stream Use the AWS Management Console and navigate to the AWS Lambda service within the console. Click on the StateLambda function to edit its configuration. See the figure below. The function overview shows that the StateLambda function doesn\u0026rsquo;t have any triggers yet. Click on the Add trigger button. Specify the following configuration (see the figure below for details): In the first drop down select Kinesis as the data source. For the Kinesis stream, select IncomingDataStream. Set the Batch size to 100. Click Add in the bottom right corner to create and enable an event source mapping on the Lambda function. At this point you have configured an event based connection between Kinesis Data Streams and AWS Lambda. The StateLambda function will be invoked whenever new messages appear in the IncomingDataStream. The messages will be passed to the Lambda function in batches of up to 100 at a time.\nHow do you know it is working? If everything was done correctly then the StateLambda function will be invoked with stream records from the Kinesis stream. Therefore, in one to two minutes you should be able to see logs from the Lambda invocations under the Monitor in the Logs tab.\nYou can also observe the outputs of StateLambda to verify the connection by reviewing the Items section of the DynamoDB console. To do that, navigate to the DynamoDB service in the AWS console, click Items on the left, and select StateTable.\nAt this stage you should see multiple rows similar to the image below. The number of items returned may vary. You can click on the orange Run button if you want to refresh the items.\nContinue on to: Step 2.\n"
},
{
	"uri": "http://Handoo464.github.io/6-leda/6.4/6.4.1/",
	"title": "Step 1: Prevent duplicates at StateLambda function",
	"tags": [],
	"description": "",
	"content": "The objective of this step is to modify the StateLambda function such that it does not successfully write duplicate messages to downstream resources.\nStudy the StateLambda code Use the AWS Management Console and navigate to the AWS Lambda service within the console. Click on the StateLambda function to edit its configuration Click on the Code tab to access the Lambda function\u0026rsquo;s code In the Lambda code browser, locate the code snipped below:\ntable.update_item( Key = { constants.STATE_TABLE_KEY: record_id }, UpdateExpression = \u0026#39;SET #VALUE = :new_value,\u0026#39; + \\ \u0026#39;#VERSION = :new_version,\u0026#39; + \\ \u0026#39;#HIERARCHY = :new_hierarchy,\u0026#39; + \\ \u0026#39;#TIMESTAMP = :new_time\u0026#39;, ExpressionAttributeNames={ \u0026#39;#VALUE\u0026#39;: constants.VALUE_COLUMN_NAME, \u0026#39;#VERSION\u0026#39;: constants.VERSION_COLUMN_NAME, \u0026#39;#HIERARCHY\u0026#39;: constants.HIERARCHY_COLUMN_NAME, \u0026#39;#TIMESTAMP\u0026#39;: constants.TIMESTAMP_COLUMN_NAME }, ExpressionAttributeValues={ \u0026#39;:new_version\u0026#39;: record_version, \u0026#39;:new_value\u0026#39;: Decimal(str(record_value)), \u0026#39;:new_hierarchy\u0026#39;: json.dumps(record_hierarchy, sort_keys = True), \u0026#39;:new_time\u0026#39;: Decimal(str(record_time)) } ) This method call constructs an UpdateItem operation with the following properties:\nA write will be done on the attributes listed in the UpdateExpression: The value, version, hierarchy, and timestamp will be set with the provided values in the ExpressionAttributeValues. The attribute names as defined in ExpressionAttributeNames are pulled from constants we defined outside of this function file. The value of the STATE_TABLE_KEY variable contains the partition key name, and the value of the attribute will be the record_id. Notably, this write has no conditional checks to prevent duplicate writes. Modify table.update_item statement to include conditional expression Now, let\u0026rsquo;s compare it to the following snippet below:\ntable.update_item( Key = { constants.STATE_TABLE_KEY: record_id }, ConditionExpression = \u0026#39;attribute_not_exists(\u0026#39; + constants.STATE_TABLE_KEY + \u0026#39;) OR \u0026#39; + constants.VERSION_COLUMN_NAME + \u0026#39;\u0026lt; :new_version\u0026#39;, UpdateExpression = \u0026#39;SET #VALUE = :new_value,\u0026#39; + \\ \u0026#39;#VERSION = :new_version,\u0026#39; + \\ \u0026#39;#HIERARCHY = :new_hierarchy,\u0026#39; + \\ \u0026#39;#TIMESTAMP = :new_time\u0026#39;, ExpressionAttributeNames={ \u0026#39;#VALUE\u0026#39;: constants.VALUE_COLUMN_NAME, \u0026#39;#VERSION\u0026#39;: constants.VERSION_COLUMN_NAME, \u0026#39;#HIERARCHY\u0026#39;: constants.HIERARCHY_COLUMN_NAME, \u0026#39;#TIMESTAMP\u0026#39;: constants.TIMESTAMP_COLUMN_NAME }, ExpressionAttributeValues={ \u0026#39;:new_version\u0026#39;: record_version, \u0026#39;:new_value\u0026#39;: Decimal(str(record_value)), \u0026#39;:new_hierarchy\u0026#39;: json.dumps(record_hierarchy, sort_keys = True), \u0026#39;:new_time\u0026#39;: Decimal(str(record_time)) } ) The code in line 5 adds a compound condition that ensures an item is only inserted if it doesn\u0026rsquo;t already exist, or if it does then then it should be replaced if the new item has a greater version number (e.g. is a newer version of the item). This is the simplified version of that condition expression:\nattribute_not_exists(\u0026lsquo;PRIMARY KEY\u0026rsquo;) OR \u0026lsquo;CURRENT VERSION\u0026rsquo; \u0026lt; \u0026lsquo;NEW VERSION\u0026rsquo;\nTo explain, the condition first states that at the moment of data insertion the table should not contain an item with partition key pk equal to the record_id or else the write should fail (see the attribute_not_exists function ), implying this is the first time such a item/message is inserted. Then, with the inclusion of the OR keyword the condition says that if a row is already present in the table and the version number of the row being inserted is greater than the current row then the write should succeed.\nYou don\u0026rsquo;t need to change anything in your Lambda code yet, this will come in just a minute if you read on.\nWhy does it work? This conditional expression allows us to detect and handle cases when a message is duplicated by the upstream components, or if some messages came out of order. These situations can occur if the upstream Lambda puts the same message into the Kinesis stream more than once, for example. In such cases a ConditionalCheckFailedException error is raised and no data is inserted into the database. Next we need to modify our code to correctly handle these errors as they are normal and expected now.\nCatch a ClientError exception and deploy the changes We want to avoid failing and subsequently restarting the Lambda function, so we will add a proper error handler in case we have an exception on the conditional write. It\u0026rsquo;s a best practice to check the name of an exception, e.g. e.response['Error']['Code'], to determine whether the exception is normal or indicates a deeper problem.\nThe following code snippet suppresses ConditionalCheckFailedException errors while raising an error for all other exceptions:\ntry: table.update_item( Key = { constants.STATE_TABLE_KEY: record_id }, ConditionExpression = \u0026#39;attribute_not_exists(\u0026#39; + constants.STATE_TABLE_KEY + \u0026#39;) OR \u0026#39; + constants.VERSION_COLUMN_NAME + \u0026#39;\u0026lt; :new_version\u0026#39;, UpdateExpression = \u0026#39;SET #VALUE = :new_value,\u0026#39; + \\ \u0026#39;#VERSION = :new_version,\u0026#39; + \\ \u0026#39;#HIERARCHY = :new_hierarchy,\u0026#39; + \\ \u0026#39;#TIMESTAMP = :new_time\u0026#39;, ExpressionAttributeNames={ \u0026#39;#VALUE\u0026#39;: constants.VALUE_COLUMN_NAME, \u0026#39;#VERSION\u0026#39;: constants.VERSION_COLUMN_NAME, \u0026#39;#HIERARCHY\u0026#39;: constants.HIERARCHY_COLUMN_NAME, \u0026#39;#TIMESTAMP\u0026#39;: constants.TIMESTAMP_COLUMN_NAME }, ExpressionAttributeValues={ \u0026#39;:new_version\u0026#39;: record_version, \u0026#39;:new_value\u0026#39;: Decimal(str(record_value)), \u0026#39;:new_hierarchy\u0026#39;: json.dumps(record_hierarchy, sort_keys = True), \u0026#39;:new_time\u0026#39;: Decimal(str(record_time)) } ) except ClientError as e: if e.response[\u0026#39;Error\u0026#39;][\u0026#39;Code\u0026#39;]==\u0026#39;ConditionalCheckFailedException\u0026#39;: print(\u0026#39;Conditional put failed.\u0026#39; + \\ \u0026#39; This is either a duplicate or a more recent version already arrived.\u0026#39;) print(\u0026#39;Id: \u0026#39;, record_id) print(\u0026#39;Hierarchy: \u0026#39;, record_hierarchy) print(\u0026#39;Value: \u0026#39;, record_value) print(\u0026#39;Version: \u0026#39;, record_version) print(\u0026#39;Timestamp: \u0026#39;, record_time) else: raise e Copy the code snippet above and replace it with the existing table.update_item(...) statement in your StateLambda function code. Then click on Deploy to apply the changes.\nThe above change will also help avoid duplicate writes when the Lambda service retries the StateLambda function after it has previously failed with a batch of incoming messages. With this change we avoid writing duplicates into StateTable which ensures we do not generate additional messages in the downstream StateTable DynamoDB stream.\nHow do you know you fixed it? Navigate to StateLambda and open Logs under the Monitor tab. Check the log messages by clicking on the hyperlinked LogStream cell and validate that you see the following string in the log lines: Conditional put failed. This is either a duplicate.... This message is produced by the exception handling code above. This tells us that the conditional expression is working as expected.\n"
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.5/3.5.3/",
	"title": "Step 3 - Query the employees table using the global secondary index with overloaded attributes",
	"tags": [],
	"description": "",
	"content": "You can query all the employees based in Washington state (WA) in the United States by running the following query_employees.py script, which includes code to query a table by using global secondary index overloading.\nif attribute == \u0026#39;name\u0026#39;: ke = Key(\u0026#39;GSI_1_PK\u0026#39;).eq(\u0026#39;root\u0026#39;) \u0026amp; Key(\u0026#39;GSI_1_SK\u0026#39;).eq(value) else: ke = Key(\u0026#39;GSI_1_PK\u0026#39;).eq(attribute + \u0026#34;#\u0026#34; + value) response = table.query( IndexName=\u0026#39;GSI_1\u0026#39;, KeyConditionExpression=ke ) Run the following python script to retrieve the employees based in Washington state (WA).\npython query_employees.py employees state \u0026#39;WA\u0026#39; The script should give you output that looks like the following.\nList of employees with WA in the attribute state: Employee name: Alice Beilby - hire date: 2014-12-03 Employee name: Alla Absalom - hire date: 2015-06-25 Employee name: Alvan Heliar - hire date: 2016-05-15 Employee name: Anders Galtone - hire date: 2015-12-22 Employee name: Ashil Hutchin - hire date: 2015-02-11 ... Employee name: Sula Prattin - hire date: 2014-01-11 Employee name: Vittoria Edelman - hire date: 2014-10-01 Employee name: Willie McCuthais - hire date: 2015-05-27 Total of employees: 46. Execution time: 0.13477110862731934 seconds You can try a different US state by changing the last parameter of the command. The following command queries all employees based in Texas.\npython query_employees.py employees state \u0026#39;TX\u0026#39; If you want to query other states, click here to open the list of US states with some data in the table\nUsing the same query, you can query the employees by job title. Run the following command as an example.\npython query_employees.py employees current_title \u0026#39;Software Engineer\u0026#39; The preceding command will give you the following results.\nList of employees with Software Engineer in the attribute current_title: Employee name: Alice Beilby - hire date: 2014-11-03 Employee name: Anetta Byrne - hire date: 2017-03-15 Employee name: Ardis Panting - hire date: 2015-08-06 Employee name: Chris Randals - hire date: 2016-10-27 Employee name: Constantine Barendtsen - hire date: 2016-06-10 Employee name: Eudora Janton - hire date: 2015-01-05 Employee name: Florella Allsep - hire date: 2015-03-31 Employee name: Horatius Trangmar - hire date: 2013-10-21 Employee name: Korey Daugherty - hire date: 2016-11-03 Employee name: Lenka Luquet - hire date: 2014-10-01 Employee name: Leonora Hyland - hire date: 2016-06-14 Employee name: Lucretia Ruffell - hire date: 2015-07-04 Employee name: Malcolm Adiscot - hire date: 2014-04-17 Employee name: Melodie Sebire - hire date: 2013-08-27 Employee name: Menard Ogborn - hire date: 2014-06-27 Employee name: Merwyn Petters - hire date: 2014-06-19 Employee name: Niels Buston - hire date: 2014-10-30 Employee name: Noelani Studde - hire date: 2015-03-30 Total of employees: 18. Execution time: 0.11937260627746582 seconds You also can try a different title, as shown in the following python command.\n1 python query_employees.py employees current_title \u0026#39;IT Support Manager\u0026#39; If you want to know the list of all the available titles, click here!\nUsing the same query with one change, you can query the employees by name, as shown in the following command.\npython query_employees.py employees name \u0026#39;Dale Marlin\u0026#39; The preceding command should give you the following result.\nList of employees with Dale Marlin in the attribute name: Employee name: Dale Marlin - hire date: 2014-10-19 Total of employees: 1. Execution time: 0.1274700164794922 seconds Summary Congratulations, you have completed this exercise and demonstrated how the global secondary index key overloading design can support multiple access patterns. Use this pattern to fit different entity types in the same DynamoDB table and retain the ability to query the data on different partition keys with global secondary indexes. In the next exercise, you will learn about Sparse Global Secondary Indexes!\n"
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.3/1.3.1/",
	"title": "Viewing Table Data",
	"tags": [],
	"description": "",
	"content": "First, go to the DynamoDB Console and click on Tables from the side menu.\nNext, choose the ProductCatalog table and click Explore table items on the top right to view the items.\nWe can see visually that the table has a Partition Key of Id (which is the Number type), no sort key, and there are 8 items in the table. Some items are Books and some items are Bicycles and some attributes like Id, Price, ProductCategory, and Title exist in every Item while other Category specific attributes like Authors or Colors exist only on some items.\nClick on the Id attribute 101 to pull up the Item editor for that Item. We can see and modify all the attributes for this item right from the console. Try changing the Title to \u0026ldquo;Book 101 Title New and Improved\u0026rdquo;. Click Add new attribute named Reviewers of the String set type and then clicking Insert a field twice to add a couple of entries to that set. When you\u0026rsquo;re done click Save changes\nYou can also use the Item editor in DynamoDB JSON notation (instead of the default Form based editor) by clicking JSON in the top right corner. This notation should look familiar if you already went through the Explore the DynamoDB CLI portion of the lab. The DynamoDB JSON format is described in the DynamoDB Low-Level API section of the Developer Guide.\n"
},
{
	"uri": "http://Handoo464.github.io/8-ldc/8.2/",
	"title": "Bank Payments Scenario",
	"tags": [],
	"description": "",
	"content": "A bank has asked you to develop a new backend system to handle their scheduled payments.\nThis is primarily an OLTP workload with daily batch processes. Items in the table(s) represent payments that are scheduled between accounts. As items are inserted, they are scheduled on a specific date to have the payment processed. Each day, items are regularly sent to a transactional system for processing, at which time their status changes to PENDING. Upon a successful transaction, an item’s status is set to PROCESSED and updated with a new transaction ID.\nWorkload dimensions Accounts can have multiple payments scheduled for any day in the future. Payments have the following data fields: AccountID, ScheduledTime, Status (SCHEDULED, PENDING or PROCESSED), DataBlob (total item size is \u0026lt;= 8 KB) One million automated scheduled payments are added every day at 1:00 AM for that day, which need to complete in 30 minutes. One million payments are added every day with the SCHEDULED state, mostly in the hours of 6 AM to 6 PM. During the day, a batch job runs regularly to query for today\u0026rsquo;s SCHEDULED payments. This service sends the SCHEDULED items to the transaction service. Upon sending the items to the transaction service, the payment status is changed to PENDING. When the transaction service completes, the status of an item is changed to PROCESSED and a new transaction ID is added to the item. Items need to be returned for a specific account that are scheduled for payment in the next 90 days. The transactional service has to retrieve all items for a specific date (for example, today) across all accounts. It has to be able to retrieve items that are specifically SCHEDULED or PENDING. Your Challenge: Develop a NoSQL data model for the bank that fulfills the scheduled payments requirements.\nBonus challenge: At the end of each day, all of the items that were PROCESSED need to be moved to a long term table (due to compliance the data needs to be in a separate table). Design a second data model that fulfills that same access requirements as above, and add another requirement to return a specific item associated with a transaction ID.\n"
},
{
	"uri": "http://Handoo464.github.io/7-lgme/7.2/7.2.2/",
	"title": "Build your entity-relationship diagram",
	"tags": [],
	"description": "",
	"content": "The first step of any data modeling exercise is to build a diagram to show the entities in your application and how they relate to each other.\nIn the application, you have the following entities:\nUser\nGame\nUserGameMapping\nA User entity represents a user in the application. A user can create multiple Game entities, and the creator of a game will determine which map is played and when the game starts. A User can create multiple Game entities, so there is a one-to-many relationship between Users and Games.\nFinally, a Game contains multiple Users and a User can play in multiple different Games over time.\nThus, there is a many-to-many relationship between Users and Games. You can represent this relationship with the UserGameMapping entity.\nWith these entities and relationships in mind, the entity-relationship diagram is shown below.\nNext, we will take a look at the access patterns the data model needs to support.\n"
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.5/1.5.2/",
	"title": "Configure MySQL Environment",
	"tags": [],
	"description": "",
	"content": "This chapter will create source environment on AWS as discussed during Exercise Overview. The CloudFormation template used below will create Source VPC, EC2 hosting MySQL server, IMDb database and load IMDb public dataset into 6 tables.\nLaunch the CloudFormation template in US West 2 to deploy the resources in your account: Optionally, download the YAML template and launch it your own way Click Next Confirm the Stack Name rdbmsmigration and update parameters if necessary (leave the default options if at all possible) Click “Next” twice then check “I acknowledge that AWS CloudFormation might create IAM resources with custom names.” Click \u0026ldquo;Submit\u0026rdquo; The CloudFormation stack will take about 5 minutes to build the environment Go to EC2 Dashboard and ensure the Status check column is 2/2 checks passed before moving to the next step. Do not continue unless the MySQL instance is passing both health checks, 2/2.\n"
},
{
	"uri": "http://Handoo464.github.io/7-lgme/7.4/7.4.2/",
	"title": "Create a sparse GSI",
	"tags": [],
	"description": "",
	"content": "In this step, you create the sparse global secondary index (GSI) for open games (games that are not full already).\nCreating a GSI is similar to creating a table. In the code you downloaded, you will find a script file in the scripts/ directory named add_secondary_index.py.\nimport boto3 dynamodb = boto3.client(\u0026#39;dynamodb\u0026#39;) try: dynamodb.update_table( TableName=\u0026#39;battle-royale\u0026#39;, AttributeDefinitions=[ { \u0026#34;AttributeName\u0026#34;: \u0026#34;map\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34; }, { \u0026#34;AttributeName\u0026#34;: \u0026#34;open_timestamp\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34; } ], GlobalSecondaryIndexUpdates=[ { \u0026#34;Create\u0026#34;: { \u0026#34;IndexName\u0026#34;: \u0026#34;OpenGamesIndex\u0026#34;, \u0026#34;KeySchema\u0026#34;: [ { \u0026#34;AttributeName\u0026#34;: \u0026#34;map\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;HASH\u0026#34; }, { \u0026#34;AttributeName\u0026#34;: \u0026#34;open_timestamp\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;RANGE\u0026#34; } ], \u0026#34;Projection\u0026#34;: { \u0026#34;ProjectionType\u0026#34;: \u0026#34;ALL\u0026#34; }, \u0026#34;ProvisionedThroughput\u0026#34;: { \u0026#34;ReadCapacityUnits\u0026#34;: 1, \u0026#34;WriteCapacityUnits\u0026#34;: 1 } } } ], ) print(\u0026#34;Table \u0026#39;battle-royale\u0026#39; updated successfully.\u0026#34;) except Exception as e: print(\u0026#34;Could not update table. Error:\u0026#34;) print(e) Change the capacity units\nEdit scripts/add_secondary_index.py, set both ReadCapacityUnits and WriteCapacityUnits to 100 for OpenGamesIndex. Then, save the file.\nWhenever attributes are used in a primary key for a table or secondary index, they must be defined in AttributeDefinitions. Then, you Create a new GSI in the GlobalSecondaryIndexUpdates property. For this GSI, you specify the index name, the schema of the primary key, the provisioned throughput, and the attributes you want to project.\nNote that you do not have to specify that the GSI is intended to be used as a sparse index. That is purely a function of the data you put in. If you write items to your table that do not have the attributes for your secondary indexes, they will not be included in your secondary index.\nYou can choose to run either the add_secondary_index.py python script or the AWS CLI command below. Both are provided to show different methods of interacting with DynamoDB.\nCreate your global secondary index (GSI) by running the following command:\npython scripts/add_secondary_index.py You should see the following message in the console:\nTable \u0026#39;battle-royale\u0026#39; updated successfully. Alternatively, you can create the global secondary index (GSI) by running the AWS CLI command below.\naws dynamodb update-table \\ --table-name battle-royale \\ --attribute-definitions AttributeName=map,AttributeType=S AttributeName=open_timestamp,AttributeType=S \\ --global-secondary-index-updates \\ \u0026#34;[ { \\\u0026#34;Create\\\u0026#34;: { \\\u0026#34;IndexName\\\u0026#34;: \\\u0026#34;OpenGamesIndex\\\u0026#34;, \\\u0026#34;KeySchema\\\u0026#34;: [ { \\\u0026#34;AttributeName\\\u0026#34;: \\\u0026#34;map\\\u0026#34;, \\\u0026#34;KeyType\\\u0026#34;: \\\u0026#34;HASH\\\u0026#34; }, { \\\u0026#34;AttributeName\\\u0026#34;: \\\u0026#34;open_timestamp\\\u0026#34;, \\\u0026#34;KeyType\\\u0026#34;: \\\u0026#34;RANGE\\\u0026#34; } ], \\\u0026#34;Projection\\\u0026#34;: { \\\u0026#34;ProjectionType\\\u0026#34;: \\\u0026#34;ALL\\\u0026#34; }, \\\u0026#34;ProvisionedThroughput\\\u0026#34;: { \\\u0026#34;ReadCapacityUnits\\\u0026#34;: 100, \\\u0026#34;WriteCapacityUnits\\\u0026#34;: 100 } } } ]\u0026#34; If you chose to run the AWS CLI command, you should see output like this:\nNotice that the TableStatus will show as UPDATING and the IndexStatus will show as CREATING\n{ \u0026#34;TableDescription\u0026#34;: { \u0026#34;AttributeDefinitions\u0026#34;: [ { \u0026#34;AttributeName\u0026#34;: \u0026#34;PK\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34; }, { \u0026#34;AttributeName\u0026#34;: \u0026#34;SK\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34; }, { \u0026#34;AttributeName\u0026#34;: \u0026#34;map\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34; }, { \u0026#34;AttributeName\u0026#34;: \u0026#34;open_timestamp\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34; } ], \u0026#34;TableName\u0026#34;: \u0026#34;battle-royale\u0026#34;, \u0026#34;KeySchema\u0026#34;: [ { \u0026#34;AttributeName\u0026#34;: \u0026#34;PK\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;HASH\u0026#34; }, { \u0026#34;AttributeName\u0026#34;: \u0026#34;SK\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;RANGE\u0026#34; } ], \u0026#34;TableStatus\u0026#34;: \u0026#34;UPDATING\u0026#34;, \u0026#34;CreationDateTime\u0026#34;: \u0026#34;2023-12-06T14:48:31.246000-06:00\u0026#34;, \u0026#34;ProvisionedThroughput\u0026#34;: { \u0026#34;NumberOfDecreasesToday\u0026#34;: 0, \u0026#34;ReadCapacityUnits\u0026#34;: 100, \u0026#34;WriteCapacityUnits\u0026#34;: 100 }, \u0026#34;TableSizeBytes\u0026#34;: 0, \u0026#34;ItemCount\u0026#34;: 0, \u0026#34;TableArn\u0026#34;: \u0026#34;arn:aws:dynamodb:\u0026lt;AWS Region\u0026gt;:\u0026lt;Account ID\u0026gt;:table/battle-royale\u0026#34;, \u0026#34;TableId\u0026#34;: \u0026#34;\u0026lt;Unique ID\u0026gt;\u0026#34;, \u0026#34;BillingModeSummary\u0026#34;: { \u0026#34;BillingMode\u0026#34;: \u0026#34;PROVISIONED\u0026#34;, \u0026#34;LastUpdateToPayPerRequestDateTime\u0026#34;: \u0026#34;2023-12-07T11:11:34.932000-06:00\u0026#34; }, \u0026#34;GlobalSecondaryIndexes\u0026#34;: [ { \u0026#34;IndexName\u0026#34;: \u0026#34;OpenGamesIndex\u0026#34;, \u0026#34;KeySchema\u0026#34;: [ { \u0026#34;AttributeName\u0026#34;: \u0026#34;map\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;HASH\u0026#34; }, { \u0026#34;AttributeName\u0026#34;: \u0026#34;open_timestamp\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;RANGE\u0026#34; } ], \u0026#34;Projection\u0026#34;: { \u0026#34;ProjectionType\u0026#34;: \u0026#34;ALL\u0026#34; }, \u0026#34;IndexStatus\u0026#34;: \u0026#34;CREATING\u0026#34;, \u0026#34;Backfilling\u0026#34;: false, \u0026#34;ProvisionedThroughput\u0026#34;: { \u0026#34;NumberOfDecreasesToday\u0026#34;: 0, \u0026#34;ReadCapacityUnits\u0026#34;: 100, \u0026#34;WriteCapacityUnits\u0026#34;: 100 }, \u0026#34;IndexSizeBytes\u0026#34;: 0, \u0026#34;ItemCount\u0026#34;: 0, \u0026#34;IndexArn\u0026#34;: \u0026#34;arn:aws:dynamodb:\u0026lt;AWS Region\u0026gt;:\u0026lt;Account ID\u0026gt;:table/battle-royale/index/OpenGamesIndex\u0026#34; } ], \u0026#34;DeletionProtectionEnabled\u0026#34;: false } } It will take a few minutes for the new GSI to get populated. You need to wait until the GSI is active. You can find out the current status of the table and its indexes by either way:\nChecking under Services, Database, DynamoDB in the AWS console.\nRunning the command below in the Cloud9 Terminal:\naws dynamodb describe-table --table-name battle-royale --query \u0026#34;Table.GlobalSecondaryIndexes[].IndexStatus\u0026#34; You also can script the command to run every 5 seconds using watch.\n# Watch checks every 5 seconds by default watch -n 5 \u0026#34;aws dynamodb describe-table --table-name battle-royale --query \\\u0026#34;Table.GlobalSecondaryIndexes[].IndexStatus\\\u0026#34;\u0026#34; Press Ctrl + C to end watch after the global secondary index has been created.\n"
},
{
	"uri": "http://Handoo464.github.io/4-lcdc/4.3/4.3.2/",
	"title": "Create Dead Letter Queue",
	"tags": [],
	"description": "",
	"content": "If the lambda function is not able to successfully process a record it receives from DynamoDB stream, the Lambda service should write the metadata for the error record to a dead letter queue (DLQ) so the reason for the failure can be investigated and resolved.\nSo create an Amazon SQS Dead Letter Queue named orders-ddbs-dlq for your lambda function trigger using the AWS CLI command below.\naws sqs create-queue --queue-name orders-ddbs-dlq Sample output:\n{ \u0026#34;QueueUrl\u0026#34;: \u0026#34;https://sqs.{aws-region}.amazonaws.com/{aws-account-id}/orders-ddbs-dlq\u0026#34; } Later you will need the queue ARN. Use the below command, modifying the queue URL after \u0026ndash;queue-url to match the result of the previous command, and then save the ARN for later use.\naws sqs get-queue-attributes --attribute-names \u0026#34;QueueArn\u0026#34; --query \u0026#39;Attributes.QueueArn\u0026#39; --output text \\ --queue-url \u0026#34;https://sqs.{aws-region}.amazonaws.com/{aws-account-id}/orders-ddbs-dlq\u0026#34; "
},
{
	"uri": "http://Handoo464.github.io/4-lcdc/4.4/4.4.2/",
	"title": "Create Dead Letter Queue",
	"tags": [],
	"description": "",
	"content": "If the lambda function is not able to successfully process any record it receives from Amazon Kinesis, the Lambda service should write the metadata for the error record to a dead letter queue (DLQ) so the reason for the failure can be investigated and resolved.\nSo create an Amazon SQS Dead Letter Queue named orders-kds-dlq for your lambda function trigger using the AWS CLI command below.\naws sqs create-queue --queue-name orders-kds-dlq Sample output:\n{ \u0026#34;QueueUrl\u0026#34;: \u0026#34;https://sqs.{aws-region}.amazonaws.com/{aws-account-id}/orders-kds-dlq\u0026#34; } As before you will need the queue ARN. Use the below command, modifying the queue URL after \u0026ndash;queue-url to match the result of the previous command.\naws sqs get-queue-attributes --attribute-names \u0026#34;QueueArn\u0026#34; --query \u0026#39;Attributes.QueueArn\u0026#39; --output text \\ --queue-url \u0026#34;https://sqs.{aws-region}.amazonaws.com/{aws-account-id}/orders-kds-dlq\u0026#34; "
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.1/1.1.2/",
	"title": "Create the DynamoDB Tables",
	"tags": [],
	"description": "",
	"content": "You can access the session manager, enter the command sudo su, then execute the commands. Alternatively, you can access Cloud9 and execute the following commands.\nWe will now create tables (and in a subsequent step load data into them) based on sample data from the DynamoDB Developer Guide .\nCopy the create-table commands below and paste them into your command prompt, hitting enter on the last command to execute it. Then use the corresponding wait commands by pasting them into the same terminal and running them.\naws dynamodb create-table \\ --table-name ProductCatalog \\ --attribute-definitions \\ AttributeName=Id,AttributeType=N \\ --key-schema \\ AttributeName=Id,KeyType=HASH \\ --provisioned-throughput \\ ReadCapacityUnits=10,WriteCapacityUnits=5 \\ --query \u0026#34;TableDescription.TableStatus\u0026#34; aws dynamodb create-table \\ --table-name Forum \\ --attribute-definitions \\ AttributeName=Name,AttributeType=S \\ --key-schema \\ AttributeName=Name,KeyType=HASH \\ --provisioned-throughput \\ ReadCapacityUnits=10,WriteCapacityUnits=5 \\ --query \u0026#34;TableDescription.TableStatus\u0026#34; aws dynamodb create-table \\ --table-name Thread \\ --attribute-definitions \\ AttributeName=ForumName,AttributeType=S \\ AttributeName=Subject,AttributeType=S \\ --key-schema \\ AttributeName=ForumName,KeyType=HASH \\ AttributeName=Subject,KeyType=RANGE \\ --provisioned-throughput \\ ReadCapacityUnits=10,WriteCapacityUnits=5 \\ --query \u0026#34;TableDescription.TableStatus\u0026#34; aws dynamodb create-table \\ --table-name Reply \\ --attribute-definitions \\ AttributeName=Id,AttributeType=S \\ AttributeName=ReplyDateTime,AttributeType=S \\ --key-schema \\ AttributeName=Id,KeyType=HASH \\ AttributeName=ReplyDateTime,KeyType=RANGE \\ --provisioned-throughput \\ ReadCapacityUnits=10,WriteCapacityUnits=5 \\ --query \u0026#34;TableDescription.TableStatus\u0026#34; Run the wait commands. When they all run and end, you can move on:\naws dynamodb wait table-exists --table-name ProductCatalog aws dynamodb wait table-exists --table-name Forum aws dynamodb wait table-exists --table-name Thread aws dynamodb wait table-exists --table-name Reply "
},
{
	"uri": "http://Handoo464.github.io/7-lgme/7.3/7.3.2/",
	"title": "Create the table",
	"tags": [],
	"description": "",
	"content": "Now that the primary key is designed, let’s create a table.\nThe code you downloaded in the initial steps include a Python script in the scripts/ directory named create_table.py. The Python script’s contents follow.\nimport boto3 dynamodb = boto3.client(\u0026#39;dynamodb\u0026#39;) try: dynamodb.create_table( TableName=\u0026#39;battle-royale\u0026#39;, AttributeDefinitions=[ { \u0026#34;AttributeName\u0026#34;: \u0026#34;PK\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34; }, { \u0026#34;AttributeName\u0026#34;: \u0026#34;SK\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34; } ], KeySchema=[ { \u0026#34;AttributeName\u0026#34;: \u0026#34;PK\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;HASH\u0026#34; }, { \u0026#34;AttributeName\u0026#34;: \u0026#34;SK\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;RANGE\u0026#34; } ], ProvisionedThroughput={ \u0026#34;ReadCapacityUnits\u0026#34;: 1, \u0026#34;WriteCapacityUnits\u0026#34;: 1 } ) print(\u0026#34;Table \u0026#39;battle-royale\u0026#39; created successfully.\u0026#34;) except Exception as e: print(\u0026#34;Could not create table. Error:\u0026#34;) print(e) Change the capacity units.\nEdit scripts/create_table.py, set both ReadCapacityUnits and WriteCapacityUnits to 100 for battle-royale table and save the file.\nThe preceding script uses the CreateTable operation using Boto 3 , the AWS SDK for Python. The operation declares two attribute definitions, which are typed attributes to be used in the primary key. Though DynamoDB is schemaless , you must declare the names and types of attributes that are used for primary keys. The attributes must be included on every item that is written to the table and thus must be specified as you are creating a table.\nBecause different entities are stored in a single table, you can’t use primary key attribute names such as UserId. The attribute means something different based on the type of entity being stored. For example, the primary key for a user might be its USERNAME, and the primary key for a game might be its GAMEID. Accordingly, you use generic names for the attributes, such as PK (for partition key) and SK (for sort key).\nAfter configuring the attributes in the key schema, you specify the provisioned throughput for the table. DynamoDB has two capacity modes: provisioned and on-demand. In provisioned capacity mode, you specify exactly the amount of read and write throughput you want. You pay for this capacity whether you use it or not.\nIn DynamoDB on-demand capacity mode, you pay per request. The cost per request is slightly higher than if you were to use provisioned throughput fully, but you don’t have to spend time doing capacity planning or worrying about getting throttled. On-demand mode works great for spiky or unpredictable workloads. In this lab, provisioned capacity mode is used.\nYou can choose to run either the create_table.py python script or the AWS CLI command below. Both are provided to show different methods of interacting with DynamoDB.\nYou can run the Python script with the following command in the Cloud9 Terminal.\npython scripts/create_table.py The script should return this message:\nTable \u0026#39;battle-royale\u0026#39; created successfully. As an alternative, you can run the AWS CLI command from your Cloud9 Terminal.\naws dynamodb create-table \\ --table-name battle-royale \\ --attribute-definitions AttributeName=PK,AttributeType=S AttributeName=SK,AttributeType=S \\ --key-schema AttributeName=PK,KeyType=HASH AttributeName=SK,KeyType=RANGE \\ --provisioned-throughput ReadCapacityUnits=100,WriteCapacityUnits=100 The CLI command should return this JSON:\n{ \u0026#34;TableDescription\u0026#34;: { \u0026#34;AttributeDefinitions\u0026#34;: [ { \u0026#34;AttributeName\u0026#34;: \u0026#34;PK\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34; }, { \u0026#34;AttributeName\u0026#34;: \u0026#34;SK\u0026#34;, \u0026#34;AttributeType\u0026#34;: \u0026#34;S\u0026#34; } ], \u0026#34;TableName\u0026#34;: \u0026#34;battle-royale\u0026#34;, \u0026#34;KeySchema\u0026#34;: [ { \u0026#34;AttributeName\u0026#34;: \u0026#34;PK\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;HASH\u0026#34; }, { \u0026#34;AttributeName\u0026#34;: \u0026#34;SK\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;RANGE\u0026#34; } ], \u0026#34;TableStatus\u0026#34;: \u0026#34;CREATING\u0026#34;, \u0026#34;CreationDateTime\u0026#34;: \u0026#34;2023-12-06T13:52:52.187000-06:00\u0026#34;, \u0026#34;ProvisionedThroughput\u0026#34;: { \u0026#34;NumberOfDecreasesToday\u0026#34;: 0, \u0026#34;ReadCapacityUnits\u0026#34;: 1, \u0026#34;WriteCapacityUnits\u0026#34;: 1 }, \u0026#34;TableSizeBytes\u0026#34;: 0, \u0026#34;ItemCount\u0026#34;: 0, \u0026#34;TableArn\u0026#34;: \u0026#34;arn:aws:dynamodb:\u0026lt;AWS Region\u0026gt;:\u0026lt;Account ID\u0026gt;:table/battle-royale\u0026#34;, \u0026#34;TableId\u0026#34;: \u0026#34;\u0026lt;Unique Identifier\u0026gt;\u0026#34;, \u0026#34;DeletionProtectionEnabled\u0026#34;: false } } "
},
{
	"uri": "http://Handoo464.github.io/2-lbed/2.3/2.3.2/",
	"title": "Create the zero-ETL Pipeline",
	"tags": [],
	"description": "",
	"content": "Amazon DynamoDB offers a zero-ETL integration with Amazon OpenSearch Service through the DynamoDB plugin for OpenSearch Ingestion. Amazon OpenSearch Ingestion offers a fully managed, no-code experience for ingesting data into Amazon OpenSearch Service.\nOpen OpenSearch Service Ingestion Pipelines Click \u0026ldquo;Create pipeline\u0026rdquo;\nName your pipeline, and include the following for your pipeline configuration. The configuration contains multiple values that need to be updated. The needed values are provided in the CloudFormation Stack Outputs as \u0026ldquo;Region\u0026rdquo;, \u0026ldquo;Role\u0026rdquo;, \u0026ldquo;S3Bucket\u0026rdquo;, \u0026ldquo;DdbTableArn\u0026rdquo;, and \u0026ldquo;OSDomainEndpoint\u0026rdquo;.\nversion: \u0026#34;2\u0026#34; dynamodb-pipeline: source: dynamodb: acknowledgments: true tables: # REQUIRED: Supply the DynamoDB table ARN - table_arn: \u0026#34;{DDB_TABLE_ARN}\u0026#34; stream: start_position: \u0026#34;LATEST\u0026#34; export: # REQUIRED: Specify the name of an existing S3 bucket for DynamoDB to write export data files to s3_bucket: \u0026#34;{S3BUCKET}\u0026#34; # REQUIRED: Specify the region of the S3 bucket s3_region: \u0026#34;{REGION}\u0026#34; # Optionally set the name of a prefix that DynamoDB export data files are written to in the bucket. s3_prefix: \u0026#34;pipeline\u0026#34; aws: # REQUIRED: Provide the role to assume that has the necessary permissions to DynamoDB, OpenSearch, and S3. sts_role_arn: \u0026#34;{ROLE}\u0026#34; # REQUIRED: Provide the region region: \u0026#34;{REGION}\u0026#34; sink: - opensearch: hosts: # REQUIRED: Provide an AWS OpenSearch endpoint, including https:// [ \u0026#34;{OS_DOMAIN_ENDPOINT}\u0026#34; ] index: \u0026#34;product-details-index-en\u0026#34; index_type: custom template_type: \u0026#34;index-template\u0026#34; template_content: | { \u0026#34;template\u0026#34;: { \u0026#34;settings\u0026#34;: { \u0026#34;index.knn\u0026#34;: true, \u0026#34;default_pipeline\u0026#34;: \u0026#34;product-en-nlp-ingest-pipeline\u0026#34; }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;ProductID\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;ProductName\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;Category\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;Description\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;Image\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;combined_field\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;product_embedding\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;knn_vector\u0026#34;, \u0026#34;dimension\u0026#34;: 1536, \u0026#34;method\u0026#34;: { \u0026#34;engine\u0026#34;: \u0026#34;nmslib\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;hnsw\u0026#34;, \u0026#34;space_type\u0026#34;: \u0026#34;l2\u0026#34; } } } } } } aws: # REQUIRED: Provide the role to assume that has the necessary permissions to DynamoDB, OpenSearch, and S3. sts_role_arn: \u0026#34;{ROLE}\u0026#34; # REQUIRED: Provide the region region: \u0026#34;{REGION}\u0026#34; Under Network, select \u0026ldquo;Public access\u0026rdquo;, then click \u0026ldquo;Next\u0026rdquo;.\nClick \u0026ldquo;Create pipeline\u0026rdquo;.\nWait until the pipeline has finished creating. This will take 5 minutes or more.\nAfter the pipeline is created, it will take some additional time for the initial export from DynamoDB and import into OpenSearch Service. After you have waited several more minutes, you can check if items have replicated into OpenSearch by making a query in Dev Tools in the OpenSearch Dashboards.\nTo open Dev Tools, click on the menu in the top left of OpenSearch Dashboards, scroll down to the Management section, then click on Dev Tools. Enter the following query in the left pane, then click the \u0026ldquo;play\u0026rdquo; arrow.\nGET /product-details-index-en/_search You may encounter a few types of results:\nIf you see a 404 error of type index_not_found_exception, then you need to wait until the pipeline is Active. Once it is, this exception will go away. If your query does not have results, wait a few more minutes for the initial replication to finish and try again. Only continue once you see a return like the above, with a response body. Your hits may vary.\n"
},
{
	"uri": "http://Handoo464.github.io/2-lbed/2.2/2.2.2/",
	"title": "Enable Amazon Bedrock Models",
	"tags": [],
	"description": "",
	"content": "Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon via a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI.\nIn this application, Bedrock will be used to make natural language product recommendation queries using OpenSearch Service as a vector database.\nBedrock requires different FMs to be enabled before they are used.\nOpen Amazon Bedrock Model Access Click on \u0026ldquo;Manage model access\u0026rdquo;\nSelect \u0026ldquo;Titan Embeddings G1 - Text\u0026rdquo; and \u0026ldquo;Claude\u0026rdquo;, then click Request model access\nWait until you are granted access to both models before continuing. The Access status should say Access granted before moving on.\nDo not continue unless the base models \u0026ldquo;Claude\u0026rdquo; and \u0026ldquo;Titan Embeddings G1 - Text\u0026rdquo; are granted to your account.\n"
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.2/",
	"title": "Exercise 1: DynamoDB Capacity Units and Partitioning",
	"tags": [],
	"description": "",
	"content": "In this exercise, you load data into DynamoDB tables that are provisioned with different write/read capacity units, and compare the load times for different datasets. First, you load a smaller dataset into a table and note the quick execution times. Next, you load a larger dataset into an underprovisioned table to simulate throttling exceptions. Finally, you simulate the global secondary index back pressure on a table by creating a table with higher provisioning and a global secondary index with only 1 write capacity unit (WCU). In this exercise, you use sample web server access log data, similiar to the web server log data generated by Apache.\n"
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.2/",
	"title": "Explore DynamoDB with the CLI",
	"tags": [],
	"description": "",
	"content": "We will be exploring DynamoDB with the AWS CLI using the AWS cloud9 management Console or Session manager\nThe highest level of abstraction in DynamoDB is a Table (there isn\u0026rsquo;t a concept of a \u0026ldquo;Database\u0026rdquo; that has a bunch of tables inside of it like in other NOSQL or RDBMS services). Inside of a Table you will insert Items, which are analogous to what you might think of as a row in other services. Items are a collection of Attributes, which are analogous to columns. Every item must have a Primary Key which will uniquely identify that row (two items may not contain the same Primary Key). At a minimum when you create a table you must choose an attribute to be the Partition Key (aka the Hash Key) and you can optionally specify another attribute to be the Sort Key.\nIf your table is a Partition Key only table, then the Partition Key is the Primary Key and must uniquely identify each item. If your table has both a Partition Key and Sort Key, then it is possible to have multiple items that have the same Partition Key, but the combination of the Partition Key and Sort Key will be the Primary Key and uniquely identify the row. In other words, you can have multiple items that have the same Partition Key as long as their Sort Keys are different. Items with the same Partition Key are said to belong to the same Item Collection.\nFor more information please read about Core Concepts in DynamoDB .\nOperations in DynamoDB consume capacity from the table. When the table is using On-Demand capacity, read operations will consume Read Request Units (RRUs) and write operations will consume Write Request Units (WRUs). When the table is using Provisioned Capacity, read operations will consume Read Capacity Units (RCUs) and write operations will consume Write Capacity Units (WCUs). For more information please see the Read/Write Capacity Mode in the DynamoDB Developer Guide.\nNow lets dive into the shell and explore DynamoDB with the AWS CLI.\n"
},
{
	"uri": "http://Handoo464.github.io/2-lbed/",
	"title": "LBED: Generative AI with DynamoDB zero-ETL to OpenSearch integration and Amazon Bedrock",
	"tags": [],
	"description": "",
	"content": "In this workshop you will have a hands on experience setting up DynamoDB zero-ETL integration with Amazon OpenSearch Service to faciliate a natural language query of a product catalog. You will create a pipeline from a DynamoDB table to OpenSearch Service, create an Amazon Bedrock Connector in OpenSearch Service, and query Bedrock leveraging OpenSearch Service as a vector store. At the end of this lesson, you should feel confident in your ability to integrate DynamoDB with OpenSearch Service to support context aware reasoning applications.\nPairing Amazon DynamoDB with Amazon OpenSearch Service is a common architecture pattern for applications that need to combine the high scalability and performance of DynamoDB for transactional workloads with the powerful search and analytics capabilities of OpenSearch.\nDynamoDB is a NoSQL database designed for high availability, performance, and scalability and focused on key/value operations. OpenSearch Service provides advanced search features such as full-text search, faceted search, and complex querying capabilities. Combined, these two services can satisfy a wide variety of application use cases.\nThis workshop will allow you to set up one such use case. DynamoDB will be the source of truth for product catalog information and OpenSearch will provide vector search capabilities to enable Amazon Bedrock (a generative AI service) to make product recommendations.\nThis lab creates OpenSearch Service, DynamoDB, and Secrets Manager resources. If running in you own account, these resources will incur charges of approximately $30 a month. Remember to delete the CloudFormation Stack after completing the lab.\n"
},
{
	"uri": "http://Handoo464.github.io/4-lcdc/4.2/4.2.2/",
	"title": "Load Sample Data",
	"tags": [],
	"description": "",
	"content": "Copy the JSON data below in to a file named Orders.json.\n{ \u0026#34;Orders\u0026#34;: [ { \u0026#34;PutRequest\u0026#34;: { \u0026#34;Item\u0026#34;: { \u0026#34;customer\u0026#34;: { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;799102280\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Salma Otero\u0026#34; }, \u0026#34;address\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;22 Milton Road, Exeter,EX2 6BN\u0026#34; }, \u0026#34;phone\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;+441482133202\u0026#34; } } }, \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;4514280\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;PLACED\u0026#34; }, \u0026#34;items\u0026#34;: { \u0026#34;L\u0026#34;: [ { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;23884750\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Metallic Long-Wear Cream Shadow\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;13\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;PURCHASED\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£15.00\u0026#34; } } }, { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;23699354\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Eye Liner\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;8\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;PURCHASED\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£9.00\u0026#34; } } }, { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;23599030\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Bronzing Powder\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;10\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;PURCHASED\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£12.00\u0026#34; } } } ] }, \u0026#34;orderDate\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;2023-10-01 01:05:54\u0026#34; }, \u0026#34;shipDate\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;2023-10-04 18:54:12\u0026#34; } } } }, { \u0026#34;PutRequest\u0026#34;: { \u0026#34;Item\u0026#34;: { \u0026#34;customer\u0026#34;: { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;941852721\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Taylor Burnette\u0026#34; }, \u0026#34;address\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;31 Walkhampton Avenue, Bradwell Common,MK13 8ND\u0026#34; }, \u0026#34;phone\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;+441663724681\u0026#34; } } }, \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;9844720\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;PLACED\u0026#34; }, \u0026#34;items\u0026#34;: { \u0026#34;L\u0026#34;: [ { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;24002126\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Shimmer Wash Eye Shadow\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;10\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;PURCHASED\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£13.00\u0026#34; } } }, { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;23607685\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Buffing Grains for Face\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;11\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;PURCHASED\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£8.00\u0026#34; } } } ] }, \u0026#34;orderDate\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;2023-10-01 01:49:13\u0026#34; }, \u0026#34;shipDate\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;2023-10-06 13:05:33\u0026#34; } } } }, { \u0026#34;PutRequest\u0026#34;: { \u0026#34;Item\u0026#34;: { \u0026#34;customer\u0026#34;: { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;558490551\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Brody Dent\u0026#34; }, \u0026#34;address\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;3 Bailey Lane, Clenchwarton,PE34 4AY\u0026#34; }, \u0026#34;phone\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;+441268381612\u0026#34; } } }, \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;6421680\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;PLACED\u0026#34; }, \u0026#34;items\u0026#34;: { \u0026#34;L\u0026#34;: [ { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;23769901\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Hydrating Face Cream\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;8\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;PURCHASED\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£12.00\u0026#34; } } }, { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;23673445\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;EXTRA Repair Serum\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;5\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;PURCHASED\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£10.00\u0026#34; } } } ] }, \u0026#34;orderDate\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;2023-10-01 20:39:08\u0026#34; }, \u0026#34;shipDate\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;2023-10-04 16:29:36\u0026#34; } } } }, { \u0026#34;PutRequest\u0026#34;: { \u0026#34;Item\u0026#34;: { \u0026#34;customer\u0026#34;: { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;242903240\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Julia Caswell\u0026#34; }, \u0026#34;address\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;81 Alwyn Road, Darlington,DL3 0AS\u0026#34; }, \u0026#34;phone\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;+441305066386\u0026#34; } } }, \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;9953371\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;PLACED\u0026#34; }, \u0026#34;items\u0026#34;: { \u0026#34;L\u0026#34;: [ { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;23924636\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Protective Face Lotion\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;9\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;PURCHASED\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£3.00\u0026#34; } } }, { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;23514506\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Nail File\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;13\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;PURCHASED\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£11.00\u0026#34; } } }, { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;23508704\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Kitten Heels Powder Finish Foot Creme\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;10\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;PURCHASED\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£11.00\u0026#34; } } } ] }, \u0026#34;orderDate\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;2023-10-01 00:21:53\u0026#34; }, \u0026#34;shipDate\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;2023-10-05 11:48:21\u0026#34; } } } } ] } Load the sample data into the Orders table using the batch-write-item AWS CLI command.\naws dynamodb batch-write-item --request-items file://Orders.json A successful load should produce a message similar to the one below.\nSample output { \u0026#34;UnprocessedItems\u0026#34;: {} } "
},
{
	"uri": "http://Handoo464.github.io/5-lmr/5.2/",
	"title": "Module 1: Deploy the backend resources",
	"tags": [],
	"description": "",
	"content": "You can access EC2 Instance or Cloud9 to perform the setup steps.\nSetup Steps This lab requires a terminal shell with Python3 and the AWS Command Line Interface (CLI) installed and configured with admin credentials.\nTo set up the environment for an EC2 Instance: Access EC2 Select the Instance and connect using the session manager To set up your AWS Cloud9 development environment: Choose Services at the top of the page, and then choose Cloud9 under Developer Tools.\nThere will be an environment ready to use under My environments.\nClick on Open under Cloud9 IDE, and your IDE should open with a welcome note.\nYou should now see your AWS Cloud9 environment. You need to be familiar with the three areas of the AWS Cloud9 console shown in the following screenshot:\nFile explorer: On the left side of the IDE, the file explorer shows a list of the files in your directory.\nFile editor: On the upper right area of the IDE, the file editor is where you view and edit files that you’ve selected in the file explorer.\nTerminal: On the lower right area of the IDE, this is where you run commands to execute code samples.\nVerify Environment Run aws sts get-caller-identity to verify the AWS CLI is functioning Run python3 --version to verify that python3 is installed Your Cloud9 environment is already configured with boto3, but for this lab we will also need AWS Chalice.\nRun sudo python3 -m pip install chalice to install AWS Chalice . You may see a couple of WARNING lines near the bottom of the command output, these are safely ignored.\nRun curl -O https://amazon-dynamodb-labs.com/assets/global-serverless.zip Run unzip global-serverless.zip \u0026amp;\u0026amp; cd global-serverless To see what application resources we will be deploying you can open the app.py file by navigating to \u0026ldquo;global-serverless/app.py\u0026rdquo; in the file explorer. This code defines Lambda function and API Gateway routes. Deploy a new DynamoDB table In your terminal, run: aws dynamodb create-table \\ --region us-west-2 \\ --table-name global-serverless \\ --attribute-definitions \\ AttributeName=PK,AttributeType=S \\ AttributeName=SK,AttributeType=S \\ --key-schema \\ AttributeName=PK,KeyType=HASH \\ AttributeName=SK,KeyType=RANGE \\ --billing-mode PAY_PER_REQUEST \\ --query \u0026#39;{\u0026#34;New Table \u0026#34;:TableDescription.TableArn, \u0026#34;Status \u0026#34;:TableDescription.TableStatus }\u0026#39; Wait a moment for the table to be created. Check to see when the table status changes from CREATING to ACTIVE by running this command:\naws dynamodb describe-table \\ --table-name global-serverless \\ --region us-west-2 \\ --query \u0026#39;{TableStatus: Table.TableStatus}\u0026#39; 3.Our table is in us-west-2 (Oregon). Let\u0026rsquo;s make it a Global Table by requesting a replica in eu-west-1 (Europe/Dublin).\nRun this command to create a new replica in the eu-west-1 (Europe/Dublin) region:\naws dynamodb update-table --table-name global-serverless --region us-west-2 --cli-input-json \\ \u0026#39;{\u0026#34;ReplicaUpdates\u0026#34;: [ { \u0026#34;Create\u0026#34;: {\u0026#34;RegionName\u0026#34;: \u0026#34;eu-west-1\u0026#34; } } ]}\u0026#39; Check to see when the table replica status changes to ACTIVE by running this command:\naws dynamodb describe-table \\ --table-name global-serverless \\ --region us-west-2 \\ --query \u0026#39;{TableStatus: Table.TableStatus, Replicas: Table.Replicas}\u0026#39; Next, add some data to the table: Writing to a Global Table is done by writing to any of the regional replica tables. Run this command to load video library items into the table with batch-write-item:\naws dynamodb batch-write-item \\ --region us-west-2 \\ --request-items file://sample-data.json These items are how the UI will display which videos are available to stream.\nVerify data was written: aws dynamodb get-item \\ --table-name global-serverless \\ --region us-west-2 \\ --key \u0026#39;{\u0026#34;PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;library\u0026#34;}, \u0026#34;SK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;01\u0026#34;}}\u0026#39; Deploy the backend API service to the first region Run export AWS_DEFAULT_REGION=us-west-2 to instruct Chalice to deploy into us-west-2 for our first region Run chalice deploy and wait for the infrastructure to be created. Chalice is a Python based serverless framework. When the script completes, it reports a list of resources deployed. Copy and paste the Rest API URL into a note as you will need it later. Copy that REST API URL and paste it into a new browser tab to test it. You should see a JSON response of {ping: \u0026ldquo;ok\u0026rdquo;} You can type in certain paths to the end of the URL. Add the word scan so that the URL now ends with /api/scan You should see a JSON response representing the results of a table scan. [\nWeb Application A single-page static web app is provided for you.\nhttps://amazon-dynamodb-labs.com/static/global-serverless-application/web/index.html This app allows you to enter one or more API endpoints, and stores each one as a browser cookie. The stored API endpoints will remain in the browser even if the backend were to have problems. In this way, the web app could make decisions about routing itself to an alternate API if there are errors or no response from the API being used. The web app does not contain any AWS code nor credentials, it simply makes HTTP GET calls to the API for you. The app\u0026rsquo;s web content can be hosted from an S3 bucket, made globally available via Cloudfront, saved locally within Chrome, or converted into a mobile app. For this workshop we assume the user always has access to the web app even if the backend services become unavailable. Steps:\nWithin the web app, press the Add API button. Paste in the API URL you created previously and click OK. Review the buttons that appear. Click Ping to generate a request to the base level URL. The round trip latency will be displayed. This may be slower than expected do to a Lambda cold start. Click Ping again and check the latency. Click the get-item button. This will return the bookmark for user100 watching a show called AshlandValley. Click the forward and back buttons. They will generate requests to increment or decrement the bookmark by 1 second. You now have a test harness where you can perform reads and writes to a DynamoDB record via the custom API.\nDeploy the service stack to the second region, Ireland Run export AWS_DEFAULT_REGION=eu-west-1 to instruct Chalice to deploy into eu-west-1 for our second region. Run chalice deploy and wait for the infrastructure to be created in eu-west-1. When the script completes, it reports a list of resources deployed. Again, copy down the new REST API URL to a note for later use. Return to the web app. Click Add API again and paste in the new API URL. A second row of buttons appears in an alternate color. Note: In this workshop you have permissions for Global Tables in us-west-2 and eu-west-1. In your own account you could add any number of replicas in any regions.\nNote 2: If you make any changes to the code in app.py, you can push the updates to your Lambda function\nby running chalice deploy again.\n"
},
{
	"uri": "http://Handoo464.github.io/6-leda/6.2/6.2.1/",
	"title": "Optional - Pipeline Deep Dive",
	"tags": [],
	"description": "",
	"content": " Understanding how the pipeline works internally is recommended, however reading this page is not mandatory to complete the workshop.\nThis section explains in detail how the pipeline works from end-to-end. For a simplified explanation refer to the figure below where we have divided the pipeline into three stages. For each stage we outline the input and the output of the stage.\nStage 1: \u0026lsquo;State\u0026rsquo; The business problem of near real-time data aggregation is faced by customers in various industries such as manufacturing, retail, gaming, utilities, and financial services. In this workshop we focus on the banking industry, and specifically on the problem of near real-time trade risk aggregation. Typically, financial institutions associate every trade that is performed on the trading floor with a risk value, and the risk management division of the bank needs a consistent view of the total risk values aggregated over all trades. In this workshop we use the following structure for risk messages:\n{ \u0026#34;RiskMessage\u0026#34;: { \u0026#34;TradeID\u0026#34; : \u0026#34;0d957268-2913-4dbb-b359-5ec5ff732cac\u0026#34;, \u0026#34;Value\u0026#34; : 34624.51, \u0026#34;Version\u0026#34; : 3, \u0026#34;Timestamp\u0026#34; : 1616413258.8997078, \u0026#34;Hierarchy\u0026#34; : {\u0026#34;RiskType\u0026#34;: \u0026#34;Delta\u0026#34;, \u0026#34;Region\u0026#34;: \u0026#34;AMER\u0026#34;, \u0026#34;TradeDesk\u0026#34;: \u0026#34;FXSpot\u0026#34;} } } TradeID: A unique identifier for each trade message. Value: The risk value (currency) associated with this trade. Version: The risk value of a trade sometimes needs to be modified at a later stage. This attribute keeps track of the latest known version for a given trade. Timestamp: The time when the trade occurred. Hierarchy: A list of attributes associated with this trade. To expand, this includes the type of risk, the region where the trade took place, and the type of the trading desk that performed the trade. These attributes will be used to group trades and aggregate the data. Consider an example where five risk messages are ingested in the pipeline, as represented in the following figure. For visibility we labeled each message with an identifier from M1 to M5. Each message has a unique TradeID, a risk Value, and a group of hierarchy attributes (as explained above). For simplicity, all messages have the RiskType \u0026quot;Delta\u0026quot; and the Version attribute is always set to 1.\nThe pipeline is event driven by an upstream data source that writes records into a Kinesis Data Stream, and the StateLambda function is invoked to process these messages further.\nAs the rate of message arrival can be very high, multiple StateLambda functions will be invoked concurrently. This means that many simultaneous instances of the StateLambda function will run, each processing a subset of records. To learn more about Lambda function scaling, see the AWS Lambda function scaling documentation page which explains the concept of a Lambda instance. The example above shows the invocation of two StateLambda function instances labeled as #1 and #2. Instance #1 processes messages M1 and M2, while instance #2 processes messages M3, M4, and M5.\nThe responsibility of the StateLambda function is to preserve all incoming messages by writing them to the DynamoDB StateTable, and furthermore to ensure exactly once processing by keeping track of the unique ID of every that was processed at this stage. In this example both StateLambda functions write their respective messages into the DynamoDB StateTable. The StateTable at the bottom of the figure stores incoming risk messages without significant modifications.\nStage 2: \u0026lsquo;Reduce\u0026rsquo; In Stage 2, all rows written into the StateTable are sent to the MapLambda for further processing by an event source mapping that connects the function to the DynamoDB stream of the StateTable. DynamoDB Streams ensure that every item change appears exactly once, and that all changes to a given item appear in the stream shards in the order they are written. However, multiple StateLambda functions write to different keys in the StateTable, and as a result the Stream records in the table\u0026rsquo;s stream might be in a different order than the order in which they were ingested in Stage 1.\nIn order to handle a high volume of incoming messages, multiple MapLambda instances are invoked to process messages from the StateTable DynamoDB stream, similar to Stage 1. In this example, MapLambda #1 takes messages M4, M3, and M1 while MapLambda #2 processes messages M5 and M2.\nThe responsibility of the MapLambda is to perform initial pre-aggregation of the messages, or more specifically to perform arithmetic summation based on the message attributes. The pre-aggregated output of each MapLambda function is written into the ReduceTable as a single row, as seen in \u0026ldquo;Output state of the ReduceTable\u0026rdquo; in the above figure. For simplicity, we refer to these rows as AM1 and AM2.\nHow does the pre-aggregation work? Consider the second row, AM2, which combines messages M2 and M5. Based on the message attributes, the M2 message belongs to Delta::EMEA::FXSpot, while M5 message belongs to Delta::AMER::FXSpot. The common \u0026ldquo;denominator\u0026rdquo; for these trades is that both of them belong to the risk type Delta. Therefore, the Delta attribute in the ReduceTable is a sum of both messages, expressed as 100 + 20 = 120. Below are the messages M2 and M5 in JSON format for reference.\n{\u0026#34;TradeID\u0026#34; : \u0026#34;8ec2fdcd\u0026#34;, \u0026#34;Value\u0026#34;: 100, \u0026#34;Version\u0026#34;: 1, \u0026#34;Hierarchy\u0026#34; : {\u0026#34;RiskType\u0026#34;: \u0026#34;Delta\u0026#34;, \u0026#34;Region\u0026#34;: \u0026#34;EMEA\u0026#34;, \u0026#34;TradeDesk\u0026#34;: \u0026#34;FXSpot\u0026#34;} } {\u0026#34;TradeID\u0026#34; : \u0026#34;395974a4\u0026#34;, \u0026#34;Value\u0026#34;: 20, \u0026#34;Version\u0026#34;: 1, \u0026#34;Hierarchy\u0026#34; : {\u0026#34;RiskType\u0026#34;: \u0026#34;Delta\u0026#34;, \u0026#34;Region\u0026#34;: \u0026#34;AMER\u0026#34;, \u0026#34;TradeDesk\u0026#34;: \u0026#34;FXSpot\u0026#34;} } Stage 3: \u0026lsquo;Aggregate\u0026rsquo; In Stage 3, all rows written into the ReduceTable are sent to the ReduceLambda for further processing via DynamoDB Streams. The responsibility of the ReduceLambda is to further combine incoming pre-aggregated messages and to update the final value in the AggregateTable.\nThis is done in three steps:\nThe ReduceLambda function is invoked with a batch of messages and reads the current state from the AggregateTable The function re-computes the aggregate considering the given batch of pre-aggregated items The reduce function writes the updated values into the AggregateTable Note: there is only one instance of the ReduceLambda function, which is achieved by setting the reserved concurrency to 1. This is desired to avoid a potential write conflict while updating the AggregateTable. From a performance point of view, a single Lambda function instance can handle aggregation of the entire pipeline because incoming messages are already pre-aggregated by the MapLambda functions.\nThe final output in the AggregateTable closely resembles the Hierarchy attribute elements, and can be easily read and displayed by a front-end!\nContinue on to: Connect the Pipeline\n"
},
{
	"uri": "http://Handoo464.github.io/6-leda/6.2/",
	"title": "Overview",
	"tags": [],
	"description": "",
	"content": "Architecture After setup, most of the components the data aggregation pipeline are already put in place for you, as shown in the diagram below. However, some of the links between neighboring components are missing or misconfigured! These connections are the crux of the problem you have to solve.\nThe workshop contains two labs. The objective of the first lab is to establish connections between these components and achieve end-to-end data processing, from the IncomingDataStream on the left to the AggregateTable at the end of the pipeline.\nHowever, the pipeline you build in the first lab doesn\u0026rsquo;t ensure exactly once message processing. Therefore, establishing exactly once processing is the goal of Lab 2.\nLab 1 The diagram below outlines a set of steps that you will need to perform in order to connect all the AWS resources. The Lab 1 section will give you more information and will explain how to perform each step.\nLab 2 In Lab 2, we will enhance the pipeline to ensure exactly once processing for any ingested message. To make sure that our pipeline can withstand different failure modes and achieve exactly once message processing we will modify two Lambda functions.\nThe Lab 2 section will give you more information and will explain how to perform each step. Next steps and competition The Pipeline deep dive with example subsection contains a detailed explanation of how data is processed in this pipeline. This information is recommended but not necessary to complete the workshop.\nTo make this workshop more exciting, when run at an AWS event all participants are rated on how many messages they can aggregate correctly using a scoreboard. The Game rules subsection outlines the rules of the game, and how the GeneratorLambda function ingests data into the start of the pipeline!\nContinue on to: Game Rules.\n"
},
{
	"uri": "http://Handoo464.github.io/7-lgme/7.2/",
	"title": "Plan your data model",
	"tags": [],
	"description": "",
	"content": "Data modeling is the process of designing how an application stores data in a given database. With a NoSQL database such as DynamoDB, data modeling is different than modeling with a relational database. A relational database is built for flexibility and can be a great fit for analytical applications. In relational data modeling, you start with your entities first. When you have a normalized relational model, you can satisfy any query pattern you need in your application.\nNoSQL databases are designed for speed and scale — not flexibility. Though the performance of your relational database may degrade as you scale up, horizontally scaling databases such as DynamoDB provides consistent performance at any scale. Some DynamoDB users have tables that are larger than 100 TB, and the read and write performance of their tables is the same as when the tables were smaller than 1 GB in size.\nAchieving best results with a NoSQL database such as DynamoDB requires a shift in thinking from the typical relational database.\nLet\u0026rsquo;s take a look at some of the best practices when modeling data with DynamoDB.\n"
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.4/1.4.2/",
	"title": "Point-In-Time Recovery Backup",
	"tags": [],
	"description": "",
	"content": "DynamoDB Point-in-time recovery aka \u0026ldquo;PITR\u0026rdquo; helps to protect your DynamoDB tables from accidental write or delete operations. With point-in-time recovery, you do not have to worry about creating, maintaining, or scheduling on-demand backups. For example, suppose that a test script writes accidentally to a production DynamoDB table. With point-in-time recovery, you can restore that table to any point in time during the last 35 days. DynamoDB maintains incremental backups of your table. By default, PITR is disabled.\nHow to enable PITR First, go to the DynamoDB Console and click on Tables from the side menu. In the list of tables, choose the ProductCatalog table.On the Backups tab of the ProductCatalog table in the Point-in-time recovery section, choose Edit. Select Enable Point-in-time-recovery and choose Save changes. To restore a table to a point in time Now let us say we get some unwanted records in ProductCatalog table as highlighted below.\nFollow the below steps to restore ProductCatalog using Point-in-time-recovery.\nSign in to the AWS Management Console and open the DynamoDB console. In the navigation pane on the left side of the console, choose Tables. In the list of tables, choose the ProductCatalog table. On the Backups tab of the ProductCatalog table, in the Point-in-time recovery section, choose Restore to point-in-time. For the new table name, enter ProductCatalogPITR. To confirm the restorable time, set the Restore date and time to the Latest restore date. Choose Restore to start the restore process. Note : You can restore the table to the same AWS Region or to a different Region from where the backup resides. You can also exclude secondary indexes from being created on the new restored table. In addition, you can specify a different encryption mode.\nThe table that is being restored is shown with the status Restoring. After the restore process is finished, the status of the ProductCatalogPITR table changes to Active.\n"
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.2/1.2.2/",
	"title": "Reading Item Collections using Query",
	"tags": [],
	"description": "",
	"content": "Item Collections are groups of Items that share a Partition Key. By definition, Item Collections can only exist in tables that have both a Partition Key and a Sort Key. We can read all or part of an Item Collection using the Query API which can be invoked using the query CLI command . It might seem confusing as the word \u0026ldquo;query\u0026rdquo; is generally used colloquially to mean \u0026ldquo;reading data from a database\u0026rdquo; but in DynamoDB \u0026ldquo;query\u0026rdquo; has a specific meaning: to read all or part of an Item Collection.\nWhen we invoke the Query API we must specify a Key Condition Expression . If we were comparing this to SQL, we would say \u0026ldquo;this is the part of the WHERE clause that acts on the Partition Key and Sort Key attributes\u0026rdquo;. This could take a couple of forms:\nJust the Partition Key value of our Item Collection. This indicates that we want to read ALL the items in the item collection. The Partition Key value and some kind of Sort Key Condition which will match a subset of the rows in item collection. Possible sort key conditions are =, \u0026lt;, \u0026gt;, \u0026lt;=, \u0026gt;=, BETWEEN, and BEGINS_WITH. The Key Condition Expression will define the number of RRUs or RCUs that are consumed by our Query. DynamoDB will add up the size of all the rows matched by the Key Condition Expression, then divide that total size by 4KB to calculate the consumed capacity (and then it will divide that number in half if you\u0026rsquo;re using an eventually consistent read).\nWe can optionally also specify a Filter Expression for our Query. If we were comparing this to SQL, we would say \u0026ldquo;this is the part of the WHERE clause that acts on the non-Key attributes\u0026rdquo;. Filter Expressions act to remove some items from the Result Set returned by the Query, but they do not affect the consumed capacity of the Query. If your Key Condition Expression matches 1,000,000 items and your FilterExpression reduces the result set down to 100 items, you will still be charged to read all 1,000,000 items. But the Filter Expression reduces the amount of data returned from the network connection so there is still a benefit to our application in using Filter Expressions even if it doesn\u0026rsquo;t affect the price of the Query.\nThe ProductCatalog table we used in the previous examples only has a Partition Key so let\u0026rsquo;s look at the data in the Reply table which has both a Partition Key and a Sort Key:\naws dynamodb scan --table-name Reply Data in this table has an Id attribute which references items in the Thread table. Our data has two threads, and each thread has 2 replies. Let\u0026rsquo;s use the query CLI to read just the items from thread 1:\naws dynamodb query \\ --table-name Reply \\ --key-condition-expression \u0026#39;Id = :Id\u0026#39; \\ --expression-attribute-values \u0026#39;{ \u0026#34;:Id\u0026#34; : {\u0026#34;S\u0026#34;: \u0026#34;Amazon DynamoDB#DynamoDB Thread 1\u0026#34;} }\u0026#39; \\ --return-consumed-capacity TOTAL Since the Sort Key in this table is a timestamp, we could specify a Key Condition Expression to return only the replies in a thread that were posted after a certain time by adding a sort key condition:\naws dynamodb query \\ --table-name Reply \\ --key-condition-expression \u0026#39;Id = :Id and ReplyDateTime \u0026gt; :ts\u0026#39; \\ --expression-attribute-values \u0026#39;{ \u0026#34;:Id\u0026#34; : {\u0026#34;S\u0026#34;: \u0026#34;Amazon DynamoDB#DynamoDB Thread 1\u0026#34;}, \u0026#34;:ts\u0026#34; : {\u0026#34;S\u0026#34;: \u0026#34;2015-09-21\u0026#34;} }\u0026#39; \\ --return-consumed-capacity TOTAL Remember we can use Filter Expressions if we want to limit our results based on non-key attributes. For example, we could find all the replies in Thread 1 that were posted by User B:\naws dynamodb query \\ --table-name Reply \\ --key-condition-expression \u0026#39;Id = :Id\u0026#39; \\ --filter-expression \u0026#39;PostedBy = :user\u0026#39; \\ --expression-attribute-values \u0026#39;{ \u0026#34;:Id\u0026#34; : {\u0026#34;S\u0026#34;: \u0026#34;Amazon DynamoDB#DynamoDB Thread 1\u0026#34;}, \u0026#34;:user\u0026#34; : {\u0026#34;S\u0026#34;: \u0026#34;User B\u0026#34;} }\u0026#39; \\ --return-consumed-capacity TOTAL Note than in the response we see these lines:\n\u0026#34;Count\u0026#34;: 1, \u0026#34;ScannedCount\u0026#34;: 2, This is telling us that the Key Condition Expression matched 2 items (ScannedCount) and thats what we were charged to read, but the Filter Expression reduced our result set size down to 1 item (Count).\n"
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.3/1.3.2/",
	"title": "Reading Item Collections using Query",
	"tags": [],
	"description": "",
	"content": "Item Collections are groups of Items that share a Partition Key. By definition, Item Collections can only exist in tables that have both a Partition Key and a Sort Key. We can read all or part of an Item Collection using the Query API which can be invoked using the query CLI command . It might seem confusing as the word \u0026ldquo;query\u0026rdquo; is generally used colloquially to mean \u0026ldquo;reading data from a database\u0026rdquo; but in DynamoDB \u0026ldquo;query\u0026rdquo; has a specific meaning: to read all or part of an Item Collection.\nWhen we invoke the Query API we must specify a Key Condition Expression . If we were comparing this to SQL, we would say \u0026ldquo;this is the part of the WHERE clause that acts on the Partition Key and Sort Key attributes\u0026rdquo;. This could take a couple of forms:\nJust the Partition Key value of our Item Collection. This indicates that we want to read ALL the items in the item collection. The Partition Key value and some kind of Explore the other options in the Item explorer and figure out how to get queries to return with Replies sorted from most recent to least recent. , \u0026gt;=, BETWEEN, and BEGINS_WITH. The Key Condition Expression will define the number of RRUs or RCUs that are consumed by our Query. DynamoDB will add up the size of all the rows matched by the Key Condition Expression, then divide that total size by 4KB to calculate the consumed capacity (and then it will divide that number in half if you\u0026rsquo;re using an eventually consistent read).\nWe can optionally also specify a Filter Expression for our Query. If we were comparing this to SQL, we would say \u0026ldquo;this is the part of the WHERE clause that acts on the non-Key attributes\u0026rdquo;. Filter Expressions act to remove some items from the Result Set returned by the Query, but they do not affect the consumed capacity of the Query. If your Key Condition Expression matches 1,000,000 items and your FilterExpression reduces the result set down to 100 items, you will still be charged to read all 1,000,000 items. But the Filter Expression reduces the amount of data returned from the network connection so there is still a benefit to our application in using Filter Expressions even if it doesn\u0026rsquo;t affect the price of the Query.\nThe ProductCatalog table we used in the previous examples only has a Partition Key so let\u0026rsquo;s look at the data in the Reply table which has both a Partition Key and a Sort Key. Select the left menu bar Explore items under Tables. You may need to click the hamburger menu icon to expand the left menu if its hidden. Once you enter the Explore Items you need to select the Reply table and then expand the Scan/Query items box.\nData in this table has an Id attribute which references items in the Thread table. Our data has two threads, and each thread has 2 replies. Let\u0026rsquo;s use the Query functionality to read just the items from thread 1 by pasting Amazon DynamoDB#DynamoDB Thread 1 into the Id (Partition key) box and then clicking Run.\nWe can see that there are two Reply items in the DynamoDB Thread 1 thread.\nSince the Sort Key in this table is a timestamp, we could specify a Key Condition Expression to return only the replies in a thread that were posted after a certain time by adding a sort key condition where ReplyDateTime is More than 2015-09-21 and clicking Run.\nRemember we can use Filter Expressions if we want to limit our results based on non-key attributes. For example, we could find all the replies in Thread 1 that were posted by User B. Clear the sort key condition, and click Add filter then use PostedBy for the Attribute name, Condition Equals and Value User B, then click Run.\n"
},
{
	"uri": "http://Handoo464.github.io/7-lgme/7.6/7.6.2/",
	"title": "Retrieve games for a user",
	"tags": [],
	"description": "",
	"content": "How that you have created the inverted index, let’s use it to retrieve the Game entities played by a User. To handle this, you need to query the inverted index with the User whose Game entities you want to see.\nIn the code you downloaded, a find_games_for_user.py script is in the scripts/ directory.\nimport sys import boto3 from entities import UserGameMapping dynamodb = boto3.client(\u0026#39;dynamodb\u0026#39;) USERNAME = sys.argv[1] if len(sys.argv) == 2 else \u0026#34;carrpatrick\u0026#34; def find_games_for_user(username): try: resp = dynamodb.query( TableName=\u0026#39;battle-royale\u0026#39;, IndexName=\u0026#39;InvertedIndex\u0026#39;, KeyConditionExpression=\u0026#34;SK = :sk\u0026#34;, ExpressionAttributeValues={ \u0026#34;:sk\u0026#34;: { \u0026#34;S\u0026#34;: f\u0026#34;USER#{username}\u0026#34; } }, ScanIndexForward=True ) except Exception as e: print(\u0026#39;Index is still backfilling. Please try again in a moment.\u0026#39;) return None return [UserGameMapping(item) for item in resp[\u0026#39;Items\u0026#39;]] games = find_games_for_user(USERNAME) if games: print(f\u0026#34;Games played by user {USERNAME}:\u0026#34;) for game in games: print(game) In this script, you have a function called find_games_for_user() that is similar to a function you would have in your gaming application. This function takes a user name and returns all the games played by the given user.\nRun the script in your terminal with the following command:\npython scripts/find_games_for_user.py The script should print all of the games played by the user carrpatrick.\nGames played by user carrpatrick: UserGameMapping: 25cec5bf-e498-483e-9a00-a5f93b9ea7c7 Username: carrpatrick Place: SILVER UserGameMapping: c6f38a6a-d1c5-4bdf-8468-24692ccc4646 Username: carrpatrick UserGameMapping: c9c3917e-30f3-4ba4-82c4-2e9a0e4d1cfd Username: carrpatrick You can run the script for other users by adding their username as a command line parameter.\nTry running the script again and finding games for user vlopez:\npython scripts/find_games_for_user.py vlopez The output should look like this:\nGames played by user vlopez: UserGameMapping: c6f38a6a-d1c5-4bdf-8468-24692ccc4646 Username: vlopez Review In this module, you satisfied the final access pattern by retrieving all Game entities played by a User. To handle this access pattern, you created a secondary index using the inverted index pattern to allow querying on the other side of the many-to-many relationship between User entities and Game entities.\n"
},
{
	"uri": "http://Handoo464.github.io/4-lcdc/4.2/",
	"title": "Scenario Overview",
	"tags": [],
	"description": "",
	"content": "Imagine you have an e-commerce website where customers place orders for different items. The website relies on Amazon DynamoDB and it requires logging of all events from when an order is placed until the item is delivered.\nWebsite Requirements:\nThe status of orders placed on your website can be ACTIVE, PLACED, COMPLETE or CANCELLED. You need to keep the current view of customers\u0026rsquo; orders on the main database table used by your application. Each order has a status attribute and contains a list of one or more items. In JSON format, an item on the orders table has the following attributes.\n{ \u0026#34;id\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;customer\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;string\u0026#34; }, \u0026#34;orderDate\u0026#34;: \u0026#34;YYYY-MM-DD hh:mm:ss\u0026#34;, \u0026#34;shipDate\u0026#34;: \u0026#34;YYYY-MM-DD hh:mm:ss\u0026#34;, \u0026#34;items\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;price\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;quantity\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;string\u0026#34; } ] } You will implement a solution to meet this requirement by using two DynamoDB tables - Orders and OrdersHistory; and a streaming solution to copy item level changes from the Orders table to OrdersHistory table.\n"
},
{
	"uri": "http://Handoo464.github.io/2-lbed/2.2/",
	"title": "Service Configuration",
	"tags": [],
	"description": "",
	"content": "In this section, you will load data into your DynamoDB table and configure your OpenSearch Service resources.\nBefore beginning this section, make sure that setup has been completed for whichever way you\u0026rsquo;re running this lab. Setup will deploy several resources.\nDependencies from Cloud9 CloudFormation Template:\nS3 Bucket: Used to store the initial export of DynamoDB data for the Zero-ETL Pipeline. IAM Role: Used to grant permissions for pipeline integration and queries. Cloud9 IDE: Console for executing commands, building integrations, and running sample queries. zETL CloudFormation Template Resources:\nDynamoDB Table: DynamoDB table to store product descriptions. Has Point-in-time Recovery (PITR) and DynamoDB Streams enabled. Amazon OpenSearch Service Domain: Single-node OpenSearch Service cluster to recieve data from DynamoDB and act as a vector database. "
},
{
	"uri": "http://Handoo464.github.io/7-lgme/7.5/7.5.2/",
	"title": "Start a game",
	"tags": [],
	"description": "",
	"content": "As soon as a game has 50 users, the creator of the game can start the game to initiate gameplay. In this step, you see how to handle this access pattern.\nWhen the application backend receives a request to start the game, you check three things:\nThe game has 50 people signed up. The requesting user is the creator of the game. The game has not already started. You can handle each of these checks in a condition expression in a request to update the game. If all of these checks pass, you need to update the entity in the following ways:\nRemove the open_timestamp attribute so that it does not appear as an open game in the sparse GSI created earlier. Add a start_time attribute to indicate when the game started. In the code you downloaded, a start_game.py script is in the scripts/ directory.\nimport datetime import boto3 from entities import Game dynamodb = boto3.client(\u0026#39;dynamodb\u0026#39;) GAME_ID = \u0026#34;c6f38a6a-d1c5-4bdf-8468-24692ccc4646\u0026#34; CREATOR = \u0026#34;gstanley\u0026#34; def start_game(game_id, requesting_user, start_time): try: resp = dynamodb.update_item( TableName=\u0026#39;battle-royale\u0026#39;, Key={ \u0026#34;PK\u0026#34;: { \u0026#34;S\u0026#34;: f\u0026#34;GAME#{game_id}\u0026#34; }, \u0026#34;SK\u0026#34;: { \u0026#34;S\u0026#34;: f\u0026#34;#METADATA#{game_id}\u0026#34; } }, UpdateExpression=\u0026#34;REMOVE open_timestamp SET start_time = :time\u0026#34;, ConditionExpression=\u0026#34;people = :limit AND creator = :requesting_user AND attribute_not_exists(start_time)\u0026#34;, ExpressionAttributeValues={ \u0026#34;:time\u0026#34;: { \u0026#34;S\u0026#34;: start_time.isoformat() }, \u0026#34;:limit\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;50\u0026#34; }, \u0026#34;:requesting_user\u0026#34;: { \u0026#34;S\u0026#34;: requesting_user } }, ReturnValues=\u0026#34;ALL_NEW\u0026#34; ) return Game(resp[\u0026#39;Attributes\u0026#39;]) except Exception as e: print(\u0026#39;Could not start game\u0026#39;) return False game = start_game(GAME_ID, CREATOR, datetime.datetime(2019, 4, 16, 10, 15, 35)) if game: print(f\u0026#34;Started game: {game}\u0026#34;) In this script, the start_game function is similar to the function you would have in your application. It takes a game_id, requesting_user, and start_time, and it runs a request to update the Game entity to start the game.\nThe ConditionExpression parameter in the update_item() call specifies each of the three checks that were listed earlier in this step — the game must have 50 people, the user requesting that the game start must be the creator of the game, and the game cannot have a start_time attribute, which would indicate it already started.\nIn the UpdateExpression parameter, you can see the changes you want to make to the entity. First you remove the open_timestamp attribute from the entity, and then you set the start_time attribute to the game’s start time.\nYou can choose to run either the start_game.py python script or the AWS CLI command below. Both are provided to show different methods of interacting with DynamoDB.\nRun this script in your terminal with the following command:\npython scripts/start_game.py You should see output in your terminal indicating that the game was started successfully.\nStarted game: Game: c6f38a6a-d1c5-4bdf-8468-24692ccc4646 Map: Urban Underground Try to run the script a second time in your terminal. This time, you should see an error message that indicates you could not start the game. This is because you have already started the game, so the start_time attribute exists. As a result, the request failed the conditional check on the entity.\nAlternatively, you can run this AWS CLI command to start the game:\naws dynamodb update-item \\ --table-name battle-royale \\ --key \\ \u0026#34;{ \\\u0026#34;PK\\\u0026#34;: { \\\u0026#34;S\\\u0026#34;: \\\u0026#34;GAME#c6f38a6a-d1c5-4bdf-8468-24692ccc4646\\\u0026#34; }, \\\u0026#34;SK\\\u0026#34;: { \\\u0026#34;S\\\u0026#34;: \\\u0026#34;#METADATA#c6f38a6a-d1c5-4bdf-8468-24692ccc4646\\\u0026#34; } }\u0026#34; \\ --update-expression \u0026#34;REMOVE open_timestamp SET start_time = :time\u0026#34; \\ --condition-expression \\ \u0026#34;people = :limit AND creator = :requesting_user AND attribute_not_exists(start_time)\u0026#34; \\ --expression-attribute-values \\ \u0026#34;{ \\\u0026#34;:time\\\u0026#34;: { \\\u0026#34;S\\\u0026#34;: \\\u0026#34;2019-04-16T10:15:35\\\u0026#34; }, \\\u0026#34;:limit\\\u0026#34;: { \\\u0026#34;N\\\u0026#34;: \\\u0026#34;50\\\u0026#34; }, \\\u0026#34;:requesting_user\\\u0026#34;: { \\\u0026#34;S\\\u0026#34;: \\\u0026#34;gstanley\\\u0026#34; } }\u0026#34; \\ --return-values \u0026#34;ALL_NEW\u0026#34; If you run the AWS CLI command, you will see the NEW values of the item you updated and you will notice that there is no longer an attribute named open_timestamp but there is an attribute named start_time.\n{ \u0026#34;Attributes\u0026#34;: { \u0026#34;creator\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;gstanley\u0026#34; }, \u0026#34;people\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;50\u0026#34; }, \u0026#34;SK\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;#METADATA#c6f38a6a-d1c5-4bdf-8468-24692ccc4646\u0026#34; }, \u0026#34;create_time\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;2019-04-16T10:12:54\u0026#34; }, \u0026#34;map\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Urban Underground\u0026#34; }, \u0026#34;start_time\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;2019-04-16T10:15:35\u0026#34; }, \u0026#34;PK\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;GAME#c6f38a6a-d1c5-4bdf-8468-24692ccc4646\u0026#34; }, \u0026#34;game_id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;c6f38a6a-d1c5-4bdf-8468-24692ccc4646\u0026#34; } } } Review In this module, you saw how to satisfy two advanced write operations in the application. First, you used DynamoDB transactions when a user joined a game. With transactions, you handled a complex conditional write across multiple entities in a single request.\nSecond, you implemented the function for a creator of a game to start a game when it’s ready. In this access pattern, you had an update operation that required checking the value of three attributes and updating two attributes. You can express this complex logic in a single request through the power of condition expressions and update expressions.\nIn the next module, you will look at the final access pattern, which involves viewing past games in the application.\n"
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.1/3.1.2/",
	"title": "Step 1 - Open the AWS Systems Manager Console",
	"tags": [],
	"description": "",
	"content": " Once you\u0026rsquo;ve gained access to the AWS Management Console for the lab, double check the region is correct and the role name WSParticipantRole appears on the top right of the console.\nIn the services search bar, search for Systems Manager and click on it to open the AWS Systems Manager section of the AWS Management Console.\nIn the AWS Systems Manager console, locate the menu in the left, identify the section Node Management and select Session Manager from the list.\nChoose Start session to launch a shell session.\nClick the radio button to select the EC2 instance for the lab. If you see no instance, wait a few minutes and then click refresh. Wait until an ec2 instance with name of DynamoDBC9 is available before continuing. Select the instance.\nClick the Start Session button (This action will open a new tab in your browser with a new black shell).\nIn the new black shell, switch to the ubuntu account by running sudo su - ubuntu\nsudo su - ubuntu run shopt login_shell and be sure it says login_shell on and then change into the workshop directory.\n#Verify login_shell is \u0026#39;on\u0026#39; shopt login_shell #Change into the workshop directory cd ~/workshop/ The output of your commands in the Session Manager session should look like the following:\n$ sudo su - ubuntu :~ $ #Verify login_shell is \u0026#39;on\u0026#39; shopt login_shell #Change into the workshop directory cd ~/workshop/ login_shell on :~/workshop $ "
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.3/3.3.2/",
	"title": "Step 2 - Execute a parallel Scan",
	"tags": [],
	"description": "",
	"content": "To perform a parallel Scan, each application worker issues its own Scan request with the following parameters:\nSegment: The segment to be scanned by a specific application worker. Each worker should use a different value for Segment. TotalSegments: The total number of segments for the parallel scan. This value must be the same as the number of workers that your application will use. Review the following code block for the parallel scan, which is from the file scan_logfile_parallel.py.\nfe = \u0026#34;responsecode \u0026lt;\u0026gt; :f\u0026#34; eav = {\u0026#34;:f\u0026#34;: 200} response = table.scan( FilterExpression=fe, ExpressionAttributeValues=eav, Limit=pageSize, TotalSegments=totalsegments, Segment=threadsegment, ProjectionExpression=\u0026#39;bytessent\u0026#39; ) After the first scan, you can continue scanning the table until LastEvaluatedKey equals null.\nwhile \u0026#39;LastEvaluatedKey\u0026#39; in response: response = table.scan( FilterExpression=fe, ExpressionAttributeValues=eav, Limit=pageSize, TotalSegments=totalsegments, Segment=threadsegment, ExclusiveStartKey=response[\u0026#39;LastEvaluatedKey\u0026#39;], ProjectionExpression=\u0026#39;bytessent\u0026#39;) for i in response[\u0026#39;Items\u0026#39;]: totalbytessent += i[\u0026#39;bytessent\u0026#39;] To run this code, execute the following AWS CLI command.\npython scan_logfile_parallel.py logfile_scan 2 Parameters:\nTable name: logfile_scan Number of threads: 2 (this is number of threads to be executed in parallel, which will be used for the number of segments as well). The output will look like the following.\nScanning 1 million rows of the `logfile_scan` table to get the total of bytes sent Total bytessent 6054250 in 8.544446229934692 seconds The execution time using a parallel scan will be shorter than the execution time for a sequential scan. The difference in execution time will be even more exaggerated for larger tables.\nSummary In this exercise, we have demonstrated use of two methods of DynamoDB table scanning: sequential and parallel, to read items from a table or secondary index. Use Scan sparingly because it can consume large amounts of capacity resources. Sometimes a Scan is appropriate (such as scanning a small table) or unavoidable (such as performing a bulk export of data). However, as a general rule, you should design your applications to avoid performing scans.\n"
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.5/3.5.2/",
	"title": "Step 2 - Load data into the new table",
	"tags": [],
	"description": "",
	"content": "Now, execute a script that loads the data from the file named ./employees.csv into the table named employees.\npython load_employees.py employees ./data/employees.csv The sample employees.csv record looks like the following:\n1000,Nanine Denacamp,Programmer Analyst,Development,San Francisco,CA,1981-09-30,2014-06-01,Senior Database Administrator,2014-01-25 When you ingest this data into the table, you concatenate some of the attributes, such as city_dept (example: San Francisco:Development) because you have an access pattern in the query that takes advantage of this concatenated attribute. The SK attribute is also a derived attribute. The concatenation is handled in the Python script, which assembles the record and then executes a put_item() to write the record to the table.\nOutput:\n$ python load_employees.py employees ./data/employees.csv employee count: 100 in 3.7393667697906494 employee count: 200 in 3.7162938117980957 ... employee count: 900 in 3.6725080013275146 employee count: 1000 in 3.6174678802490234 RowCount: 1000, Total seconds: 36.70457601547241 The output confirms that 1000 items have been inserted to the table.\nReview the employees table in the DynamoDB console (as shown in the following screenshot) by choosing the employees table and then choosing the Items menu item.\nOn the same page in the right pane, choose [Index] from the dropdown menu and then click Run.\nNow you can see result of \u0026ldquo;Scan\u0026rdquo; operation on an overloaded global secondary index. There are many different entity types in the result set: a root, a previous title, and a current title. "
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.2/3.2.2/",
	"title": "Step 2 - Load sample data into the table",
	"tags": [],
	"description": "",
	"content": "Now that you have created the table, you can load some sample data into the table by running the following Python script.\ncd /home/ubuntu/workshop python load_logfile.py logfile ./data/logfile_small1.csv The parameters in the preceding command: 1) Table name = logfile 2) File name = logfile_small1.csv\nThe output will look like the following.\nrow: 100 in 0.780548095703125 row: 200 in 7.2669219970703125 row: 300 in 1.547729730606079 row: 400 in 3.9651060104370117 row: 500 in 3.98996901512146 RowCount: 500, Total seconds: 17.614499807357788 Curious behavior: You might wonder why one of the runs took more than five seconds. See the next step for the explanation.\n"
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.7/3.7.2/",
	"title": "Step 2 - Query all the employees from a state",
	"tags": [],
	"description": "",
	"content": "You can use the new global secondary index to query the table. If you use only the state, the query does not use the sort key attribute. However, if the query has a value for the second parameter, the code uses the GSI_3_SK attribute of the global secondary index, which holds the same value as the city_dept attribute, to query all the values that begin with the parameter value.\nThe following screenshot shows using composite key attributes to query by city and department. We can perform this same query in a Python script. This snippet shows how a script can take two input parameters (shown as value1 and value2) and craft a query against the GSI_3 global secondary index.\nif value2 == \u0026#34;-\u0026#34;: ke = Key(\u0026#39;GSI_3_PK\u0026#39;).eq(\u0026#34;state#{}\u0026#34;.format(value1)) else: ke = Key(\u0026#39;GSI_3_PK\u0026#39;).eq(\u0026#34;state#{}\u0026#34;.format(value1)) \u0026amp; Key(\u0026#39;GSI_3_SK\u0026#39;).begins_with(value2) response = table.query( IndexName=\u0026#39;GSI_3\u0026#39;, KeyConditionExpression=ke ) Run the following Python script to query global secondary index GSI_3 for all employees in the state of Texas.\npython query_city_dept.py employees TX The result should look similar to the following.\nList of employees . State: TX Name: Bree Gershom. City: Austin. Dept: Development Name: Lida Flescher. City: Austin. Dept: Development Name: Tristam Mole. City: Austin. Dept: Development Name: Malinde Spellman. City: Austin. Dept: Development Name: Giovanni Goutcher. City: Austin. Dept: Development ... Name: Cullie Sheehy. City: San Antonio. Dept: Support Name: Ari Wilstead. City: San Antonio. Dept: Support Name: Odella Kringe. City: San Antonio. Dept: Support Total of employees: 197. Execution time: 0.238062143326 seconds "
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.4/3.4.2/",
	"title": "Step 2 - Querying the GSI with shards",
	"tags": [],
	"description": "",
	"content": "To get all the log records with a 404 response code, you need to query all the global secondary index partitions by using the sort key. You can do that using parallel threads in your application, and by using the partition key and sort key.\nif date:= \u0026#34;all\u0026#34;: ke = Key(\u0026#39;GSI_1_PK\u0026#39;).eq(\u0026#34;shard#{}\u0026#34;.format(shardid)) \u0026amp; Key(\u0026#39;GSI_1_SK\u0026#39;).begins_with(responsecode) else: ke = Key(\u0026#39;GSI_1_PK\u0026#39;).eq(\u0026#34;shard#{}\u0026#34;.format(shardid)) \u0026amp; Key(\u0026#39;GSI_1_SK\u0026#39;).begins_with(responsecode+\u0026#34;#\u0026#34;+date) response = table.query( IndexName=\u0026#39;GSI_1\u0026#39;, KeyConditionExpression=ke ) Run the following script to retrieve the items from the sharded global secondary index by using only the partition key and response code.\npython query_responsecode.py logfile_scan 404 This will query the logfile_scan table for items with sort keys that begins_with 404. begins_with is a parameter in the DynamoDB Query\u0026rsquo;s KeyConditionExpression as described in our documentation . A query is run for each shard on the GSI and the results are counted on the client. The output of the script will look like the following.\nRecords with response code 404 in the shardid 0 = 0 Records with response code 404 in the shardid 1 = 1750 Records with response code 404 in the shardid 2 = 2500 Records with response code 404 in the shardid 3 = 1250 Records with response code 404 in the shardid 4 = 1000 Records with response code 404 in the shardid 5 = 1000 Records with response code 404 in the shardid 6 = 1750 Records with response code 404 in the shardid 7 = 1500 Records with response code 404 in the shardid 8 = 3250 Records with response code 404 in the shardid 9 = 2750 Number of records with responsecode 404 is 16750. Query time: 1.5092344284057617 seconds You also can query the same global secondary index for the same response code and specify a date. This will query the logfile_scan table for items with sort keys that begins_with 404#2017-07-21.\npython query_responsecode.py logfile_scan 404 --date 2017-07-21 The output will look like the following.\nRecords with response code 404 in the shardid 0 = 0 Records with response code 404 in the shardid 1 = 750 Records with response code 404 in the shardid 2 = 750 Records with response code 404 in the shardid 3 = 250 Records with response code 404 in the shardid 4 = 500 Records with response code 404 in the shardid 5 = 0 Records with response code 404 in the shardid 6 = 250 Records with response code 404 in the shardid 7 = 1000 Records with response code 404 in the shardid 8 = 1000 Records with response code 404 in the shardid 9 = 1000 Number of records with responsecode 404 is 5500. Query time: 1.190359354019165 seconds Review In this exercise, we used a sharded global secondary index (GSI) to quickly retrieve sorted results, which used composite keys that are covered later in the lab in Exercise 6. Use GSI write sharding when you need a scalable sorted index.\nThe sharded GSI example used a set range of keys from 0 to 9 inclusive, but in your own application you can choose any range. In your application, you can add more shards as the number of items indexed increase. In each shard, the data is sorted on disk by the sort key. This allowed us to retrieve server access logs sorted by status code and the date, e.g. 404#2017-07-21.\nFor more information on how to choose the right number of shards, read Choosing the right number of shards for your large-scale Amazon DynamoDB table on the AWS Database Blog.\n"
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.9/3.9.2/",
	"title": "Step 2 - Review the AWS IAM policy for the IAM role",
	"tags": [],
	"description": "",
	"content": "We have pre-created the IAM role DDBReplicationRole that will be used as the AWS Lambda Execution Role . This IAM role allows provides several permissions to the AWS Lambda function we will need to replicate data.\nReview the following policy which is attached to the IAM role DDBReplicationRole.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:DescribeStream\u0026#34;, \u0026#34;dynamodb:GetRecords\u0026#34;, \u0026#34;dynamodb:GetShardIterator\u0026#34;, \u0026#34;dynamodb:ListStreams\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34; }, { \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:DeleteItem\u0026#34;, \u0026#34;dynamodb:PutItem\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34; }, { \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34; } ] } These are some of the permissions granted to the Lambda function in the policy:\nThe AWS Lambda service should have the ability to call DynamoDB Streams and retrieve records from the stream. { \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:DescribeStream\u0026#34;, \u0026#34;dynamodb:GetRecords\u0026#34;, \u0026#34;dynamodb:GetShardIterator\u0026#34;, \u0026#34;dynamodb:ListStreams\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34; } The Lambda function can put and delete items in any DynamoDB table. { \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:DeleteItem\u0026#34;, \u0026#34;dynamodb:PutItem\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34; } Log events are published to Amazon CloudWatch Logs (but in this lab they are not available). { \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34; } "
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.8/3.8.2/",
	"title": "Step 2 - Review the InvoiceAndBills table on the DynamoDB console",
	"tags": [],
	"description": "",
	"content": "In the DynamoDB console, open the InvoiceAndBills table and choose the Items menu option. From the dropdown menu, choose InvoiceAndBills GSI_1 and then Scan the table.\nIn the output, choose PK to sort the data in reverse. Notice the different entity types in the same table.\nIn the following steps you will query the table and retrieve different entity types. Optionally consider performing the queries in the AWS console right after you query them with the Python scripts for extra insight.\n"
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.6/3.6.2/",
	"title": "Step 2 - Scan the employees table to find managers without using the sparse global secondary index",
	"tags": [],
	"description": "",
	"content": "The sparse index pattern cuts the haystack representing your data down into a smaller pile so that your searches on the index, with either the Scan or Query API, are more efficient. Instead of combing through all the data in the DynamoDB base table, you can create a sparse index to hold a fraction of your information for easy querying and searching. To learn the definition of a DynamoDB sparse index please review our best practices documentation .\nTo start, scan the table to find all the managers without using the global secondary index. The throughput consumed will give us a baseline for comparison later in the exercise. In this case, you need to use a filter expression to return only the items where the attribute is_manager is equal to 1, as shown in the following code example.\nfe = \u0026#34;is_manager = :f\u0026#34; eav = {\u0026#34;:f\u0026#34;: \u0026#34;1\u0026#34;} response = table.scan( FilterExpression=fe, ExpressionAttributeValues=eav, Limit=pageSize ) Run the following python script to find all managers without using the global secondary index.\npython scan_for_managers.py employees 100 Parameters:\nTable name = employees Page size = 100 (this is size of the pagination for the scan). The following output includes the scanned count and the execution time.\nManagers count: 84. # of records scanned: 4000. Execution time: 0.596132993698 seconds Review the number of items scanned to return the values. The value of the number of records scanned in the above sample output, 4000, should match the number in your script\u0026rsquo;s output. If you receive an error or inconsistency, ensure you completed Step 1 in this exercise and both indexes are ACTIVE. Try changing the page size to a larger number such as 1000. The execution time will decrease because there are fewer round trips to DynamoDB. A Scan API call can return up to 1MB of data at a time.\n"
},
{
	"uri": "http://Handoo464.github.io/6-leda/6.3/6.3.2/",
	"title": "Step 2: Check MapLambda trigger",
	"tags": [],
	"description": "",
	"content": " The MapLambda function is already connected for you, so let us quickly check if it works as expected!\nCheck that MapLambda has a correctly configured trigger to receive messages from the StateTable stream:\nNavigate to the AWS Lambda service within the AWS Management Console. Click on the MapLambda function to view its configuration. Verify that the MapLambda function has a DynamoDB trigger and this trigger points to the StateTable (see figure below). How do you know it is working? Any row written to the StateTable should trigger the MapLambda function. Therefore, you should be able to see logs for the Lambda invocations.\nAlternatively, you can observe the outputs of the MapLambda function in the DynamoDB ReduceTable. To do that, navigate to the DynamoDB service in the AWS console, click Items on the left, and select ReduceTable. At this stage you should see multiple rows similar to the image below.\nContinue on to: Step 3.\n"
},
{
	"uri": "http://Handoo464.github.io/6-leda/6.4/6.4.2/",
	"title": "Step 2: Ensure idempotency of ReduceLambda function",
	"tags": [],
	"description": "",
	"content": "The objective of this step is to modify the ReduceLambda function to ensure idempotency, which means the values of the downstream AggregateTable will remain unchanged when old records are re-processed in the ReduceLambda function. DynamoDB transactions provide idempotency via the parameter ClientRequestToken that can be supplied with the TransactWriteItems API operation. The ClientRequestToken makes sure that subsequent invocations of transactions with a token that was already used in the last 10 minutes don’t result in updates to the DynamoDB table.\nWe compute the hash over all messages in the batch that the Lambda function is invoked with to use as the ClientRequestToken. Lambda ensures that the function is retried with the same batch of messages on failure. Therefore, by ensuring that all code paths in the Lambda function are deterministic we can ensure idempotency of the transactions and achieve exactly once processing at this last stage of the pipeline. This method has a weakness because we only protect against re-processed messages within a 10-minute window after the first completed TransactWriteItems call since a ClientRequestToken is valid for no more than 10 minutes, as outlined in the official documentation .\nNavigate to the AWS Lambda service within the AWS Management console. Click on the ReduceLambda function to edit its configuration. Click on the Code tab to access the Lambda function\u0026rsquo;s code. Locate the following snippet in the Lambda function code:\n# Batch of Items batch = [ { \u0026#39;Update\u0026#39;: { \u0026#39;TableName\u0026#39; : constants.AGGREGATE_TABLE_NAME, \u0026#39;Key\u0026#39; : {constants.AGGREGATE_TABLE_KEY : {\u0026#39;S\u0026#39; : entry}}, \u0026#39;UpdateExpression\u0026#39; : \u0026#34;ADD #val :val \u0026#34;, \u0026#39;ExpressionAttributeValues\u0026#39; : { \u0026#39;:val\u0026#39;: {\u0026#39;N\u0026#39; : str(totals[entry])} }, \u0026#39;ExpressionAttributeNames\u0026#39;: { \u0026#34;#val\u0026#34; : \u0026#34;Value\u0026#34; } } } for entry in totals.keys()] response = ddb_client.transact_write_items( TransactItems = batch ) This section of code does the following:\nCreates a list of Python dictionaries containing entries corresponding to item operations to be processed by the TransactWriteItems API. To see all options for the field including Update, see the API documentation . Specifically, each Update entry in the API call makes a modification to one DynamoDB item keyed by entry by atomically incrementing the Value attribute by the calculated total. Modify ddb_client.transact_write_items statement to include the ClientRequestToken The code below contains two modifications:\nComputes a ClientRequestToken attribute as a hash value of all messages in the Lambda batch. Provides the ClientRequestToken as part of the DynamoDB TransactWriteItems API call. # Batch of Items batch = [ { \u0026#39;Update\u0026#39;: { \u0026#39;TableName\u0026#39; : constants.AGGREGATE_TABLE_NAME, \u0026#39;Key\u0026#39; : {constants.AGGREGATE_TABLE_KEY : {\u0026#39;S\u0026#39; : entry}}, \u0026#39;UpdateExpression\u0026#39; : \u0026#34;ADD #val :val \u0026#34;, \u0026#39;ExpressionAttributeValues\u0026#39; : { \u0026#39;:val\u0026#39;: {\u0026#39;N\u0026#39; : str(totals[entry])} }, \u0026#39;ExpressionAttributeNames\u0026#39;: { \u0026#34;#val\u0026#34; : \u0026#34;Value\u0026#34; } } } for entry in totals.keys()] # Calculate hash to ensure this batch hasn\u0026#39;t been processed already: record_list_hash = hashlib.md5(str(records).encode()).hexdigest() response = ddb_client.transact_write_items( TransactItems = batch, ClientRequestToken = record_list_hash ) Apply these changes to your Lambda function code, either manually or just by copying the code snippet from above:\nCompute a hash over all the records in the batch (see line 18 in the previous snippet). Provide this hash to the ddb_client.transact_write_items function, as a ClientRequestToken (line 8 in the snippet above). Finally, click on Deploy to apply the changes. How do you know it is working? Check your scoreboard. If all the previous steps are completed successfully you should start accumulating a score above 300 points. If not, check the CloudWatch Logs of the ReduceLambda function to check for any errors. If you see any errors, they may provide a hint on how to fix them. If you need help, go to Summary \u0026amp; Conclusions on the left, then Solutions, and you can see the desired code of the ReduceLambda.\nEven if you\u0026rsquo;ve done everything correctly, the error rate won\u0026rsquo;t drop to zero! The manually induced failures will still be there, but now the pipeline is able to sustain them and still ensure consistent aggregation.\nContinue on to: Summary \u0026amp; Conclusions\nor: Optional: Add a simple Python frontend\n"
},
{
	"uri": "http://Handoo464.github.io/7-lgme/7.3/7.3.3/",
	"title": "Bulk-load data",
	"tags": [],
	"description": "",
	"content": "In this step, you bulk-load some data into the DynamoDB you created in the preceding step. This means that in succeeding steps, you will have sample data to use.\nIn the scripts/directory, you will find a file called items.json. This file contains 835 example items that were randomly generated for this lab. These items include User, Game, and UserGameMapping entities. Open the file if you want to see some of the example items.\nThe scripts/ directory also has a file called bulk_load_table.py that reads the items in the items.json file and bulk-writes them to the DynamoDB table. Below is the content of the file:\nimport json import boto3 dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamodb.Table(\u0026#39;battle-royale\u0026#39;) items = [] with open(\u0026#39;scripts/items.json\u0026#39;, \u0026#39;r\u0026#39;) as f: for row in f: items.append(json.loads(row)) with table.batch_writer() as batch: for item in items: batch.put_item(Item=item) In this script, rather than using the low-level client in Boto 3, you use a higher-level Resource object . Resource objects provide an easier interface for using the AWS APIs. The Resource object is useful in this situation because it batches the requests. The BatchWriteItem operation accepts as many as 25 items in a single request. The Resource object handles that batching for you rather than making you divide the data into requests of 25 or fewer items.\nRun the bulk_load_table.py script and load your table with data by running the following command in the terminal:\npython scripts/bulk_load_table.py You can ensure that all your data was loaded into the table by running a Scan operation and getting the count.\naws dynamodb scan --table-name battle-royale --select COUNT --return-consumed-capacity TOTAL This should display the following results:\n{ \u0026#34;Count\u0026#34;: 835, \u0026#34;ScannedCount\u0026#34;: 835, \u0026#34;ConsumedCapacity\u0026#34;: { \u0026#34;CapacityUnits\u0026#34;: 14.5, \u0026#34;TableName\u0026#34;: \u0026#34;battle-royale\u0026#34;, \u0026#34;Table\u0026#34;: { \u0026#34;CapacityUnits\u0026#34;: 14.5 } } } You should see a Count of 835, indicating that all of your items were loaded successfully.\nYou can also browse the table by navigating to Services -\u0026gt; Database -\u0026gt; DynamoDB in the AWS console.\nIn the next step, you see how to retrieve multiple entity types in a single request, which can reduce the total network requests you make in your application and enhance application performance.\n"
},
{
	"uri": "http://Handoo464.github.io/4-lcdc/4.3/",
	"title": "Change Data Capture using DynamoDB Streams",
	"tags": [],
	"description": "",
	"content": "DynamoDB streams records a time ordered sequence of item level changes that occur on a DynamoDB table. Once enabled, information about all item level changes will be logged and stored for up to 24 hours.\nChanges to items on DynamoDB tables with streams enabled are captured in near real-time so the events can be used as triggers for event driven applications that consume data from your DynamoDB stream.\nIn this chapter, you enable DynamoDB streams on the Orders table and deploy an AWS Lambda function that copies changes from the Orders table to the OrdersHistory table every time an item is updated on the Orders table.\nThe resulting solution is shown in the image below.\n"
},
{
	"uri": "http://Handoo464.github.io/7-lgme/7.3/",
	"title": "Core usage: user profiles and games",
	"tags": [],
	"description": "",
	"content": "In the previous module, the game application’s access patterns were defined. In this module, the primary key for the DynamoDB table is defined and the core access patterns are handled.\nWhen designing the primary key for a DynamoDB table, keep the following best practices in mind: Start with the different entities in your table. If you are storing multiple different types of data in a single table—such as employees, departments, customers, and orders — be sure your primary key has a way to distinctly identify each entity and enable core actions on individual items. Use prefixes to distinguish between entity types. Using prefixes to distinguish between entity types can prevent collisions and assist in querying. For example, if you have both customers and employees in the same table, the primary key for a customer could be CUSTOMER#\u0026lt;CUSTOMERID\u0026gt;, and the primary key for an employee could be EMPLOYEE#\u0026lt;EMPLOYEEID\u0026gt;. Focus on single-item actions first, and then add multiple-item actions if possible. For a primary key, it’s important that you can satisfy the read and write options on a single item by using the single-item APIs: GetItem , PutItem , UpdateItem , and DeleteItem . You may also be able to satisfy your multiple-item read patterns with the primary key by using Query . If not, you can add a secondary index to handle the Query use cases. With these best practices in mind, let’s design the primary key for the game application’s table and perform some basic actions.\n"
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.5/1.5.3/",
	"title": "Create DMS Resources",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s create the DMS resources for the workshop.\nGo to IAM console \u0026gt; Roles \u0026gt; Create Role Under “Select trusted entity” select “AWS service” then under “Use case” select “DMS” from the pulldown list and click the “DMS” radio button. Then click “Next” Under “Add permissions” use the search box to find the “AmazonDMSVPCManagementRole” policy and select it, then click “Next” Under “Name, review, and create” add the role name as exactly dms-vpc-role and click Create role Do not continue unless you have made the IAM role.\nLaunch the CloudFormation template in US West 2 to deploy the resources in your account: Optionally, download the YAML template and launch it your own way Click Next Confirm the Stack Name dynamodbmigration and keep the default parameters (modify if necessary) Click “Next” twice Check “I acknowledge that AWS CloudFormation might create IAM resources with custom names.” Click Submit. The CloudFormation template will take about 15 minutes to build a replication environment. You should continue the lab while the stack creates in the background. Do not wait for the stack to complete creation. Please continue the lab and allow it to create in the background\n"
},
{
	"uri": "http://Handoo464.github.io/4-lcdc/4.3/4.3.3/",
	"title": "Create Lambda Function",
	"tags": [],
	"description": "",
	"content": "Create a lambda function to copy changed records from the Orders DynamoDB streams to the OrdersHistory table.\nOpen the AWS Management Console and go to the Lambda Service dashboard. In the Functions section, click on Create function. Select Author from scratch. Set create-order-history-ddbs as the function name. Select a version of Python as the runtime. Expand the Change default execution role section. Select Create a new role from AWS policy templates. Set create-order-history-ddbs-execution-role as the role name. Select the Simple microservice permissions from the Policy templates options menu. Click Create function and wait to be redirected to the AWS Lambda console for your newly created function.\nReplace the content of lambda_function.py with the function code below.\nimport os import boto3 import datetime from aws_lambda_powertools import Logger from aws_lambda_powertools.utilities.data_classes.dynamo_db_stream_event import ( DynamoDBStreamEvent, DynamoDBRecordEventName ) from botocore.exceptions import ClientError table_name = os.getenv(\u0026#34;ORDERS_HISTORY_DB\u0026#34;) logger = Logger() dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamodb.Table(table_name) def store_history_record(old_item_image): logger.debug({\u0026#34;operation\u0026#34;: \u0026#34;store_history_record\u0026#34;, \u0026#34;old_image\u0026#34;: old_item_image}) # Set a value for the partition key - pk, and sort key - sk; for the OrdersHistory table # before writing the record to the OrdersHistory table. pk = old_item_image[\u0026#34;id\u0026#34;] sk = str(datetime.datetime.now()) old_item_image[\u0026#34;pk\u0026#34;] = pk old_item_image[\u0026#34;sk\u0026#34;] = sk try: response = table.put_item( Item=old_item_image ) logger.debug({\u0026#34;operation\u0026#34;: \u0026#34;table.put_item\u0026#34;, \u0026#34;response\u0026#34;: response}) except ClientError as err: logger.error({\u0026#34;operation\u0026#34;: \u0026#34;store_history_record\u0026#34;, \u0026#34;details\u0026#34;: err}) raise Exception(err) def lambda_handler(event, context): logger.debug({\u0026#34;operation\u0026#34;: \u0026#34;lambda_handler\u0026#34;, \u0026#34;event\u0026#34;: event, \u0026#34;context\u0026#34;: context}) event: DynamoDBStreamEvent = DynamoDBStreamEvent(event) for record in event.records: if record.event_name == DynamoDBRecordEventName.MODIFY or record.event_name == DynamoDBRecordEventName.REMOVE: logger.debug({\u0026#34;record\u0026#34;: record}) if \u0026#34;id\u0026#34; in record.dynamodb.keys: store_history_record(record.dynamodb.old_image) else: raise ValueError(\u0026#34;Expected partition key attribute - \u0026#39;id\u0026#39; not found.\u0026#34;) Line Number Description 11 - 15 Get the Orders History DynamoDB table name from an environment variable, instantiate a logger for the lambda function then create a boto3 resource for interacting with the Orders hHstory DynamoDB table. 23 - 26 Create a new item for the Orders History table using an old image of an item updated on the Orders table. Set the partition key for the new item to the id of the updated order then set the sort key for the new item to the current time. 28 - 35 Write the new item to the Orders History table. Raise an exception if the item is not successfully written to the table. 42 - 45 Process old images of update item event and delete item event received from the Orders DynamoDB table through DynamoDB streams. This lambda function receives events from DynamoDB streams and writes new items to a DynamoDB table i.e. the OrdersHistory table.\nSince we only need to record changes to items on the Orders table, the lambda function is set to process only stream events for modified and deleted items from the Orders table.\nDeploy the code changes to your function by selecting Deploy. Do not execute the lambda function you created yet. Additional configuration is required for the set up to work correctly. You will update your lambda function configuration in the next step.\n"
},
{
	"uri": "http://Handoo464.github.io/4-lcdc/4.4/4.4.3/",
	"title": "Create Lambda Function",
	"tags": [],
	"description": "",
	"content": "Create a lambda function to copy changed records from the Orders DynamoDB streams to the OrdersHistory table.\nOpen the AWS Management Console and go to the Lambda Service dashboard. In the Functions section, click on Create function. Select Author from scratch. Set create-order-history-kds as the function name. Select Python 3.11 as the runtime. Expand the Change default execution role section. Select Create a new role from AWS policy templates. Set create-order-history-kds-execution-role as the role name. Select the Simple microservice permissions from the Policy templates options menu. Click Create function and wait to be redirected to the AWS Lambda console for your newly created function. Replace the content of lambda_function.py with the function code below. import os import boto3 import datetime from aws_lambda_powertools import Logger from aws_lambda_powertools.utilities.data_classes import event_source, KinesisStreamEvent from botocore.exceptions import ClientError table_name = os.getenv(\u0026#34;ORDERS_HISTORY_DB\u0026#34;) logger = Logger() client = boto3.client(\u0026#39;dynamodb\u0026#39;) def store_history_record(old_order_image): logger.debug({\u0026#34;Saving order history\u0026#34;}) # Set a value for the partition key - pk, and sort key - sk; for the OrderHistory table # before writing the record to the OrderHistory table. pk = old_order_image[\u0026#34;id\u0026#34;] sk = str(datetime.datetime.now()) old_order_image[\u0026#34;pk\u0026#34;] = pk old_order_image[\u0026#34;sk\u0026#34;] = { \u0026#34;S\u0026#34;: sk } logger.debug({\u0026#34;old_image\u0026#34;: old_order_image}) try: response = client.put_item( TableName=table_name, Item=old_order_image ) logger.debug({\u0026#34;operation\u0026#34;: \u0026#34;table.put_item\u0026#34;, \u0026#34;response\u0026#34;: response}) except ClientError as err: logger.error({\u0026#34;operation\u0026#34;: \u0026#34;store_history_record\u0026#34;, \u0026#34;details\u0026#34;: err}) raise Exception(err) @event_source(data_class=KinesisStreamEvent) def lambda_handler(event: KinesisStreamEvent, context): logger.debug({\u0026#34;operation\u0026#34;: \u0026#34;lambda_handler\u0026#34;, \u0026#34;event\u0026#34;: event, \u0026#34;context\u0026#34;: context}) kinesis_record = next(event.records).kinesis data = kinesis_record.data_as_json() if data[\u0026#34;eventName\u0026#34;] == \u0026#34;MODIFY\u0026#34; or data[\u0026#34;eventName\u0026#34;] == \u0026#34;REMOVE\u0026#34;: logger.debug({\u0026#34;data\u0026#34;: data}) if \u0026#34;dynamodb\u0026#34; in data: if \u0026#34;Keys\u0026#34; in data[\u0026#34;dynamodb\u0026#34;]: if \u0026#34;id\u0026#34; not in data[\u0026#34;dynamodb\u0026#34;][\u0026#34;Keys\u0026#34;]: raise ValueError(\u0026#34;Expected partition key attribute - \u0026#39;id\u0026#39; not found.\u0026#34;) if \u0026#34;OldImage\u0026#34; in data[\u0026#34;dynamodb\u0026#34;]: store_history_record(data[\u0026#34;dynamodb\u0026#34;][\u0026#34;OldImage\u0026#34;]) This section for an explanation of the function code:\nLine Number Description 8 - 11 Get the Orders History DynamoDB table name from an environment variable, instantiate a logger for the lambda function then create a boto3 client for interacting with DynamoDB tables. 19 - 24 Create a new item for the Orders History table using an old image of an item updated on the Orders table. Set the partition key for the new item to the id of the updated order then set the sort key for the new item to the current time. 27 - 35 Write the new item to the Orders History table. Raise an exception if the item is not successfully written to the table. 42 - 48 Process old images of update item event and delete item event received from the Orders Kinesis Data stream. This lambda function receives events from the Orders Kinesis data stream and writes them to the OrdersHistory dynamoDB table.\nSince we only need to record changes to items on the Orders table, the lambda function is set to process only Kinesis stream events for modified and deleted items from the Orders table.\nDeploy the code changes to your function by selecting Deploy. "
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.3/",
	"title": "Exercise 2: Sequential and Parallel Table Scans",
	"tags": [],
	"description": "",
	"content": "This exercise demonstrates the two methods of DynamoDB table scanning: sequential and parallel.\nEven though DynamoDB distributes items across multiple physical partitions, a Scan operation can only read one partition at a time. To learn more, read our documentation page on partitions and data distribution . For this reason, the throughput of a Scan is constrained by the maximum throughput of a single partition.\nIn order to maximize the utilization of table-level provisioning, use a parallel Scan to logically divide a table (or secondary index) into multiple logical segments, and use multiple application workers to scan these logical segments in parallel. Each application worker can be a thread in programming languages that support multithreading or an operating system process. To learn more about how to implement a parallel scan, read our developer documentation page on parallel scans . The Scan API is not suitable for all query patterns, and for more information on why scans are less efficient than queries please read about the performance implications of Scan in our documentation.\nThe following diagram shows how a multithreaded application performs a parallel Scan with three application worker threads. The application spawns three threads and each thread issues a Scan request, scans its designated segment, retrieving data 1 MB at a time, and returns the data to the main application thread.\n"
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.3/",
	"title": "Explore the DynamoDB Console",
	"tags": [],
	"description": "",
	"content": "In this lab we will be exploring the DynamoDB section of the AWS Management Console . There are two versions of the console and while you can always click \u0026ldquo;Revert to the current console\u0026rdquo; we will be working with V2 of the console.\nThe highest level of abstraction in DynamoDB is a Table (there isn\u0026rsquo;t a concept of a \u0026ldquo;Database\u0026rdquo; that has a bunch of tables inside of it like in other NOSQL or RDBMS services). Inside of a Table you will insert Items, which are analogous to what you might think of as a row in other services. Items are a collection of Attributes, which are analogous to columns. Every item must have a Primary Key which will uniquely identify that row (two items may not contain the same Primary Key). At a minimum when you create a table you must choose an attribute to be the Partition Key (aka the Hash Key) and you can optionally specify another attribute to be the Sort Key.\nIf your table is a Partition Key only table, then the Partition Key is the Primary Key and must uniquely identify each item. If your table has both a Partition Key and Sort Key, then it is possible to have multiple items that have the same Partition Key, but the combination of the Partition Key and Sort Key will be the Primary Key and uniquely identify the row. In other words, you can have multiple items that have the same Partition Key as long as their Sort Keys are different. Items with the same Partition Key are said to belong to the same Item Collection.\nFor more information please read about Core Concepts in DynamoDB .\nOperations in DynamoDB consume capacity from the table. When the table is using On-Demand capacity, read operations will consume Read Request Units (RRUs) and write operations will consume Write Request Units (WRUs). When the table is using Provisioned Capacity, read operations will consume Read Capacity Units (RCUs) and write operations will consume Write Capacity Units (WCUs). For more information please see the Read/Write Capacity Mode in the DynamoDB Developer Guide.\nNow lets dive into the shell and explore the DynamoDB Console.\n"
},
{
	"uri": "http://Handoo464.github.io/2-lbed/2.3/",
	"title": "Integrations",
	"tags": [],
	"description": "",
	"content": "In this section, you will configure integrations between services. You\u0026rsquo;ll first set up ML and Pipeline connectors in OpenSearch Service followed by a zero ETL connector to move data written to DynamoDB to OpenSearch. Once these integrations are set up, you\u0026rsquo;ll be able to write records to DynamoDB as your source of truth and then automatically have that data available to query in other services.\n"
},
{
	"uri": "http://Handoo464.github.io/6-leda/6.3/",
	"title": "Lab 1: Connect the pipeline",
	"tags": [],
	"description": "",
	"content": "Let’s go deep on streaming data sources with AWS Lambda! In this first lab you will process streaming data to create an end-to-end data processing pipeline. You will use Amazon Kinesis, AWS Lambda, Amazon DynamoDB and one of its powerful features called DynamoDB Streams to accomplish this. We will take some time to explain some of the key features of these services.\nAmazon DynamoDB Amazon DynamoDB is a massive horizontally scaled NoSQL database designed for single-digit-millisecond latency with virtually no limits in scale, either in throughput or in storage. DynamoDB offers built-in security, continuous backups, automated multi-region replication, in-memory caching, and data export tools.\nThis diagram shows the features and integrations for DynamoDB:\nIf you want to learn more you can review the DynamoDB\u0026rsquo;s features page on aws.amazon.com DynamoDB Streams DynamoDB Streams provides an ordered set of changes from your DynamoDB table for up to 24 hours after items are written. It is a change data capture (CDC) service, and once you enable the stream on your table changes begin to flow into the stream. DynamoDB item mutations appear exactly once in the stream, and the mutation usually includes the old and new copy of the item but you can choose to have one or the other, or to only have the item keys. The service is durable and highly available. While similar in name to Kinesis Data Streams, DynamoDB Streams has a completely different roadmap, engineering team, and server fleet.\nA DynamoDB stream is made of many shards. A shard contains a time-ordered list of changes from one DynamoDB partition. The item changes are put into the shard as stream records. Let\u0026rsquo;s unpack these terms:\nStream: A single stream of DynamoDB Streams, which contains CDC data from exactly one DynamoDB table. Streams can be enable or disabled, and at the time of enablement you can choose whether old and new item copies are kept, only one or the other are kept, or only keys. Every stream has a unique id called a stream label. DynamoDB partition: A partition is an allocation of storage for a table, backed by solid state drives (SSDs) and automatically replicated across multiple Availability Zones within an AWS Region. Every DynamoDB item will reside on exactly one partition, but the partition is 3-way replicated for durability and availability. Changes from a DynamoDB partition are copied into a stream shard in less than a second. Shard: A stream shard holds stream records. A stream is made up of many stream shards. In DynamoDB, a partition will write to exactly one stream shard, and the two are not to be confused. A stream shard is either open (new stream records can be added) or closed (no stream records can be added). A closed stream shard is defined as a shard with an EndingSequenceNumber, whereas an open stream shard as no EndingSequenceNumber. To learn more about shards and shard lineage, see our documentation on Reading and Processing a Stream . Stream record: A single mutation on the DynamoDB table. Any create, update, or delete operation on the table can generate a stream record so long as the item was actually modified. If you delete an item that doesn\u0026rsquo;t exist or \u0026ldquo;update\u0026rdquo; an item but there is no change to its attributes, a stream record is not created. A record has an ApproximateCreationDateTime timestamp accurate to the nearest millisecond for ordering across all shards along with a sequence number that can be used for ordering inside the shard, only. Stream records and the content types (NEW_IMAGE | OLD_IMAGE | NEW_AND_OLD_IMAGES | KEYS_ONLY) are explained in our StreamRecord documentation . DynamoDB Streams shard lineage When 4 hours has elapsed, when a shard reaches a predetermined size in bytes, or when a DynamoDB partition splits a shard is closed and new one(s) are created. With the exception of the first shards made when a stream is enabled, all shards have a parent shard. In order for a client such as Lambda to retrieve stream records, they must determine the shard lineage by retrieving all information about shards using the parent shard id to find which shards are the oldest and which are the newest.\nConsider a DynamoDB table with 4 partitions. Shards are changed over every 4 hours, for each partition. As you may know, there\u0026rsquo;s a 1:1 mapping between an open shard and a DynamoDB partition. Once the stream has been enabled for 24 hours (the longest retention time for stream data), there will be [(4 partitions) * (24 hours / 4 hours per shard)] = 24 stream shards. At any one time only four will be open and the rest will be closed. When you connect a Lambda function to this stream, there will be four Lambda instances.\nLet\u0026rsquo;s take some time to explain this connection and define what a Lambda instance is.\nAWS Lambda Triggers AWS Lambda is a serverless, event-driven compute service that lets you run code for virtually any type of application or backend service without provisioning or managing servers. Lambda supports many triggers including Kinesis Data Streams and DynamoDB. In this lab you will connect several functions to streaming sources, setting the batch size and concurrency. For this reason we will take time to explain how they work.\nThe Lambda service supports Lambda function triggers through what is called a event source mapping. When a Lambda function is connected to a streaming source, you choose whether to begin with the oldest records in the stream or the newest by setting the StartingPosition in the event source mapping. Then the Lambda service begins to poll the stream. The Lambda polling service then hands batches of records (size defined by the BatchSize in the event source mapping) to a Lambda instance. You can limit the number of concurrent Lambda instances using reserved concurrency.\nLambda function: From the documentation: A function is a resource that you can invoke to run your code in Lambda. A function has code to process the events that you pass into the function or that other AWS services send to the function. In this lab, we have three Lambda functions under your control. Reserved concurrency: Reserved concurrency guarantees the maximum number of concurrent instances for the function. You can reserve concurrency to prevent your function from using all the available concurrency in the account, or from overloading downstream resources. In this lab we modify one function\u0026rsquo;s concurrency to 1. Lambda instance: A single instance (a running container) of a Lambda function. Through reserved concurrency with streaming sources, the number of concurrently running Lambda instances can be controlled, however you have no control over the total number of instances. With streaming sources like DynamoDB, Lambda creates one instance for every interesting stream shard by default. Event source mapping: Event source mappings are the logical connection between a data source and a Lambda function. When you add a trigger to your Lambda function in the lab, the console calls CreateEventSourceMapping to link the streaming source to the Lambda function. The API documentation provides many options, but we\u0026rsquo;ll point out three interesting ones: BatchSize: The maximum number of stream records Lambda provides your Lambda function instance when its invoked. The Lambda function will need to process those records and return them in a timely manner. A batch size of 1 would be very inefficient because of the overhead of function invocation. The maximum value is 1,000 for DynamoDB Streams. StartingPosition: The position in a stream from which to start reading. With TRIM_HORIZON, Lambda will begin reading from the oldest data in the stream. With LATEST, Lambda will begin reading from the newest shards (the open stream shards, as defined above). As mentioned before, there is one Lambda instance per interesting shard. Over time, there can be more shards added to a stream in which case the number of running instances will grow to match the number of interesting shards. Lambda will discover the shard lineage so it can decide which shards are interesting to begin (the oldest ones, or the newest ones depending on this parameter setting). In this lab we will use the setting LATEST. ParallelizationFactor: How many Lambda instances to create per stream shard. The default is 1, but you could increase it to 10 instances per shard. With a value above 1, the Lambda service uses a consistent hashing scheme to make sure the same DynamoDB item keys are passed to the same instance, maintaining the ordering of the stream shard. ParallelizationFactor essentially \u0026ldquo;shards\u0026rdquo; a stream shard to increase processing speed when your Lambda function code is inefficient or the code\u0026rsquo;s task is complex or slow to complete. We don\u0026rsquo;t set this value during the lab. "
},
{
	"uri": "http://Handoo464.github.io/3-ladv/",
	"title": "LADV: Advanced Design Patterns for Amazon DynamoDB",
	"tags": [],
	"description": "",
	"content": "In this workshop, you review Amazon DynamoDB design patterns and best practices to build highly scalable applications that are optimized for performance and cost. This workshop implements these design patterns by using Python scripts. At the end of this workshop, you will have the knowledge to build and monitor DynamoDB applications that can grow to any size and scale.\nHere\u0026rsquo;s what this workshop includes:\nStart here: Getting Started Exercise 1: DynamoDB Capacity Units and Partitioning Exercise 2: Sequential and Parallel Table Scans Exercise 3: Global Secondary Index Write Sharding Exercise 4: Global Secondary Index Key Overloading Exercise 5: Sparse Global Secondary Indexes Exercise 6: Composite Keys Exercise 7: Adjacency Lists Exercise 8: Amazon DynamoDB Streams and AWS Lambda "
},
{
	"uri": "http://Handoo464.github.io/8-ldc/8.3/",
	"title": "Links: NoSQL Design: Reference Materials",
	"tags": [],
	"description": "",
	"content": "DynamoDB Design: Reference Materials DynamoDB Data Model Design:\nWorking with Queries in DynamoDB Advanced Design Patterns for DynamoDB DynamoDB Best Practices for Designing and Architecting with DynamoDB Best Practices for Using Sort Keys to Organize Data Using Global Secondary Indexes in DynamoDB Best Practices for Managing Many-to-Many Relationships Understanding Distributed Systems and DynamoDB:\nAmazon DynamoDB Under the Hood: How We Built a Hyper-Scale Database Amazon DynamoDB: How It Works DynamoDB Related Tools:\nNoSQL Workbench for Amazon DynamoDB EMR-DynamoDB-Connector: Access data stored in Amazon DynamoDB with Apache Hadoop, Apache Hive, and Apache Spark Online Training Courses:\nA Cloud Guru: Amazon DynamoDB Deep Dive A Cloud Guru: Amazon DynamoDB Data Modeling edX: Amazon DynamoDB: Building NoSQL Database-Driven Applications "
},
{
	"uri": "http://Handoo464.github.io/2-lbed/2.2/2.2.3/",
	"title": "Load DynamoDB Data",
	"tags": [],
	"description": "",
	"content": "Next, you\u0026rsquo;ll load example product data into your DynamoDB Table. Pipelines will move this data into OpenSearch Service in later steps.\nLoad and Review Data Return to the Cloud9 IDE. If you accidentally closed the IDE, you may search for the service in the AWS Management Console or use the Cloud9IDE URL found in the Outputs section of the CloudFormation stack.\nLoad the sample data into your DynamoDB Table.\ncd ~/environment/OpenSearchPipeline aws dynamodb batch-write-item --request-items=file://product_en.json Next, navigate to the DynamoDB section of the AWS Management Console and click Explore items and then select the ProductDetails table. This is where the product information for this exercise originates from. Review the product names to get an idea for what kind of natural language searches you might want to provide later at the end of the lab.\n"
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.1/1.1.3/",
	"title": "Load Sample Data",
	"tags": [],
	"description": "",
	"content": "Execute the command sudo su\nDownload and unzip the sample data:\ncurl -O https://amazon-dynamodb-labs.com/static/hands-on-labs/sampledata.zip unzip sampledata.zip Load the sample data using the batch-write-item CLI:\naws dynamodb batch-write-item --request-items file://ProductCatalog.json aws dynamodb batch-write-item --request-items file://Forum.json aws dynamodb batch-write-item --request-items file://Thread.json aws dynamodb batch-write-item --request-items file://Reply.json After each data load, you will receive a message indicating that there are no unprocessed items.\n{ \u0026#34;UnprocessedItems\u0026#34;: {} } Sample output "
},
{
	"uri": "http://Handoo464.github.io/5-lmr/5.3/",
	"title": "Module 2: Explore Global Tables",
	"tags": [],
	"description": "",
	"content": "Application Overview Congratulations! You now have a serverless application stack running in both Oregon and Ireland. The stacks you deployed with Chalice each contain the following core components:\nAmazon API Gateway web service that responds to HTTP GET calls and forwards them to the Lambda function. AWS Lambda function that performs read and write requests to the DynamoDB table using Python. AWS IAM role to grant necessary permissions . You then used the AWS Command Line Interface to deploy a DynamoDB Global Table called global-serverless and fill it with several items. These items represent bookmark records that can be set to record and retrieve the progress a customer has made when watching video content. There are also items representing a catalog of video content available.\nDynamoDB Global Table Details Bookmark Design Our table has a two-part primary key consisting of a Partition Key and a Sort Key, named PK and SK. Each bookmark record will have the viewer\u0026rsquo;s UserID as the PK value, and a ContentID value in the SK to denote the video they are watching. A third attribute, called Bookmark, will record the progress. The application will make periodic updates to this Bookmark attribute as the show is watched. If the user stops and then returns again later, a Get-Item call can locate the bookmark, read the Bookmark value, and queue up the video player to the right spot for the customer to continue watching.\nReplication Performance When the application stack in Oregon makes calls to DynamoDB, it connects to the DynamoDB table in the same region. We can call this local table a regional replica since it participates in Global Tables replication. Writes to any regional replica will be detected by the DynamoDB service and the new item image will be shipped and applied to all other regional replicas. The goal of Global Tables is to bring all replicas to an identical state as quickly as possible. Callers such as our Lambda function in Oregon do not need to be aware of other regions and do not connect to any global endpoint since there isn\u0026rsquo;t one.\nWrite operations to a replica table are confirmed successful to the caller with the same performance as a non-Global Table, typically within 10 milliseconds.\nThe distance between Oregon and Ireland is 4500 miles (7000 km). Information traveling at the speed of light will cross this distance in 24 ms.\nThe time it takes for DynamoDB to replicate changes to other regions can vary, but is typically 1-2 seconds. The ReplicationLatency statistic in Cloudwatch tracks the time required to replicate items.\nTesting Edge Cases in the Web Application Let\u0026rsquo;s prove that Global Tables replication is working.\nClick the plus button + in the first region and notice that the new bookmark value is displayed. Click Get-Item in the second region and compare the value to the bookmark in the first region. If they are the same, that means that Global Tables as applied the new state to all regions. Repeat steps 1 and 2 as quickly as possible. The goal is to check the bookmark in region two before the replication completes. Since replication can occur in about one second, you will have to be quick to detect this. If you do, then click Get-Item again and the synchronized value should now be shown.\nNote the app keeps a timer after every update, so you can see how many seconds have elapsed when performing a subsequent read.\nGenerate a Conflict There is an edge case with Global Tables that happens when writes to the same item occur at the same time in different regions. If the writes conflict during the 1-2 seconds of in-flight replication, then DynamoDB detects this as a Conflict, and makes a decision on which one of the writes will win the conflict. The timestamps of the updates are compared, and the later write becomes the winner. The earlier write is thrown out as if it never happened.\nIn region one, click Get-Item and note the current bookmark value. Next, click the plus button. In region two, immediately click the minus button. Carefully examine the output. Did region two change the value back to the starting value? If so, then there was no conflict. The first update replicated completely before the second update began. If the second update took the bookmark lower than the starting value, that shows there WAS a conflict and that the first update was un-done. You can read more about DynamoDB Global Tables in the final chapter of this workshop or in the Global Tables Documentation\n"
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.4/1.4.3/",
	"title": "On-Demand Backup",
	"tags": [],
	"description": "",
	"content": "You can use the DynamoDB on-demand backup capability to create full backups of your tables for long-term retention and archival for regulatory compliance needs. You can back up and restore your table data anytime with a single click on the AWS Management Console or with a single API call. Backup and restore actions run with zero impact on table performance or availability.\nFirst, go to the DynamoDB Console and click on Tables from the side menu.Choose ProductCatalog table. On the Backups tab of the ProductCatalog table, choose Create backup. Make sure that ProductCatalog is the source table name. Choose Customize settings and then select Backup with DynamoDB. Enter the name ProductCatalogBackup. Click Create backup to create the backup. While the backup is being created, the backup status is set to Creating. After the backup is complete, the backup status changes to Available.\nRestore Backup Go to the DynamoDB Console and click on Tables from the side menu.Choose ProductCatalog table. Choose Backups tab. In the list of backups, choose ProductCatalogBackup. Choose Restore. Enter ProductCatalogODRestore as the new table name. Confirm the backup name and other backup details. Choose Restore to start the restore process. The table that is being restored is shown with the status Creating. After the restore process is finished, the status of the ProductCatalogODRestore table changes to Active. To delete a backup The following procedure shows how to use the console to delete the ProductCatalogBackup. You can only delete the backup after the table ProductCatalogODRestore is done restoring.\nGo to the DynamoDB Console and click on Tables from the side menu Choose ProductCatalog table. Choose Backups tab. In the list of backups, choose ProductCatalogBackup. Click Delete: Finally, type the world Delete and click Delete to delete the backup.\n"
},
{
	"uri": "http://Handoo464.github.io/7-lgme/7.4/7.4.3/",
	"title": "Query the sparse GSI",
	"tags": [],
	"description": "",
	"content": "Now that you have configured the GSI, let’s use it to satisfy some of the access patterns.\nTo use a secondary index, there are two API calls available: Query and Scan. With Query, you must specify the partition key, and it returns a targeted result. With Scan, you don’t specify a partition key, and the operation runs across your entire table. Scans are discouraged in DynamoDB except in specific circumstances because they access every item in your database. If you have a significant amount of data in your table, scanning can take a very long time. In the next step, you see why Scans can be a powerful tool when used with sparse indexes.\nYou can use the Query API against the global secondary index (GSI) you created in the previous step to find all open games by map name. The GSI is partitioned by map name, allowing you to make targeted queries to find open games.\nIn the code you downloaded, a find_open_games_by_map.py file is in the scripts/ directory.\nimport sys import boto3 from entities import Game dynamodb = boto3.client(\u0026#39;dynamodb\u0026#39;) MAP_NAME = sys.argv[1] if len(sys.argv) == 2 else \u0026#34;Green Grasslands\u0026#34; def find_open_games_by_map(map_name): resp = dynamodb.query( TableName=\u0026#39;battle-royale\u0026#39;, IndexName=\u0026#34;OpenGamesIndex\u0026#34;, KeyConditionExpression=\u0026#34;#map = :map\u0026#34;, ExpressionAttributeNames={ \u0026#34;#map\u0026#34;: \u0026#34;map\u0026#34; }, ExpressionAttributeValues={ \u0026#34;:map\u0026#34;: { \u0026#34;S\u0026#34;: map_name }, }, ScanIndexForward=True ) games = [Game(item) for item in resp[\u0026#39;Items\u0026#39;]] return games games = find_open_games_by_map(MAP_NAME) print(f\u0026#34;Open games for map: {MAP_NAME}:\u0026#34;) for game in games: print(game) In the preceding script, the find_open_games_by_map function is similar to a function you would have in your application. The function accepts a map name and makes a query against the OpenGamesIndex to find all open games for the map. It then assembles the returned entities into Game objects that can be used in your application.\nExecute this script by running the following command in your terminal:\npython scripts/find_open_games_by_map.py The terminal will show the following output with four open games for the Green Grasslands map.\nOpen games for map: Green Grasslands: Game: 14c7f97e-8354-4ddf-985f-074970818215 Map: Green Grasslands Game: 3d4285f0-e52b-401a-a59b-112b38c4a26b Map: Green Grasslands Game: 683680f0-02b0-4e5e-a36a-be4e00fc93f3 Map: Green Grasslands Game: 0ab37cf1-fc60-4d93-b72b-89335f759581 Map: Green Grasslands You can run the python script again and use a specific map name. Try running the code below for the map named Dirty Desert.\npython scripts/find_open_games_by_map.py \u0026#34;Dirty Desert\u0026#34; The terminal will show the following output with three open games for the Dirty Desert map.\nOpen games for map: Dirty Desert: Game: d06af94a-2363-441d-a69b-49e3f85e748a Map: Dirty Desert Game: 873aaf13-0847-4661-ba26-21e0c66ebe64 Map: Dirty Desert Game: fe89e561-8a93-4e08-84d8-efa88bef383d Map: Dirty Desert Additionally, using PartiQL , you can run SQL-compatible query language to retrieve items from the table and its indexes in DynamoDB.\nYou can navigate to PartiQL editor in the left hand menu as shown below after navigating to the DynamoDB service under Services, Database, DynamoDB in the AWS console, and run a Query to receive a similar result.\nIn the query window, you can run the SQL query from below. You will see the same four open games for the Green Grasslands map as above:\nSELECT * FROM \u0026#34;battle-royale\u0026#34;.\u0026#34;OpenGamesIndex\u0026#34; WHERE map = \u0026#39;Green Grasslands\u0026#39; You can change the map name in the where clause to see open games for other maps. For example, check how many open games there are for the map named Juicy Jungle.\nSELECT * FROM \u0026#34;battle-royale\u0026#34;.\u0026#34;OpenGamesIndex\u0026#34; WHERE map = \u0026#39;Juicy Jungle\u0026#39; "
},
{
	"uri": "http://Handoo464.github.io/7-lgme/7.2/7.2.3/",
	"title": "Review Access Patterns",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s take a look at the different access patterns we need to model the data for.\nConsider user profile access patterns The users of the gaming application need to create user profiles. These profiles include data such as a user name, avatar, game statistics, and other information about each user. The game displays these user profiles when a user logs in. Other users can view the profile of a user to review their game statistics and other details.\nAs a user plays games, the game statistics are updated to reflect the number of games the user has played, the number of games the user has won, and the number of kills by the user.\nBased on this information, you have three access patterns:\nCreate user profile (Write) Update user profile (Write) Get user profile (Read) Consider pre-game access patterns The game is an online multiplayer game similar to Fortnite or PUBG . Players can create a game at a particular map, and other players can join the game. When 50 players have joined the game, the game starts and no additional players can join.\nWhen searching for games to join, players may want to play a particular map. Other players won’t care about the map and will want to browse open games across all maps.\nBased on this information, you have the following seven access patterns:\nCreate game (Write) Find open games (Read) Find open games by map (Read) View game (Read) View users in game (Read) Join game for a user (Write) Start game (Write) In-game and post-game access patterns Finally, let’s consider the access patterns during and after a game.\nDuring the game, players try to defeat other players with the goal of being the last player standing. The application tracks how many kills each player has during a game as well as the amount of time a player survives in a game. If a player is one of the last three surviving in a game, the player receives a gold, silver, or bronze medal for the game.\nLater, players may want to review past games they’ve played or that other players have played.\nBased on this information, you have three access patterns:\nUpdate game for user (Write) Update game (Write) Find all past games for a user (Read) Review You have now mapped all access patterns for the gaming application. In the following modules, you implement these access patterns by using DynamoDB.\nNote that the planning phase might take a few iterations. Start with a general idea of the access patterns your application needs. Map the primary key, secondary indexes, and attributes in your table. Go back to the beginning and make sure all of your access patterns are satisfied. When you are confident the planning phase is complete, move forward with implementation.\n"
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.1/3.1.3/",
	"title": "Step 2 - Check the Python and AWS CLI installation",
	"tags": [],
	"description": "",
	"content": "Run the following command to check Python on your EC2 instance:\n#Check the python version: python --version Output:\nPython 3.10.12 Note: The major and minor version of Python may vary from what you see above\nRun the following command to check the AWS CLI on your EC2 instance:\n#Check the AWS CLI version. aws --version Sample output:\n#Note that your linux kernel version may differ from the example. aws-cli/2.13.26 Python/3.11.6 Linux/6.2.0-1013-aws exe/x86_64.ubuntu.22 prompt/off Make sure you have AWS CLI version 2.x or higher and python 3.10 or higher before proceeding. If you do not have these versions, you may have difficultly successfully completing the lab.\n"
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.9/3.9.3/",
	"title": "Step 3 - Create the Lambda function",
	"tags": [],
	"description": "",
	"content": "This AWS Lambda function will attach to the DynamoDB Stream of the logfile table to replicate item puts and deletes to the logfile_replica table. The Lambda function code has been provided for you in the file ddbreplica_lambda.py. You may review the contents of the script if you would like with vim or less.\nZip the contents of the script. We will upload this to AWS Lambda when we create the function.\nzip ddbreplica_lambda.zip ddbreplica_lambda.py lab_config.py Get the Amazon Resource Name (ARN) of the precreated IAM role so that you can associate it with the Lambda function. Run the following command to retrieve the ARN of the role that was created during the lab creation.\ncat ~/workshop/ddb-replication-role-arn.txt The output looks like the following.\narn:aws:iam::\u0026lt;ACCOUNTID\u0026gt;:role/XXXXX-DDBReplicationRole-XXXXXXXXXXX Now, run the following command to create the Lambda function.\naws lambda create-function \\ --function-name ddbreplica_lambda --zip-file fileb://ddbreplica_lambda.zip \\ --handler ddbreplica_lambda.lambda_handler --timeout 60 --runtime python3.7 \\ --description \u0026#34;Sample lambda function for dynamodb streams\u0026#34; \\ --role $(cat ~/workshop/ddb-replication-role-arn.txt) "
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.2/3.2.3/",
	"title": "Step 3 - Load a larger file to compare the execution times",
	"tags": [],
	"description": "",
	"content": "Run the script again, but this time use a larger input data file.\npython load_logfile.py logfile ./data/logfile_medium1.csv Parameters: 1) Table name = logfile 2) File name = logfile_medium1.csv\nThe output will look like the following. It will run slower toward the end and take anywhere from one to three minutes to complete, depending on how quickly you run this command after Step 2.\nrow: 100 in 0.490761995316 ... row: 2000 in 3.188856363296509 RowCount: 2000, Total seconds: 75.0764648914 OR:\nrow: 100 in 0.490761995316 ... row: 2000 in 18.479122161865234 RowCount: 2000, Total seconds: 133.84829711914062 Review the output: You will notice that the load time for each batch of 100 rows was frequently above five seconds. This is because in each multisecond batch, you are seeing throttles that cause the Boto3 SDK to slow down the rate of inserts (also known as exponential backoff). The Boto3 SDK is waiting for DynamoDB to replenish the capacity of the DynamoDB table, which occurs every second for provisioned throughput tables. In Amazon CloudWatch, these throttles appear under the metric name WriteThrottleEvents.\n"
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.7/3.7.3/",
	"title": "Step 3 - Query all the employees of a city",
	"tags": [],
	"description": "",
	"content": "You have a new global secondary index that you can use for querying employees by city. Run the following Python command to list all employees by department in Dallas, Texas.\npython query_city_dept.py employees TX --citydept Dallas The result should look like the following.\nList of employees . State: TX Name: Grayce Duligal. City: Dallas. Dept: Development Name: Jere Vaughn. City: Dallas. Dept: Development Name: Valeria Gilliatt. City: Dallas. Dept: Development ... Name: Brittani Hunn. City: Dallas. Dept: Support Name: Oby Peniello. City: Dallas. Dept: Support Total of employees: 47. Execution time: 0.21702003479 seconds "
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.8/3.8.3/",
	"title": "Step 3 - Query the table&#39;s invoice details",
	"tags": [],
	"description": "",
	"content": "Run the following script to query the table’s invoice details.\npython query_invoiceandbilling.py InvoiceAndBills \u0026#39;I#1420\u0026#39; Here\u0026rsquo;s a look at the output.\n========================================================= Invoice ID:I#1420, BillID:B#2485, BillAmount:$135,986.00 , BillBalance:$28,322,352.00 Invoice ID:I#1420, BillID:B#2823, BillAmount:$592,769.00 , BillBalance:$8,382,270.00 Invoice ID:I#1420, Customer ID:C#1420 Invoice ID:I#1420, InvoiceStatus:Cancelled, InvoiceBalance:$28,458,338.00 , InvoiceDate:10/31/17, InvoiceDueDate:11/20/17 ========================================================= Review the invoice details, customer details, and bill details. Notice how the results show the relationships between invoice ID, customer ID, and bill ID entities.\n"
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.6/3.6.3/",
	"title": "Step 3 - Scan the employees table to find managers by using the sparse global secondary index",
	"tags": [],
	"description": "",
	"content": "Step 3 - Scan the employees table to find managers by using the sparse global secondary index Now, scan the new global secondary index GSI_2 on the employees table. In using our new sparse index we expect that we\u0026rsquo;ll consume read capacity for fewer items. We\u0026rsquo;ll use the sparse index as a very effective filter to improve efficiency for this access pattern.\nresponse = table.scan( Limit=pageSize, IndexName=\u0026#39;GSI_2\u0026#39; ) Run the following AWS CLI command to execute this scan using the sparse index.\npython scan_for_managers_gsi.py employees 100 Parameters:\nTable name = employees Page size = 100 (this is size of the pagination for the scan). The following output includes the scanned count and the execution time.\nNumber of managers: 84. # of records scanned: 84. Execution time: 0.287754058838 seconds Observe the scanned count and execution time using the sparse index. How does this compare to the result achieved from the Scan of the base table in Step 2? The sparse index has less data and is more efficient.\n"
},
{
	"uri": "http://Handoo464.github.io/6-leda/6.3/6.3.3/",
	"title": "Step 3: Connect ReduceLambda",
	"tags": [],
	"description": "",
	"content": "The objective of this last step in Lab 1 is to correctly configure the ReduceLambda function, connect it to the DynamoDB stream of ReduceTable, and ensure the total aggregates are written to the AggregateTable. When you successfully complete this step, you will begin to accumulate points on the scoreboard.\nConfigure Lambda concurrency We start by setting the concurrency of the ReduceLambda function to 1. This ensures that there is only a single active instance of the ReduceLambda function at any time. This is desired because we want to avoid write conflicts, where multiple instances attempt to update the AggregateTable at the same time. From a performance point-of-view, a single Lambda instance can handle the aggregation of the entire pipeline because incoming messages are already pre-aggregated by the MapLambda functions.\nNavigate to the AWS Lambda service within the AWS Management Console. Click on the function ReduceLambda to edit its configuration (see figure below). Open the Configuration tab, then select Concurrency on the left. Click the Edit button in the top right corner, select Reserve concurrency and enter 1. After you clicked Save, your configuration should look like the image below. Connect the ReduceLambda to the ReduceTable stream Next, we want to connect the ReduceLambda function to the DynamoDB stream of the ReduceTable.\nThe function overview shows that the ReduceLambda function does not have a trigger. Click on the button Add trigger. Specify the following configuration:\nIn the drop down select DynamoDB as the data source. In the DynamoDB table select ReduceTable. Set Batch size to 1000. Click the Add button in the bottom right corner.\nYou will see an error here! Before we can enable this trigger we need to add IAM permissions to this Lambda functions.\nAdd required IAM permissions The error message above informs you that the ReduceLambda function doesn\u0026rsquo;t have the necessary permissions to read from the stream of the ReduceTable. While we have already assigned IAM roles with the required privileges to the StateLambda and the MapLambda, it\u0026rsquo;s left to you to do it for the ReduceLambda function:\nKeep your current Lambda console tab open on the page where you received the IAM error trying to add the trigger to the ReduceLambda function. Shortly you will need it open to retry the request. Open a new browser tab, go the AWS Lambda service and select the ReduceLambda function. Navigate to the Configuration tab and click on Permissions. You should see the Lambda execution role called ReduceLambdaRole. Click on this role to modify it. Now you\u0026rsquo;re redirected to the IAM service, where you see the details of the ReduceLambdaRole. There is a policy associated with this role, the ReduceLambdaPolicy. Expand the view to see the current permissions of the ReduceLambda function. Now, click on the button Edit to add additional permissions. Modify the IAM policy There is already an IAM permission in place for DynamoDB: this is necessary to ensure the workshop runs as expected. Don\u0026rsquo;t get confused by this and please don\u0026rsquo;t delete the permissions we\u0026rsquo;ve already granted! All of the Lambda functions need to be able to access the ParameterTable to check the current progress of the lab and the respective failure modes.\nFirst we need to add permissions so the ReduceLambda function is able to read messages from the stream of the ReduceTable. Click on Add new statement For Service, select DynamoDB Under Access level - read, check the following four checkboxes: DescribeStream, GetRecords, GetShardIterator, and ListStreams Now we need to associate these permissions with specific resources (e.g. we want the ReduceLambda to be able to read exclusively from the ReduceTable alone). Hence, under Add a resouce, and click on Add. Then in Resource type choose stream. Next, fill out the following:\n{Region} - The lab defaults to us-west-2, but verify your region and ensure the correct one is entered {Account} - The AWS account id. You can put an asterisk here if you don\u0026rsquo;t want to get the exact account id. {TableName} - The name should be ReduceTable {StreamLabel} - Add an asterisk * so that any stream label is supported. A Stream label is a unique identifier for a DynamoDB stream. Finally, click on Add resource. You\u0026rsquo;ve now granted permission for the ReduceLambda to read from the ReduceTable stream, but there is more to be done still. Be sure to remove all curly braces from your ARN before clicking Add resource\nIf we make no further change, the ReduceLambda function will not be able to update the final results in the AggregateTable. We must modify the policy to add additional permissions to grant UpdateItem access to the function.\nClick on Add new statement For Service, select DynamoDB Under Access level - read or write, select the checkbox UpdateItem Again, we want to associate theses permissions with a specific resources: We want the ReduceLambda to be able to write to the AggregateTable alone. Hence, click on Add a resource and in the Resource type drop down choose table. Next, enter the values for Region (using the same region as before), Account (consider using an asterisk *), and TableName (this time it should be AggregateTable). Click Add resource. Finally, click Next and then Save changes in the bottom right corner. Try again to connect ReduceLambda to the ReduceTable stream If all of the above steps are executed correctly you will be able to connect the ReduceLambda to the DynamoDB stream of the ReduceTable by switching back to the open tab and again trying to click on Add. You may need to wait a couple of seconds for the IAM policy changes to propagate.\nIf you\u0026rsquo;re not able to add the trigger, this may be due to a misconfiguration of the IAM policy. If you need help, go to Summary \u0026amp; Conclusions on the left, then Solutions, and you should see the desired ReduceLambdaPolicy.\nHow do you know it is working? If everything was done right, then the DynamoDB stream of the ReduceTable should trigger the ReduceLambda. Therefore, you should be able to see logs for each Lambda invocation under the Monitor -\u0026gt; Logs tab.\nAnother way to verify it is working is to observe the items written by ReduceLambda to the DynamoDB table AggregateTable. To do that, navigate to the DynamoDB service in the AWS console, click Items on the left, and select AggregateTable. At this stage you should see multiple rows similar to the image below.\nAWS Event: If Steps 1, 2, and 3 of Lab 1 were completed successfully you should start gaining score points within one to two minutes. Please check the scoreboard! Ask your lab moderator to provide a link to the scoreboard.\n"
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.2/1.2.3/",
	"title": "Working with Table Scans",
	"tags": [],
	"description": "",
	"content": "As mentioned previously, the Scan API which can be invoked using the scan CLI command . Scan will do a full table scan and return the items in 1MB chunks.\nThe Scan API is similar to the Query API except that since we want to scan the whole table and not just a single Item Collection, there is no Key Condition Expression for a Scan. However, you can specify a Filter Expression which will reduce the size of the result set (even though it will not reduce the amount of capacity consumed).\nFor example, we could find all the replies in the Reply that were posted by User A:\naws dynamodb scan \\ --table-name Reply \\ --filter-expression \u0026#39;PostedBy = :user\u0026#39; \\ --expression-attribute-values \u0026#39;{ \u0026#34;:user\u0026#34; : {\u0026#34;S\u0026#34;: \u0026#34;User A\u0026#34;} }\u0026#39; \\ --return-consumed-capacity TOTAL Note than in the response we see these lines:\n\u0026#34;Count\u0026#34;: 3, \u0026#34;ScannedCount\u0026#34;: 4, This is telling us that the Scan scanned all 4 items (ScannedCount) in the table and thats what we were charged to read, but the Filter Expression reduced our result set size down to 3 items (Count).\nSometimes when scanning data there will be more data than will fit in the response if the scan hits the 1MB limit on the server side, or there may be more items left than our specified \u0026ndash;max-items parameter. In that case, the scan response will include a NextToken which we can then issue to a subsequent scan call to pick up where we left off. For example in the previous scan we know that 3 items were in the result set. Let\u0026rsquo;s run it again but limit the max items to 2:\naws dynamodb scan \\ --table-name Reply \\ --filter-expression \u0026#39;PostedBy = :user\u0026#39; \\ --expression-attribute-values \u0026#39;{ \u0026#34;:user\u0026#34; : {\u0026#34;S\u0026#34;: \u0026#34;User A\u0026#34;} }\u0026#39; \\ --max-items 2 \\ --return-consumed-capacity TOTAL We can see in the response that there is a\n1 \u0026#34;NextToken\u0026#34;: \u0026#34;eyJFeGNsdXNpdmVTdGFydEtleSI6IG51bGwsICJib3RvX3RydW5jYXRlX2Ftb3VudCI6IDJ9\u0026#34; So we can invoke the scan request again, this time passing that NextToken value into the \u0026ndash;starting-token parameter:\naws dynamodb scan \\ --table-name Reply \\ --filter-expression \u0026#39;PostedBy = :user\u0026#39; \\ --expression-attribute-values \u0026#39;{ \u0026#34;:user\u0026#34; : {\u0026#34;S\u0026#34;: \u0026#34;User A\u0026#34;} }\u0026#39; \\ --max-items 2 \\ --starting-token eyJFeGNsdXNpdmVTdGFydEtleSI6IG51bGwsICJib3RvX3RydW5jYXRlX2Ftb3VudCI6IDJ9 \\ --return-consumed-capacity TOTAL "
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.3/1.3.3/",
	"title": "Working with Table Scans",
	"tags": [],
	"description": "",
	"content": "The Scan API which can be invoked using the scan CLI command . Scan will do a full table scan and return the items in 1MB chunks.\nThe Scan API is similar to the Query API except that since we want to scan the whole table and not just a single Item Collection, there is no Key Condition Expression for a Scan. However, you can specify a Filter Expression which will reduce the size of the result set (even though it will not reduce the amount of capacity consumed).\nLet us look at the data in the Reply table which has both a Partition Key and a Sort Key. Select the left menu bar Explore items. You may need to click the hamburger menu icon to expand the left menu if its hidden. Once you enter the Explore Items you need to select the Reply table and then expand the Scan/Query items box.\nFor example, we could find all the replies in the Reply that were posted by User A.\nYou should see 3 Reply items posted by User A.\n"
},
{
	"uri": "http://Handoo464.github.io/7-lgme/7.4/",
	"title": "4. Find open games",
	"tags": [],
	"description": "",
	"content": "One of the biggest adjustments for users who are new to DynamoDB and NoSQL is how to model data to filter across an entire dataset. For example, in the gaming application, you need to find games with open spots so that you can show users which games they can join.\nIn a relational database, you would write some SQL to query the data.\nSELECT * FROM games WHERE status = “OPEN” DynamoDB can filter results on a Query or Scan operation, but DynamoDB doesn’t work like a relational database. A DynamoDB filter applies after the initial items that match the Query or Scan operation have been retrieved. The filter reduces the size of the payload sent from the DynamoDB service, but the number of items retrieved initially is subject to the DynamoDB size limits .\nFortunately, there are a number of ways you can allow filtered queries against your dataset in DynamoDB. To provide efficient filters on your DynamoDB table, you need to plan the filters into your table’s data model from the beginning. Remember the lesson you learned in the earlier modules of this lab: Consider your access patterns, and then design your table.\nIn the next steps, you use a global secondary index to find open games. Specifically, you will use the sparse index technique to handle this access pattern.\n"
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.4/",
	"title": "Backups",
	"tags": [],
	"description": "",
	"content": "Amazon DynamoDB offers two types of backup, on-demand and point-in-time recovery (PITR). PITR is on a rolling window, on-demand backups stay around forever (even after the table is deleted) until someone tells DynamoDB to delete the backups. When you enable PITR, DynamoDB backs up your table data automatically with per-second granularity. The retention period is a fixed 35 days (5 calendar weeks) and can\u0026rsquo;t be modified. However, large enterprise customers who are used to deploying tradition backup solutions in their data centers often want a centralized backup solution that can schedule backups through “jobs” and handle tasks such as expiring/deleting older backups time, monitoring the status of on-going backups, verifying compliance, and finding / restoring backups, all using a central console. At the same time they don\u0026rsquo;t want to manually manage their backups, create their own tools via scripts or AWS Lambda functions, or use a different solution for each application they have to protect. They want the ability to have a standardized way to manage their backups at scale to the resources in their AWS account.\nAWS Backup aims to be the single point of centralize backup management and source of truth that customers can rely on. You can schedule periodic or future backups by using AWS Backup, The backup plans include schedules and retention policies for your resources. AWS Backup creates the backups and deletes prior backups based on your retention schedule. Backups of resources are always required in case of disaster recovery.\nAWS Backup removes the undifferentiated heavy lifting of manually making and deleting On-demand backups by automating the schedule and deletion for you. In this lab we will be exploring how to schedule periodic backups of a DynamoDB table using AWS Backup. I am going to create a backup plan where I take daily backup and keep it for a month. Next I setup a backup rule to transition the backup to cold storage after 31 days and auto-delete the backup after 366 days from backup creation date.\nAlso, I show you how you can restrict people in your organization to delete the backups from AWS backup and DynamoDB console while able to do others operations like create backup, create table etc.\nNow lets dive into the different DynamoDB backup options.\n"
},
{
	"uri": "http://Handoo464.github.io/4-lcdc/4.4/",
	"title": "Change Data Capture using Kinesis Data Streams",
	"tags": [],
	"description": "",
	"content": "Amazon Kinesis Data Streams can be used to collect and process large streams of data records from applications that produce streaming data in real-time. At a high level, data producers push data records to Amazon Kinesis Data Streams and consumers can read and process the data in real-time.\nData on Amazon Kinesis Data Streams is by default available for 24 hours after the data is written to the stream and the retention period can be increased to a maximum of 365 days.\nAmazon DynamoDB has native integration with Kinesis streams so Kinesis Data Streams can also be used to record item level changes to DynamoDB tables.\nIn this chapter, you will repeat the process of capturing item level changes on a DynamoDB table and write those changes to a different DynamoDB table. But in this section, change data capture will be done using Amazon Kinesis Data Streams.\nThe architecture of the resulting solution is shown in the image below.\n"
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.1/1.1.4/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "If you used an account provided by Workshop Studio Event Delivery, you do not need to do any cleanup. The account terminates when the event is over.\nIf you used your own account, please remove the following resources:\nThe four DynamoDB tables created in the Getting Started section of the lab:\naws dynamodb delete-table \\ --table-name ProductCatalog aws dynamodb delete-table \\ --table-name Forum aws dynamodb delete-table \\ --table-name Thread aws dynamodb delete-table \\ --table-name Reply The Cloudformation template that was launched during the getting started section. Navigate to the Cloudformation console, select the amazon-dynamodb-labs stack and click Delete.\nThis should wrap up the cleanup process.\n"
},
{
	"uri": "http://Handoo464.github.io/4-lcdc/4.4/4.4.4/",
	"title": "Configure Lambda Function",
	"tags": [],
	"description": "",
	"content": "Configure your lambda function to copy changed records from the Orders Kinesis Data Stream to the OrdersHistory table.\nGo to the IAM dashboard on the AWS Management Console and inspect the IAM policy, i.e. AWSLambdaMicroserviceExecutionRole\u0026hellip;, created when you created the create-order-history-kds lambda function. { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:DeleteItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:Scan\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:{aws-region}:{aws-account-id}:table/*\u0026#34; } ] } Update the policy statement by editing and replacing the existing policy using the following IAM policy statement. { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;kinesis:DescribeStream\u0026#34;, \u0026#34;kinesis:GetRecords\u0026#34;, \u0026#34;kinesis:GetShardIterator\u0026#34;, \u0026#34;kinesis:ListStreams\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:kinesis:{aws-region}:{aws-account-id}:stream/Orders\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:{aws-region}:{aws-account-id}:table/OrdersHistory\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;sqs:SendMessage\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:sqs:{aws-region}:{aws-account-id}:orders-kds-dlq\u0026#34; } ] } Replace {aws-region} and {aws-account-id} in the policy statement above with your AWS region and account ID respectively.\nSelect Layers then select Add a Layer. Select Specify an ARN, enter the Lambda Layer ARN below. 1 arn:aws:lambda:{aws-region}:017000801446:layer:AWSLambdaPowertoolsPythonV2:58 Click Verify then select Add. Replace {aws-region} with ID for the AWS region that you are currently working on.\nGo to the configuration section of the lambda console editor. Select Environment variables then select Edit. Add a new environment variable called ORDERS_HISTORY_DB and set its value to OrdersHistory. Go to the configuration section of the lambda console editor. Select Triggers then select Add trigger. Select Kinesis as the trigger source. Select the Orders DynamoDB table. Set the Batch size to 10 and leave all other values unchanged. Click Additional settings to expand the section. Provide the ARN of the orders-kds-dlq SQS queue you created earlier. Set the Retry attempts to 3. Select Add. "
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.4/",
	"title": "Exercise 3: Global Secondary Index Write Sharding",
	"tags": [],
	"description": "",
	"content": "The primary key of a DynamoDB table or a global secondary index consists of a partition key and an optional sort key. The way you design the content of those keys is extremely important for the structure and performance of your database. Partition key values determine the logical partitions in which your data is stored. Therefore, it is important to choose a partition key value that uniformly distributes the workload across all partitions in the table or global secondary index. For a discussion on how to choose the right partition key, see our blog titled Choosing the Right DynamoDB Partition Key .\nIn this exercise, you learn about global secondary index write sharding, which is an effective design pattern to query selectively the items spread across different logical partitions in a very large table. Let\u0026rsquo;s review the server access logs example [from Exercise 1]{href=\u0026quot;/design-patterns/ex1capacity\u0026quot;}, which is based on Apache service access logs. This time, you query the items with response code 4xx. Note that the items with response code 4xx are a very small percentage of the total data and do not have an even distribution by response code. For example, the response code 200 OK has more records than the others, which is as expected for any web application.\nThe following chart shows the distribution of the log records by response code for the sample file, logfile_medium1.csv.\nYou will create a write-sharded global secondary index on a table to randomize the writes across multiple logical partition key values. In effect, this increases the write and read throughput of the application. To apply this design pattern, you can create a random number from a fixed set (for example, 1 to 10), and use this number as the logical partition key for a global secondary index. Because you are randomizing the partition key, the writes to the table are spread evenly across all of the partition key values independent of any attribute. This will yield better parallelism and higher overall throughput. Also, if the application needs to query the log records by a specific response code on a specific date, you can create a composite sort key using a combination of the response code and the date.\nIn this exercise, you create a global secondary index using random values for the partition key, and the composite key responsecode#date#hourofday as the sort key. The logfile_scan table that you created and populated during the preparation phase of the workshop already has these two attributes. If you did not complete the setup steps, return to Setup - Step 6 and complete the step. These attributes were created using the following code.\nSHARDS = 10 newitem[\u0026#39;GSI_1_PK\u0026#39;] = \u0026#34;shard#{}\u0026#34;.format((newitem[\u0026#39;requestid\u0026#39;] % SHARDS) + 1) newitem[\u0026#39;GSI_1_SK\u0026#39;] = row[7] + \u0026#34;#\u0026#34; + row[2] + \u0026#34;#\u0026#34; + row[3] "
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.5/1.5.4/",
	"title": "Explore Source Model",
	"tags": [],
	"description": "",
	"content": "IMDb (Internet Movie Database) is one of the most recognized names for its comprehensive online database collection of movies, films, TV series and so on. The exercise is going to use subset of IMDb dataset (available in TSV format). This workshop will utilize 6 IMDb dataset that are related to US based movies since year 2000. The dataset has around 106K+ movies, ratings, votes and cast/crew information.\nThe CloudFormation template has launched EC2 Amazon Linux 2 instance with MySQL installed and running. It has created imdb database, 6 new tables (one for each IMDb dataset), downloaded IMDb TSV files to MySQL server local directory and uploaded the files to 6 new tables. To explore dataset, follow below instructions to login EC2 server. It has also configured a remote MySQL user based on the CloudFormation input parameter.\nGo to EC2 console Select the MySQL-Instance and click Connect Make sure ec2-user is filled under the User name field. Click Connect Elevate your privilege using sudo command\nsudo su Go to the file directory\ncd /var/lib/mysql-files/ ls -lrt You can see all the 6 files copied from the IMDB dataset to the local EC2 directory Feel free to explore the files.\nGo to AWS CloudFormation Stacks and click on the stack you created earlier. Go to the Parameters tab and copy the user name and password mentioned next to DbMasterUsername \u0026amp; DbMasterPassword Go back to EC2 Instance console and login to mysql\nmysql -u DbMasterUsername -pDbMasterPassword 10. Congratulations! You are now connected to a self-managed MySQL source database on EC2. In next steps, we will explore database and tables hosting IMDb datasets\nuse imdb; 11. Show all the tables created;\nshow tables; For illustration purpose, below is a logical diagram represents relationship between various source tables hosting IMDb dataset.\ntitle_basics table has movies published in US after year 2000. tconst is an alphanumeric key uniquely assigned to each movie. title_akas has published regions, languages and respective movie titles. It\u0026rsquo;s 1:many relationship with title_basics table. title_ratings has movies rating and vote count. For this exercise, we can assume the information has high frequency update post movie release. It\u0026rsquo;s 1:1 related with title_basics table title_principals has cast and crew information. It\u0026rsquo;s 1:many relationship with title_basics table. title_crew has writer and director information. The table is 1:1 related with title_basics table. name_basics has cast and crew details. Every member has unique nconst value assigned. We will create denormalized view with 1:1 static information and get it ready for migration to Amazon DynamoDB table. For now, go ahead and copy below code and paste into the MySQL command line. The details around target data model will be discussed in the next chapter. CREATE VIEW imdb.movies AS\\ SELECT tp.tconst,\\ tp.ordering,\\ tp.nconst,\\ tp.category,\\ tp.job,\\ tp.characters,\\ tb.titleType,\\ tb.primaryTitle,\\ tb.originalTitle,\\ tb.isAdult,\\ tb.startYear,\\ tb.endYear,\\ tb.runtimeMinutes,\\ tb.genres,\\ nm.primaryName,\\ nm.birthYear,\\ nm.deathYear,\\ nm.primaryProfession,\\ tc.directors,\\ tc.writers\\ FROM imdb.title_principals tp\\ LEFT JOIN imdb.title_basics tb ON tp.tconst = tb.tconst\\ LEFT JOIN imdb.name_basics nm ON tp.nconst = nm.nconst\\ LEFT JOIN imdb.title_crew tc ON tc.tconst = tp.tconst; Use below command to review count of records from the denormalized view. At this point, your source database is ready to migrate to Amazon DynamoDB.\nselect count(*) from imdb.movies; "
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.2/1.2.4/",
	"title": "Inserting/Updating Data",
	"tags": [],
	"description": "",
	"content": "Inserting Data The DynamoDB PutItem API is used to create a new item or to replace existing items completely with a new item. It is invoked using the put-item CLI command .\nLet\u0026rsquo;s say we wanted to insert a new item into the Reply table:\naws dynamodb put-item \\ --table-name Reply \\ --item \u0026#39;{ \u0026#34;Id\u0026#34; : {\u0026#34;S\u0026#34;: \u0026#34;Amazon DynamoDB#DynamoDB Thread 2\u0026#34;}, \u0026#34;ReplyDateTime\u0026#34; : {\u0026#34;S\u0026#34;: \u0026#34;2021-04-27T17:47:30Z\u0026#34;}, \u0026#34;Message\u0026#34; : {\u0026#34;S\u0026#34;: \u0026#34;DynamoDB Thread 2 Reply 3 text\u0026#34;}, \u0026#34;PostedBy\u0026#34; : {\u0026#34;S\u0026#34;: \u0026#34;User C\u0026#34;} }\u0026#39; \\ --return-consumed-capacity TOTAL We can see in the response that this request consume 1 WCU:\n{ \u0026#34;ConsumedCapacity\u0026#34;: { \u0026#34;TableName\u0026#34;: \u0026#34;Reply\u0026#34;, \u0026#34;CapacityUnits\u0026#34;: 1.0 } } Updating Data The DynamoDB UpdateItem API is used to create a new item or to replace existing items completely with a new item. It is invoked using the update-item CLI command . This API requires you to specify the full Primary Key and can selectively modify specific attributes without changing others(you don\u0026rsquo;t need to pass in the full item).\nThe update-item API call also allows you to specify a ConditionExpression, meaning the Update request will only execute if the ConditionExpression is satisfied. For more information please see Condition Expressions in the Developer Guide.\nLet\u0026rsquo;s say we want to update the Forum item for DynamoDB to note that there are 5 messages how instead of 4, we only want that change to execute if no other processing thread has updated the item first. This allows us to create idempotent modifications. For more information on idempotent changes please see Working With Items in the Developer Guide.\nTo do this from the CLI, we would run:\naws dynamodb update-item \\ --table-name Forum \\ --key \u0026#39;{ \u0026#34;Name\u0026#34; : {\u0026#34;S\u0026#34;: \u0026#34;Amazon DynamoDB\u0026#34;} }\u0026#39; \\ --update-expression \u0026#34;SET Messages = :newMessages\u0026#34; \\ --condition-expression \u0026#34;Messages = :oldMessages\u0026#34; \\ --expression-attribute-values \u0026#39;{ \u0026#34;:oldMessages\u0026#34; : {\u0026#34;N\u0026#34;: \u0026#34;4\u0026#34;}, \u0026#34;:newMessages\u0026#34; : {\u0026#34;N\u0026#34;: \u0026#34;5\u0026#34;} }\u0026#39; \\ --return-consumed-capacity TOTAL Note that if you run this exact same command again you will see this error:\nAn error occurred (ConditionalCheckFailedException) when calling the UpdateItem operation: The conditional request failed Because the Messages attribute had already been incremented to 5 in the previous update-item call, the second request fails with a ConditionalCheckFailedException.\n"
},
{
	"uri": "http://Handoo464.github.io/6-leda/6.4/",
	"title": "Lab 2: Ensure fault tolerance and exactly once processing",
	"tags": [],
	"description": "",
	"content": " Points and scoreboard only apply when this lab is run during an AWS Event.\nAre you ready to start Lab 2? Before proceeding to Lab 2 let\u0026rsquo;s verify that Lab 1 was successfully completed. There are two phases to complete before continuing:\nFirst, you started to accumulate points on the scoreboard. If you have non-zero points then this phase is complete. Open the scoreboard and find your team. Find your account ID in the AWS Management Console on the top right of the console. Second, you need accumulate 300 points to continue. The workshop will automatically switch to Lab 2 when you reach this milestone, and this phase is complete. Once 300 points are accumulated, new failure modes will be introduced and all three Lambda functions (StateLambda, MapLambda, and ReduceLambda) will start failing randomly. This is a pre-programmed evolution of the workshop. In the Lambda console, click on any of the three Lambda functions, navigate to the Monitor tab and then to the Metrics sub-tab. You should expect to see a non-zero error rate on some of the graphs! If the dashboard has 300 points, then congratulations: you can start Lab 2!\nLet’s utilize different features of DynamoDB to ensure data integrity and fault tolerance In Lab 2 we will achieve exactly once processing of the messages. To make sure that our pipeline can withstand different failure modes and achieve exactly once message processing.\nContinue on to: Step 1\n"
},
{
	"uri": "http://Handoo464.github.io/4-lcdc/",
	"title": "LCDC: Change Data Capture for Amazon DynamoDB",
	"tags": [],
	"description": "",
	"content": "In this workshop, you will learn how to perform change data capture of item level changes on DynamoDB tables using Amazon DynamoDB Streams and Amazon Kinesis Data Streams . This technique allows you to develop event-driven solutions that are initiated by alterations made to item-level data stored in DynamoDB.\nHere is what this workshop includes:\nGetting Started Scenario Overview Change Data Capture using DynamoDB Streams Change Data Capture using Kinesis Data Streams Summary and Clean Up Target audience This workshop is designed for developers, engineers, and database administrators who are involved in designing and maintaining DynamoDB applications.\nRequirements Basic knowledge of AWS services Among other services this lab will guide you through the use of AWS Cloud9 and AWS Lambda . Basic understanding of DynamoDB If you’re not familiar with DynamoDB or are not participating in this lab as part of an AWS event, consider reviewing the documentation on \u0026ldquo;What is Amazon DynamoDB? \u0026quot; Recommended study before taking the lab If you\u0026rsquo;re not part of an AWS event and you haven\u0026rsquo;t recently reviewed DynamoDB design concepts, we suggest you watch this video on Advanced Design Patterns for DynamoDB , which is about an hour in duration.\n"
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.3/1.3.4/",
	"title": "Modifying Data",
	"tags": [],
	"description": "",
	"content": "Inserting Data The DynamoDB PutItem API is used to create a new item or to replace existing items completely with a new item. It is invoked using the put-item CLI command .\nLet\u0026rsquo;s say we wanted to insert a new item into the Reply table from the console. First, navigate to the Reply table click the Create Item button.\nClick JSON view, ensure View DynamoDB JSON is deselected, paste the following JSON, and then click Create Item to insert the new item.\n{ \u0026#34;Id\u0026#34; : \u0026#34;Amazon DynamoDB#DynamoDB Thread 2\u0026#34;, \u0026#34;ReplyDateTime\u0026#34; : \u0026#34;2021-04-27T17:47:30Z\u0026#34;, \u0026#34;Message\u0026#34; : \u0026#34;DynamoDB Thread 2 Reply 3 text\u0026#34;, \u0026#34;PostedBy\u0026#34; : \u0026#34;User C\u0026#34; } Updating or Deleting Data The DynamoDB UpdateItem API is used to create a new item or to replace existing items completely with a new item. It is invoked using the update-item CLI command . This API requires you to specify the full Primary Key and can selectively modify specific attributes without changing others(you don\u0026rsquo;t need to pass in the full item).\nThe DynamoDB DeleteItem API is used to create a new item or to replace existing items completely with a new item. It is invoked using the delete-item CLI command .\nYou can easily modify or delete an item using the console by selecting the checkbox next to the item of interest, clicking the Actions dropdown and performing the desired action.\n"
},
{
	"uri": "http://Handoo464.github.io/5-lmr/5.4/",
	"title": "Module 3: Interact with the Globalflix Interface",
	"tags": [],
	"description": "",
	"content": "Navigate to the Globalflix Web App Click the link below to open the Globalflix web app, or if you still have the app open from module 2, click the Globalflix logo on the top right. https://amazon-dynamodb-labs.com/static/global-serverless-application/web/globalflix.html If you have already successfully loaded the API urls in the last module, you should see a grid of 12 video thumbnails. This has been displayed by performing a query against DynamoDB for the sample data you loaded in module 1.\nThe API region(s) you set in the previous module should be displayed on the top right, with the current \u0026ldquo;active\u0026rdquo; region in solid orange.\nSelect the outlined region to perform a local \u0026ldquo;failover\u0026rdquo; to the second region For the sake of time in this module we are executing this regional failover within the web application, but in production a more common pattern is to make use of something like Amazon Route 53 Application Recovery Controller to manage the health checks, failover, and recovery of your regional services.\nWatch a video to start loading bookmarks into the database Click on any of the videos to load the video player page. On this page, the selected video will start playing in the middle with the following metrics displayed around it (from left to right):\nVideo Progress: Current Timestamp of your progress through the video as seen by your web browser Write Latency: Time in milliseconds it took to write the last bookmark to the database Region 1 and 2 Progress: Current Timestamp of your progress through the video when reading the bookmark item from each region Underneath the player you can see a log of each write operation performed, note the region being used.\nSimulate a Region Failure Return to the AWS console and search for \u0026ldquo;Lambda\u0026rdquo; using the search bar at the top A function named \u0026ldquo;global-serverless-dev\u0026rdquo; should be listed on the functions page, click the function name. If you do not see it listed check to make sure you are in one of the two regions you deployed to with Chalice on the top right of the page Use the Throttle button on the top right of the page to set the Lambda functions maximum concurrency to 0, halting any future invocations of the function in this region. Switch back to the Globalflix video player and observe that an API failure in that region has been detected Wait for the application to wait to failover Even though the application stack in that region is now unresponsive, because we are using DynamoDB Global Tables, data updates are still being replicated into that region. When the service recovers, we need not worry about data loss during that outage.\nYou can verify this if you would like by running a query against the \u0026ldquo;global-serverless\u0026rdquo; DynamoDB Table in each of your regions\naws dynamodb query \\ --table-name global-serverless \\ --region us-west-2 \\ --key-condition-expression \u0026#34;PK = :PK\u0026#34; \\ --expression-attribute-values \u0026#39;{\u0026#34;:PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;user10\u0026#34;}}\u0026#39; \\ --query \u0026#39;Items[*].bookmark.S\u0026#39; \\ --output text | awk \u0026#39;{print $1\u0026#34;: us-west-2\u0026#34;}\u0026#39; aws dynamodb query \\ --table-name global-serverless \\ --region eu-west-1 \\ --key-condition-expression \u0026#34;PK = :PK\u0026#34; \\ --expression-attribute-values \u0026#39;{\u0026#34;:PK\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;user10\u0026#34;}}\u0026#39; \\ --query \u0026#39;Items[*].bookmark.S\u0026#39; \\ --output text | awk \u0026#39;{print $1\u0026#34;: eu-west-1\u0026#34;}\u0026#39; Return to the Lambda console and click \u0026ldquo;Edit concurrency\u0026rdquo; at the top right Select the \u0026ldquo;Use unreserved account concurrency\u0026rdquo; button and then Save Your resilient application has now successfully tolerated a failed regional stack, failed to an alternate region, and failed back, all with zero data loss or impact to the users experience.\n"
},
{
	"uri": "http://Handoo464.github.io/2-lbed/2.4/",
	"title": "Query and Conclusion",
	"tags": [],
	"description": "",
	"content": "Now that you\u0026rsquo;ve created all required connectors and pipelines and data has replicated from DynamoDB into OpenSearch Service, you have quite a few options for how you want to query your data. You can do key/value looksups directly to DynamoDB, execute search queries against OpenSearch, and use Bedrock togther with Opensearch for natural language product recommendation.\nThis query will use OpenSearch as a vector database to find the product that most closely matches your desired intent.The contents of the OpenSearch index were created through the DynamoDB Zero ETL connector. When records are added to DynamoDB, the connector automatically moves them into OpenSearch. OpenSearch then uses the Titan Embeddings model to decorate that data.\nThe script constructs a query that searches the OpenSearch index for products that are most relevant to your input text. This is done using a \u0026ldquo;neural\u0026rdquo; query, which leverages the embeddings stored in OpenSearch to find products with similar textual content. After retrieving relevant products, the script uses Bedrock to generate a more sophisticated response through the Claude model. This involves creating a prompt that combines your original query with the retrieved data and sending this prompt to Bedrock for processing.\nReturn to the Cloud9 IDE Console.\nFirst, let\u0026rsquo;s make a request to DynamoDB directly\naws dynamodb get-item \\ --table-name ProductDetails \\ --key \u0026#39;{\u0026#34;ProductID\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;S020\u0026#34;}}\u0026#39; This is an example of a key/value lookup that DynamoDB excels at. It returns product details for a specific product, identified by its ProductID.\nNext, let\u0026rsquo;s make a search query to OpenSearch. We\u0026rsquo;ll find skirts that include \u0026ldquo;Spandex\u0026rdquo; in their description.\ncurl --request POST \\ ${OPENSEARCH_ENDPOINT}/product-details-index-en/_search \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --header \u0026#39;Accept: application/json\u0026#39; \\ --header \u0026#34;x-amz-security-token: ${METADATA_AWS_SESSION_TOKEN}\u0026#34; \\ --aws-sigv4 aws:amz:${METADATA_AWS_REGION}:es \\ --user \u0026#34;${METADATA_AWS_ACCESS_KEY_ID}:${METADATA_AWS_SECRET_ACCESS_KEY}\u0026#34; \\ --data-raw \u0026#39;{ \u0026#34;_source\u0026#34;: { \u0026#34;excludes\u0026#34;: [\u0026#34;product_embedding\u0026#34;] }, \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;Category\u0026#34;: \u0026#34;Skirt\u0026#34; } }, { \u0026#34;match_phrase\u0026#34;: { \u0026#34;Description\u0026#34;: \u0026#34;Spandex\u0026#34; } } ] } } }\u0026#39; | jq . Try changing Spandex to Polyester and see how the results change.\nFinally, let\u0026rsquo;s ask Bedrock to provide some product recommendations using one of the scripted provided with the lab.\nThis query will use OpenSearch as a vector database to find the product that most closely matches your desired intent. The contents of the OpenSearch index were created through the DynamoDB Zero ETL connector. When records are added to DynamoDB, the connector automatically moves them into OpenSearch. OpenSearch then uses the Titan Embeddings model to decorate that data.\nThe script constructs a query that searches the OpenSearch index for products that are most relevant to your input text. This is done using a \u0026ldquo;neural\u0026rdquo; query, which leverages the embeddings stored in OpenSearch to find products with similar textual content. After retrieving relevant products, the script uses Bedrock to generate a more sophisticated response through the Claude model. This involves creating a prompt that combines your original query with the retrieved data and sending this prompt to Bedrock for processing.\nIn the console, execute the provided python script to make a query to Bedrock and return product results.\npython bedrock_query.py product_recommend en \u0026#34;I need a warm winter coat\u0026#34; $METADATA_AWS_REGION $OPENSEARCH_ENDPOINT $MODEL_ID | jq . Try adding a new item to your DynamoDB table.\naws dynamodb put-item \\ --table-name ProductDetails \\ --item \u0026#39;{ \u0026#34;ProductID\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;S021\u0026#34;}, \u0026#34;Category\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Socks\u0026#34;}, \u0026#34;Description\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;{\\\u0026#34;Style\\\u0026#34;: \\\u0026#34;Outdoor\\\u0026#34;, \\\u0026#34;Pattern\\\u0026#34;: \\\u0026#34;Striped\\\u0026#34;, \\\u0026#34;Length\\\u0026#34;: \\\u0026#34;Knee-High\\\u0026#34;, \\\u0026#34;Type\\\u0026#34;: \\\u0026#34;Thick\\\u0026#34;, \\\u0026#34;Fabric\\\u0026#34;: \\\u0026#34;Wool\\\u0026#34;, \\\u0026#34;Composition\\\u0026#34;: \\\u0026#34;80% Wool, 20% Nylon\\\u0026#34;, \\\u0026#34;Care Instructions\\\u0026#34;: \\\u0026#34;Hand wash cold, lay flat to dry\\\u0026#34;, \\\u0026#34;Ideal For\\\u0026#34;: \\\u0026#34;Outdoor Activities\\\u0026#34;, \\\u0026#34;Stretch\\\u0026#34;: \\\u0026#34;Moderate\\\u0026#34;, \\\u0026#34;Opacity\\\u0026#34;: \\\u0026#34;Opaque\\\u0026#34;, \\\u0026#34;Lining\\\u0026#34;: \\\u0026#34;No\\\u0026#34;, \\\u0026#34;Pockets\\\u0026#34;: \\\u0026#34;No Pockets\\\u0026#34;, \\\u0026#34;Closure\\\u0026#34;: \\\u0026#34;Pull Up\\\u0026#34;, \\\u0026#34;Shoe Height\\\u0026#34;: \\\u0026#34;Knee-High\\\u0026#34;, \\\u0026#34;Occasion\\\u0026#34;: \\\u0026#34;Outdoor\\\u0026#34;, \\\u0026#34;Season\\\u0026#34;: \\\u0026#34;Fall, Winter\\\u0026#34;}\u0026#34;}, \u0026#34;Image\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;https://example.com/S021.jpg\u0026#34;}, \u0026#34;ProductName\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Striped Wool Knee-High Socks\u0026#34;} }\u0026#39; Try modifying the DynamoDB get-item above to retrieve your new item. Next, try modifying the OpenSearch query to search for \u0026ldquo;Socks\u0026rdquo; that contain \u0026ldquo;Wool\u0026rdquo;. Finally, tell Bedrock \u0026ldquo;I need warm socks for hiking in winter\u0026rdquo;. Did it recommend your new item?\nKeeping querying! Don\u0026rsquo;t just stop there with your queries. Trying asking for clothing for winter (will it recommend products with wool?) or for bedtime. Note that there is a very small catalog of products to be embedded, so your search terms should be limited based on what you saw when you reviewed the DynamoDB table.\nCongratulations! You have completed the lab. If running in you own account, remember to delete the CloudFormation Stack after completing the lab to avoid unexpected charges.\n"
},
{
	"uri": "http://Handoo464.github.io/7-lgme/7.4/7.4.4/",
	"title": "Scan the sparse GSI",
	"tags": [],
	"description": "",
	"content": "In the previous step, you saw how to find games for a particular map. Some players may prefer to play a specific map, so this is useful. Other players may be willing to play a game at any map. In this section, you learn how to find any open game in the application, regardless of the type of map. To do this, you use the Scan API.\nIn general, you do not want to design your table to use the DynamoDB Scan operation because DynamoDB is built for surgical queries that grab the exact entities you need. A Scan operation grabs a random collection of entities across your table, so finding the entities you need can require multiple round trips to the database.\nHowever, sometimes Scan can be useful. For example, when you have a sparse secondary index, meaning that the index shouldn’t have that many entities in it. In addition, the index includes only those games that are open, and that is exactly what you need.\nFor this use case, Scan works great. Let’s see how it works. In the code you downloaded, a find_open_games.py file is in the scripts/ directory.\nimport boto3 from entities import Game dynamodb = boto3.client(\u0026#39;dynamodb\u0026#39;) def find_open_games(): resp = dynamodb.scan( TableName=\u0026#39;battle-royale\u0026#39;, IndexName=\u0026#34;OpenGamesIndex\u0026#34;, ) games = [Game(item) for item in resp[\u0026#39;Items\u0026#39;]] return games games = find_open_games() print(\u0026#34;Open games:\u0026#34;) for game in games: print(game) This code is similar to the code in the previous step. However, rather than using the query() method on the DynamoDB client, you use the scan() method. Because you are using scan(), you don’t need to specify anything about the key conditions like you did with query().DynamoDB returns a bunch of items in no specific order.\nRun the script with the following command in your terminal:\npython scripts/find_open_games.py Your terminal should print a list of nine games that are open across a variety of maps.\nOpen games: Game: c6f38a6a-d1c5-4bdf-8468-24692ccc4646 Map: Urban Underground Game: d06af94a-2363-441d-a69b-49e3f85e748a Map: Dirty Desert Game: 873aaf13-0847-4661-ba26-21e0c66ebe64 Map: Dirty Desert Game: fe89e561-8a93-4e08-84d8-efa88bef383d Map: Dirty Desert Game: 248dd9ef-6b17-42f0-9567-2cbd3dd63174 Map: Juicy Jungle Game: 14c7f97e-8354-4ddf-985f-074970818215 Map: Green Grasslands Game: 3d4285f0-e52b-401a-a59b-112b38c4a26b Map: Green Grasslands Game: 683680f0-02b0-4e5e-a36a-be4e00fc93f3 Map: Green Grasslands Game: 0ab37cf1-fc60-4d93-b72b-89335f759581 Map: Green Grasslands Again, using PartiQL , you can run a Scan index query to receive a similar result.\nClick on the 3 dots (\u0026hellip;) next to the OpenGamesIndex and choose Scan index.\nIn this step, you saw how using the Scan operation can be the right choice in specific circumstances. You used Scan to grab an assortment of entities from the sparse global secondary index (GSI) to show open games to players.\nReview In this module, you added a global secondary index (GSI) to the table. This satisfied two additional access patterns:\nFind open games by map (Read) Find open games (Read) To accomplish this, you used a sparse index that included only the games that were still open for additional players. You then used both the Query and Scan APIs against the index to find open games.\nIn the next module, you will use DynamoDB transactions as you add new players to a game and close games when they are full.\n"
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.4/1.4.4/",
	"title": "Scheduled Backup",
	"tags": [],
	"description": "",
	"content": "You must create at least one vault before creating a backup plan or starting a backup job.\nIn the AWS Management Console, navigate to Services -\u0026gt; AWS Backup. Click on Create Backup vault under Backup vaults. Provide Backup vault name of your choice. AWS KMS encryption master key. By default, AWS Backup creates a master key with the alias aws/backup for you. You can choose that key or choose any other key in your account. Click on Create Backup vault You can see Backup vault is created successfully\nNow, we need to create backup plan.\nClick on Create Backup plan under Backup plans. Select Build a new plan. Provide backup plan name and rule name. Select backup frequency. The backup frequency determines how often a backup is created. Using the console, you can choose a frequency of every 12 hours, daily, weekly, or monthly. Choose a backup window. Backup window consists of the time that the backup window begins and the duration of the window in hours. Backup jobs are started within this window. I am configuring backup at 6 PM UTC start within 1 hour and completes within 4 hours.\nFurther, select lifecycle. The lifecycle defines when a backup is transitioned to cold storage and when it expires. I am configuring backup to move cold storage after 31 days and expire after 366 days.\nSelect backup vault we created earlier. Click on Create plan. Note: Backups that are transitioned to cold storage must be stored in cold storage for a minimum of 90 days\nNow assign the resource to backup plan. When you assign a resource to a backup plan, that resource is backed up automatically according to the backup plan.\nGive Resource a assignment name. Choose the default role. Select Include specific resource types under \u0026ldquo;1. Define resource selection\u0026rdquo; Under \u0026ldquo;2. Select specific resource types\u0026rdquo; select the resource type DynamoDB in the drop down. Click choose resources, uncheck All, and select the ProductCatalog table. Click Assign resources You can see the status of your backup job under jobs section after your scheduled backup window timeframe. You can see your DynamoDB backup is completed. Restore a Backup: After a resource has been backed up at least once, it is considered protected and is available to be restored using AWS Backup. In your account a backup may not yet be available. If this is the case, review the screenshots in lieu of doing this in your own account.\nOn the Protected resources page, you can explore details of the resources that are backed up in AWS Backup. Choose our DynamoDB table resource. Choose the recovery point ID of the resource. Click on Restore. Note: If you do not see a recovery point, you can click \u0026ldquo;Create an on-demand backup\u0026rdquo; and complete the backup. For the purposes of this lab, you need a completed backup to continue, and you may not want to wait for your backup plan\u0026rsquo;s scheduled backup. Provide new DynamoDB table name. Leave all the settings on the defaults and click Restore backup The Restore jobs pane appears. A message at the top of the page provides information about the restore job. You can see job status is running.After some time you can see status changes to completed\nYou can also monitor the all backup and restore jobs in central dashboard.\nTo see the restored table, go to the DynamoDB Console and click on Tables from the side menu.Choose ProductCatalogRestored table. You can see the table is restored along with data.\n"
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.1/3.1.4/",
	"title": "Step 3 - Check boto3 installation",
	"tags": [],
	"description": "",
	"content": "Boto3 is the Amazon Web Services (AWS) Software Development Kit (SDK) for Python, which allows Python developers to build applications using AWS services.\nIn the EC2 shell window, run python to make an interactive console with the first command and then copy and paste the following Python code:\n# Open python: python # Run this code: import boto3 ddb = boto3.client(\u0026#39;dynamodb\u0026#39;) ddb.describe_limits() You will see the following result:\n{u\u0026#39;TableMaxWriteCapacityUnits\u0026#39;: 40000, u\u0026#39;TableMaxReadCapacityUnits\u0026#39;: 40000, u\u0026#39;AccountMaxReadCapacityUnits\u0026#39;: 80000, \u0026#39;ResponseMetadata\u0026#39;: {\u0026#39;RetryAttempts\u0026#39;: 0, \u0026#39;HTTPStatusCode\u0026#39;: 200, \u0026#39;RequestId\u0026#39;: \u0026#39;BFMGAS4P48I3DJTP5NU22QRDDJVV4KQNSO5AEMVJF66Q9ASUAAJG\u0026#39;, \u0026#39;HTTPHeaders\u0026#39;: {\u0026#39;x-amzn-requestid\u0026#39;: \u0026#39;BFMGAS4P48I3DJTP5NU22QRDDJVV4KQNSO5AEMVJF66Q9ASUAAJG\u0026#39;, \u0026#39;content-length\u0026#39;: \u0026#39;143\u0026#39;, \u0026#39;server\u0026#39;: \u0026#39;Server\u0026#39;, \u0026#39;connection\u0026#39;: \u0026#39;keep-alive\u0026#39;, \u0026#39;x-amz-crc32\u0026#39;: \u0026#39;3062975651\u0026#39;, \u0026#39;date\u0026#39;: \u0026#39;Tue, 31 Dec 2020 00:00:00 GMT\u0026#39;, \u0026#39;content-type\u0026#39;: \u0026#39;application/x-amz-json-1.0\u0026#39;}}, u\u0026#39;AccountMaxWriteCapacityUnits\u0026#39;: 80000} To close Python console, type:\nquit() "
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.9/3.9.4/",
	"title": "Step 4 - Enable DynamoDB Streams",
	"tags": [],
	"description": "",
	"content": "Now that we have an AWS Lambda function created to process the DynamoDB Streams records, we need to enable the DynamoDB Stream on the logfile table. In the following step we will connect the stream with the function.\nWe will enable the stream on the logfile table. When a stream is enabled you can choose whether DynamoDB copies the new item, or the old item, or both old and new items, or just the partition and sort keys of an item that has been created, updated, or deleted. For more information on the different options you can review the documentation on StreamSpecification , which lists the options as follows: NEW_IMAGE, OLD_IMAGE, NEW_AND_OLD_IMAGES, or KEYS_ONLY.\nEnable DynamoDB Streams for the logfile table with the NEW_IMAGE, which includes \u0026ldquo;The entire item, as it appears after it was modified, is written to the stream.\u0026rdquo; according to the documentation linked above.\naws dynamodb update-table --table-name \u0026#39;logfile\u0026#39; --stream-specification StreamEnabled=true,StreamViewType=NEW_IMAGE Get the full ARN for DynamoDB Streams in the response. We will need this for the next step.\naws dynamodb describe-table --table-name \u0026#39;logfile\u0026#39; --query \u0026#39;Table.LatestStreamArn\u0026#39; --output text The output will look like the following\narn:aws:dynamodb:\u0026lt;REGION\u0026gt;:\u0026lt;ACCOUNTID\u0026gt;:table/logfile/stream/2018-10-27T02:15:46.245 Note the ARN for use in the next step.\n"
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.8/3.8.4/",
	"title": "Step 4 - Query the Customer details and Bill details using the Index",
	"tags": [],
	"description": "",
	"content": "Query using the customer ID, and review the output showing the customer details and the list of related invoices. Notice how the output shows all the invoices associated with the customer.\npython query_index_invoiceandbilling.py InvoiceAndBills \u0026#39;C#1249\u0026#39; Here\u0026rsquo;s a look at the output.\n========================================================= Invoice ID: I#661, Customer ID: C#1249 Invoice ID: I#1249, Customer ID: C#1249 ========================================================= Now, query using the bill ID, and review the output showing the bill details and the details of the associated invoices. Notice how the output shows all the invoices associated with a specific bill.\nRun the following Python script:\npython query_index_invoiceandbilling.py InvoiceAndBills \u0026#39;B#3392\u0026#39; Here\u0026rsquo;s a look at the output.\n========================================================= Invoice ID: I#506, Bill ID: B#3392, BillAmount: $383,572.00 , BillBalance: $5,345,699.00 Invoice ID: I#1721, Bill ID: B#3392, BillAmount: $401,844.00 , BillBalance: $25,408,787.00 Invoice ID: I#390, Bill ID: B#3392, BillAmount: $581,765.00 , BillBalance: $11,588,362.00 ========================================================= "
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.7/3.7.4/",
	"title": "Step 4 - Querying all the employees of a city and a specific department",
	"tags": [],
	"description": "",
	"content": "You also can use the global secondary index to query employees by state. Run the following Python script to list all employees in the Operation department in Dallas, Texas.\npython query_city_dept.py employees TX --citydept \u0026#39;Dallas#Op\u0026#39; Output:\nList of employees . State: TX Name: Brady Marvel. City: Dallas. Dept: Operation Name: Emmye Fletcher. City: Dallas. Dept: Operation Name: Audra Leahey. City: Dallas. Dept: Operation Name: Waneta Parminter. City: Dallas. Dept: Operation Name: Lizbeth Proudler. City: Dallas. Dept: Operation Name: Arlan Cummings. City: Dallas. Dept: Operation Name: Bone Ruggs. City: Dallas. Dept: Operation Name: Karlis Prisk. City: Dallas. Dept: Operation Name: Marve Bignold. City: Dallas. Dept: Operation Total of employees: 9. Execution time: 0.174154996872 seconds In this exercise, we created a global secondary index to query additional attributes. Data can now be retrieved using the City and Department fields.\n"
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.2/3.2.4/",
	"title": "Step 4 - View the CloudWatch metrics on your table",
	"tags": [],
	"description": "",
	"content": "To view the Amazon CloudWatch metrics for your table:\nNavigate to the DynamoDB section of the AWS management console.\nAs shown in the following image, in the navigation pane, choose Tables. Choose the logfile table, and in the right pane, choose the Metrics tab\nThe CloudWatch metrics will look like what you see in the following image.\nYou might not see provisioned capacity data in your read or write capacity graphs, which are displayed as red lines. It takes time for DynamoDB to generate provisioned capacity CloudWatch metrics, especially for new tables.\nThe CloudWatch metrics will look like what you see in the following image for the global secondary index.\nYou might be wondering: Why are there throttling events on the table but not on the global secondary index? The reason is a base table receives the writes immediately and consumes write capacity doing so, whereas a global secondary index\u0026rsquo;s capacity is consumed asynchronously some time after the initial write to the base table succeeds. In order for this system to work inside the DynamoDB service, there is a buffer between a given base DynamoDB table and a global secondary index (GSI). A base table will quickly surface a throttle if capacity is exhausted, whereas only an imbalance over an extended period of time on a GSI will cause the buffer to fill, thereby generating a throttle. In short, a GSI is more forgiving in the case of an imbalanced access pattern.\nContinue this exercise to see what happens when you add more write capacity to the base DynamoDB table.\n"
},
{
	"uri": "http://Handoo464.github.io/4-lcdc/4.3/4.3.4/",
	"title": "Update IAM Role",
	"tags": [],
	"description": "",
	"content": "Configure your lambda function to copy changed records from the Orders DynamoDB streams to the OrdersHistory table by doing the following.\nGo to the IAM dashboard on the AWS Management Console and inspect the IAM policy, i.e. AWSLambdaMicroserviceExecutionRole\u0026hellip;, created when you created the create-order-history-ddbs lambda function. { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:DeleteItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:Scan\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:{aws-region}:{aws-account-id}:table/*\u0026#34; } ] } Update the policy statement by editing and replacing the existing policy using the following IAM policy statement. { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:DescribeStream\u0026#34;, \u0026#34;dynamodb:GetRecords\u0026#34;, \u0026#34;dynamodb:GetShardIterator\u0026#34;, \u0026#34;dynamodb:ListStreams\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:{aws-region}:{aws-account-id}:table/Orders/stream/*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:{aws-region}:{aws-account-id}:table/OrdersHistory\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;sqs:SendMessage\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:sqs:{aws-region}:{aws-account-id}:orders-ddbs-dlq\u0026#34; } ] } The updated IAM policy gives the create-order-history-ddbs lambda function the permissions required to read events from the Orders DynamoDB stream, write new items to the OrdersHistory DynamoDB table and send messages to the orders-ddbs-dlq SQS queue.\nReplace {aws-region} and {aws-account-id} in the policy statement above with the correct value for your AWS region and your AWS account ID.\nGo to the lambda function console editor. Select Layers then select Add a Layer. Select Specify an ARN, enter the Lambda Layer ARN below. 1 arn:aws:lambda:{aws-region}:017000801446:layer:AWSLambdaPowertoolsPythonV2:58 Click Verify then select Add. Replace {aws-region} with ID for the AWS region that you are currently working on.\nGo to the configuration section of the lambda console editor. Select Environment variables then select Edit. Add a new environment variable called ORDERS_HISTORY_DB and set its value to OrdersHistory. Select Triggers then select Add trigger. Select DynamoDB as the trigger source. Select the Orders DynamoDB table. Set the Batch size to 10 and leave all other values unchanged. Click Additional settings to expand the section. Provide the ARN of the orders-ddbs-dlq SQS queue you created earlier. arn:aws:sqs:{aws-region}:{aws-account-id}:orders-ddbs-dlq Set the Retry attempts to 3. Select Add. "
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.2/1.2.5/",
	"title": "Deleting Data",
	"tags": [],
	"description": "",
	"content": "The DynamoDB DeleteItem API is used to delete an item. It is invoked using the delete-item CLI command .\nDeletes in DynamoDB are always singleton operations. There is no single command you can run that would delete all the rows in the table for example.\nRemember that item we added to the Reply table in the previous path:\naws dynamodb get-item \\ --table-name Reply \\ --key \u0026#39;{ \u0026#34;Id\u0026#34; : {\u0026#34;S\u0026#34;: \u0026#34;Amazon DynamoDB#DynamoDB Thread 2\u0026#34;}, \u0026#34;ReplyDateTime\u0026#34; : {\u0026#34;S\u0026#34;: \u0026#34;2021-04-27T17:47:30Z\u0026#34;} }\u0026#39; Let\u0026rsquo;s delete this item. When using the delete-item command we need to reference the full Primary Key just like we do with get-item:\naws dynamodb delete-item \\ --table-name Reply \\ --key \u0026#39;{ \u0026#34;Id\u0026#34; : {\u0026#34;S\u0026#34;: \u0026#34;Amazon DynamoDB#DynamoDB Thread 2\u0026#34;}, \u0026#34;ReplyDateTime\u0026#34; : {\u0026#34;S\u0026#34;: \u0026#34;2021-04-27T17:47:30Z\u0026#34;} }\u0026#39; It\u0026rsquo;s safe to delete the same item more than once. You can run the same command above as many times as you want and it won\u0026rsquo;t report an error; if the key doesn\u0026rsquo;t exist then the DeleteItem API still returns success.\nNow that we\u0026rsquo;ve removed that item from the Reply table we also need to decrement the related Forum Messages count.\naws dynamodb update-item \\ --table-name Forum \\ --key \u0026#39;{ \u0026#34;Name\u0026#34; : {\u0026#34;S\u0026#34;: \u0026#34;Amazon DynamoDB\u0026#34;} }\u0026#39; \\ --update-expression \u0026#34;SET Messages = :newMessages\u0026#34; \\ --condition-expression \u0026#34;Messages = :oldMessages\u0026#34; \\ --expression-attribute-values \u0026#39;{ \u0026#34;:oldMessages\u0026#34; : {\u0026#34;N\u0026#34;: \u0026#34;5\u0026#34;}, \u0026#34;:newMessages\u0026#34; : {\u0026#34;N\u0026#34;: \u0026#34;4\u0026#34;} }\u0026#39; \\ --return-consumed-capacity TOTAL "
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.5/",
	"title": "Exercise 4: Global Secondary Index Key Overloading",
	"tags": [],
	"description": "",
	"content": "You can create 20 global secondary indexes for a DynamoDB table as of the time this page was written. Sometimes, though, your application might need to support multiple access patterns and exceed the current limit of global secondary indexes per table. The global secondary index key overloading design pattern is enabled by designating and reusing an attribute name (column header) across different item types and storing a value in that attribute depending on the context of the item type. When you create a global secondary index on that attribute, you are indexing for multiple access patterns, each for a different item type—and have used only 1 global secondary index. For example, an employees table. An employee can contain items of type metadata (for employee details), employee-title (all the job titles that the employee has held), or employee-location (all the office buildings and locations where the employee has worked).\nThe access patterns required for this scenario are:\nQuery all employees of a state Query all employees with one specific current title Query all employees who had ever one specific title Query employees by name The following screenshot shows the design of the employees table. The attribute called PK has the employee ID, which is prefixed by the letter e. The hash sign (#) is used as a separator between the entity type identifier (e) and the actual employee ID. The SK is an overloaded attribute, and has either current title, previous title, or the keyword root, which denotes the primary item for the employee that holds most of their important attributes. The GSI_1_PK attribute includes the title or the name of the employee. The re-use of a given global secondary index for multiple entity types such as employees, employee locations, and employee titles lets us simplify our management of the DynamoDB table because we only need to monitor and pay for one global secondary index as opposed to three separate indexes.\n"
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.5/1.5.5/",
	"title": "Explore Target Model",
	"tags": [],
	"description": "",
	"content": "Relational Database Management System (RDBMS) platforms store data in a normalized relational structure. This structure reduces hierarchical data structures and stores data across multiple tables. You can often query the data from multiple tables and assemble at the presentation layer. However, that is not preferrable and won\u0026rsquo;t be efficient for ultra-low read latency workloads. To support high-traffic queries with ultra-low latency, designing a schema to take advantage of a NoSQL system generally makes technical and economic sense.\nTo start designing a target data model in Amazon DynamoDB that will scale efficiently, you must identify the common access patterns. For the IMDb use case we have identified a set of access patterns as described below: A common approach to DynamoDB schema design is to identify application layer entities and use denormalization and composite key aggregation to reduce query complexity. In DynamoDB, this means using composite sort keys , overloaded global secondary indexes , and other design patterns.\nIn this scenario, we will follow the Adjacency List Design Pattern with primary key overloading to store relational data in the DynamoDB table. The advantages of this pattern include optimal data duplication and simplified query patterns to find all metadata related to each movie. Normally the adjacency list pattern stores duplicate data under two items, each representing one half of the relationship. To associate a title with a region, for example, you would write one item for the region under the title and one item under the title under the region, like this:\nPartition Key Sort Key Attribute List tt0309377 REGN|NZ ordering, language, region, title, types REGN|NZ tt0309377 language, region, title However in this lab we\u0026rsquo;ll only work with one side of the relationship: the data residing under the title. Our partition key will be mpkey and our sort key mskey for the movies table. Each partition and sort key is prefixed with letters to identify the entity type, and the sort key uses | as a separator between the entity type and value. The partition key is prefixed with tt (unique movie id) and sort key is overloaded to define the entity type.\nThe following entity types are found in the table:\ntt: A unique movie id. This is the entity type of the partition key in the base table nm: A unique entry for each movie crew member. This is the entity type of the GSI partition key DETL: Contains cast/crew information per movie. There is 1: many relationships between title_basics and title_principals. title_principals has all cast and crew information stored as separate rows per movie where as title_basics has movie metadata. Information in both the tables are considered static once a movie is published. The access patterns required the movies and cast/crew information to be fetched together. During target modelling, each cast/crew member (actor, director, producer etc.) metadata is denormalized with movie information and stored with entity type DETL. REGN: Contains all regions, languages and titles that a movie is published. During target modelling the data is migrated as is to the DynamoDB table. RTNG: Contains IMDb rating and number of votes. This is considered dynamic and frequent changing records for a movie. In order to reduce I/O during update scenario, the record is not denormalized with other information in the the DynamoDB table. A new GSI is created on the movies table with new partion key: nconst (unique per movie\u0026rsquo;s crew with entity type nm) and sort key: startYear. This will help to query access pattern by crew member (#6 inside the common access pattern table)\nClick here to view a video that demonstrates how all of these access pattern are evaluated against target DynamoDB model\n"
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.3/1.3.5/",
	"title": "Global Secondary Indexes",
	"tags": [],
	"description": "",
	"content": "We have concerned ourself hitherto with accessing data based on the key attributes. If we wanted to look for items based on non-key attributes we had to do a full table scan and use filter conditions to find what we wanted, which would be both very slow and very expensive for systems operating at large scale.\nDynamoDB provides a feature called Global Secondary Indexes (GSIs) which will automatically pivot your data around different Partition and Sort Keys. Data can be re-grouped and re-sorted to allow for more access patterns to be quickly served with the Query and Scan APIs.\nRemember the previous example where we wanted to find all the replies in the Reply table that were posted by User A and needed to use a Scan operation? If there had been a billion Reply items but only three of them were posted by User A, we would have to pay (both in time and money) to scan through a billion items just to find the three we wanted.\nArmed with this knowledge of GSIs, we can now create a GSI on the Reply table to service this new access pattern. GSIs can be created and removed at any time, even if the table has data in it already! This new GSI will use the PostedBy attribute as the Partition (HASH) key and we will still keep the messages sorted by ReplyDateTime as the Sort (RANGE) key. We want all the attributes from the table copied (projected) into the GSI so we will use the ALL ProjectionType. Note that the name of the index we create is PostedBy-ReplyDateTime-gsi.\nNavigate to the Reply table, switch to the Indexes tab and click Create Index.\nEnter PostedBy as the Partition key, ReplyDateTime as the Sort key, and PostedBy-ReplyDateTime-gsi as the Index name. Leave the other settings as defaults and click Create Index. Once the index leaves the Creating state you can continue on to the exercise below.\nCleanup When you\u0026rsquo;re done, make sure to remove the GSI. Return to the Indexes tab, select the PostedBy-ReplyDateTime-gsi index and click Delete.\n"
},
{
	"uri": "http://Handoo464.github.io/5-lmr/5.5/",
	"title": "Global Tables Discussion Topics",
	"tags": [],
	"description": "",
	"content": "Below are some discussion topics for people who’ve finished the development work or would like to discuss interesting aspects of Global Tables with a DynamoDB specialist.\nWhat are Global Tables? Global tables build on the global Amazon DynamoDB footprint to provide you with a fully managed, multi-Region, and multi-active database that delivers fast, local, read and write performance for massively scaled, global applications. Global tables replicate your DynamoDB tables automatically across your choice of AWS Regions. Global tables eliminate the difficult work of replicating data between Regions and resolving update conflicts, enabling you to focus on your application\u0026rsquo;s business logic. In addition, global tables enable your applications to stay highly available even in the unlikely event of isolation or degradation of an entire Region. You can set up global tables in the AWS Management Console or AWS CLI. No application changes are required because global tables use existing DynamoDB APIs. There are no upfront costs or commitments for using global tables, and you pay only for the resources provisioned. How am I charged for using Global Tables? A write to a traditional DynamoDB table is priced in Write Units (where if you write a 5 KB item it incurs a charge of 5 Write Units). A write to a global table is priced in Replicated Write Capacity Units (rWCUs, for provisioned tables) or Replicated Write Request Units (rWRUs, for on-demand tables). Replicated write units include the cost of the streaming infrastructure needed to manage the replication. For on-demand tables in us-east-1 the price is $1.875 per replicated million write units instead of $1.25 per million. For Provisioned it’s $0.000975 per rWCU-hour instead of $0.00065 per WCU-hour. Cross-region data transfer fees do apply. Replicated Write Unit charges are incurred in every Region where the item is directly written or replicate written. Writing to a Global Secondary Index (GSI) is considered a local write and uses regular Write Units. There is no Reserved Capacity available for rWCUs at this time. Purchasing Reserved Capacity may still be beneficial for tables with GSIs consuming write units. What’s the main difference between GTv1 (2017) and GTv2 (2019)? DynamoDB has two version of Global Tables. Both are still supported, but we suggest you use GTv2 (2019) or upgrade when you can. All discussion other than this question here refers to GTv2 behaviors. With GTv2 the source and target tables are maintained together and kept aligned automatically (for throughput, TTL settings, auto-scaling settings, etc). With GTv2 the metadata attributes required to control replication are now hidden, preventing any accidental (or intentional) writing of them which would cause issues with the replication. Customer Managed Key (CMK) encryption is only available on GTv2. More Regions are supported with GTv2. GTv2 lets you add/remove Regions to an existing table. GTv2 is generally more cost effective. How would I upgrade from Global Tables v1 to v2? It’s a push-button on the Console. It’s a live upgrade that should finish in less than an hour. How is read and write throughput managed for Global Tables? The write capacity must be the same on all table instances across Regions. With GTv2 the write capacity is automatically kept in sync by the GT infrastructure, so a write capacity change to one table replicates to the others. The table must support auto scaling or be in on-demand mode. Read capacity is allowed to differ because reads may not be equal across Regions. When adding a global replica to a table the capacity of the source Region is propagated. After creation you can adjust the read capacity, which is not transferred to the other side. Read capacity can be adjusted for each region\u0026rsquo;s global secondary index as well through provisioned throughput overrides . What Regions does Global Tables support? As of today, GTv2 supports more than 32 Regions. The latest list can be seen in the drop-down on the Console when choosing a Region in which to add a replica. How are GSIs handled with Global Tables? With GTv2, you create a GSI in one Region, and it’s automatically replicated to the other Region(s) as well as automatically backfilled. Write capacity must be the same on each index copy, but you can override the read capacity on a per-region basis. How do I delete a global table? You can delete a replica table the same as any other, which will stop replication to that Region and delete the table copy kept in that Region. You cannot however ask to sever the replication and have copies of the table exist as independent entities. There’s also a rule you can’t delete a source table quickly after it’s used to initiate a new Region. If you try you get the error: “Replica cannot be deleted because it has acted as a source Region for new replica(s) being added to the table in the last 24 hours..” How are conflicting writes handled with Global Tables? Conflicts can arise if applications update the same item in different Regions at about the same time. To help ensure eventual consistency, DynamoDB global tables use a last writer wins reconciliation between concurrent updates, in which DynamoDB makes a best effort to determine the last writer. With this conflict resolution mechanism, all the replicas will agree on the latest update and converge toward a state in which they all have identical data. There are several ways to avoid conflicts, such as using an IAM policy to only allow writes to the table in one region, routing users to only one region and keeping the other as an idle standby, routing odd users to one region and even users to another region, avoiding the use of non-idempotent updates such as Bookmark = Bookmark + 1 in favor of static updates such as Bookmark=25. For more information, review our best practice guide on request routing in with global tables . What are best practices for deploying Global Tables? How can I automate deployment? In AWS CloudFormation, each global table is controlled by a single stack, in a single Region, regardless of the number of replicas. When you deploy your template, CloudFormation will create/update all replicas as part of a single stack operation. You should not deploy the same AWS::DynamoDB::GlobalTable resource in multiple Regions. Doing so will result in errors, and is unsupported. If you deploy your application template in multiple Regions, you can use conditions to only create the resource in a single Region. Alternatively, you can choose to define your AWS::DynamoDB::GlobalTable resources in a stack separate from your application stack, and make sure it is only deployed to a single Region. https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-dynamodb-globaltable.html A DynamoDB table is AWS::DynamoDB::Table and a global table is AWS::DynamoDB::GlobalTable, which essentially makes them two different resources in regards to CFN. One approach then is to create all tables that might ever be global by using the GlobalTable construct, keep them as standalone tables initially, and later add Regions if needed. If you have a regular table and you want to convert it while using CloudFormation, here is the recipe: Set the deletion policy to retain, remove the table from the stack, convert the table to a Global Table in the console, then import the global table as a new resource to the stack. Note that cross-account replication is not supported at this time (mid-2024). How do I monitor Global Tables? Using Amazon CloudWatch you can observe a metric ReplicationLatency which tracks the elapsed time between when an item is written to a replica table and when that item appears in another replica in the global table. It’s expressed in milliseconds and is emitted for every source- and destination-Region pair. The latencies you will observe depends on many things including the distance between your chosen Regions. It’s common to see latencies in the 0.5 to 2.5 second range for Regions within the same geographic area. This is the only CloudWatch metric provided by Global Tables v2. How is Time To Live (TTL) handled with Global Tables v2? TTL is a feature where you can specify an attribute name whose value (as a number in seconds since epoch) indicates the time of expiration for the item. After that time DynamoDB can delete the item without incurring write costs. With Global Table, you configure TTL in one Region, and the setting is auto replicated to the other Region(s). When an item is deleted via a TTL rule, that work is performed without consuming Write Units on the source table, but the target table(s) do incur Replicated Write Unit costs. Be aware that if the source and target table have very low Provisioned write capacity, this may cause throttling as the TTL deletes require write capacity. How do DynamoDB Streams interact with Global Tables? Each global table produces an independent stream based on all its writes, wherever they started from. You can choose to consume the DynamoDB stream in one Region or in all Regions (independently). If you should want local writes processed but not replicated writes, you can add your own region attribute to each item that identifies the writing Region, then use a Lambda event filter to only invoke the Lambda for writes in the local Region. How do Global Tables handle transactions? Transactional operations provide ACID guarantees ONLY within the Region where the write is made originally. Transactions are not supported across Regions in global tables. For example, if you have a global table with replicas in the US East (Ohio) and US West (Oregon) Regions and perform a TransactWriteItems operation in the US East (Ohio) Region, you may observe partially completed transactions in US West (Oregon) Region as changes are replicated. Changes will only be replicated to other Regions once they have been committed in the source Region. How do Global Tables interact with the DynamoDB Accelerator cache (DAX)? Global Tables “write around” DAX by updating DynamoDB directly, so DAX will not be aware it’s holding stale data. The DAX cache will only be refreshed when the cache’s TTL expires. Do tags on tables propagate? No, they do not automatically propagate. Should I backup tables in all Regions or just one? The answer depends on the purpose of the backup. If it’s to ensure data durability, that’s a feature intrinsic to the DynamoDB service. If it’s about keeping a snapshot for historic records (such as for regulatory requirements) then backing up in one Region should suffice. The backed up data can be replicated to multiple Regions via AWS Backup. If it’s about recovering erroneously deleted or modified data, then PITR in one Region should suffice. What’s the best practice for using Global Tables as part of handling a potential Region outage? Have (or be able to quickly create) independent copies of your execution stack in alternative Regions, each accessing its local DynamoDB endpoint. Use Route53 or Global Accelerator to route to the nearest healthy Region, or have the client aware of the multiple endpoints it might use. Use health checks in each Region that will be able to determine reliably if there’s any issue with the stack, including if DynamoDB is degraded. For example, don’t just ping that the DynamoDB endpoint is up, actually do a call that ensures a full successful database flow. Should the health check fail, traffic can route to other Regions (by updating the DNS entry with Route53, by having Global Accelerator route differently, or by having the client choose a different endpoint). Global Tables have a good RPO (recovery point objective) because the data is continuously syncing and a good RTO (recovery time objective) because both Regions always keep a table ready for both read and write traffic. Note that DynamoDB is a core service on which other services frequently build their control plane operations, thus it’s unlikely you’ll encounter a scenario where DynamoDB has degraded service in a Region while other services are unimpacted. Read Evacuating a Region with global tables in our developer docs for more information. "
},
{
	"uri": "http://Handoo464.github.io/7-lgme/7.5/",
	"title": "Join and close games",
	"tags": [],
	"description": "",
	"content": "Through the earlier modules in this lab, you have satisfied the access patterns for the creation and retrieval of core entities in the gaming application, such as Users and Games. You also used a sparse GSI to find open games that users can join.\nIn this module, you satisfy two access patterns:\nJoin game for a user (Write) Start game (Write) Note that both of these access patterns are writing data to DynamoDB, in contrast to the read-heavy patterns you have done so far in this lab.\nDynamoDB transactions To satisfy the “Join game for a user” access pattern in this module’s steps, you are going to use DynamoDB transactions . Transactions are popular in relational systems for operations that affect multiple data elements at once. For example, imagine you are running a bank. One customer transfers $100 to another customer. When recording this transaction, you would use a transaction to make sure the changes are applied to the balances of both customers rather than just one.\nDynamoDB transactions make it easier to build applications that alter multiple items as part of a single operation. With transactions, you can operate on up to 100 items as part of a single transaction request. In a TransactWriteItems API call, you can use the following operations:\nPut: For inserting or overwriting an item. Update: For updating an existing item. Delete: For removing an item. ConditionCheck: For asserting a condition on an existing item without altering the item. In the following steps, you use a DynamoDB transaction when adding new users to a game while preventing the game from becoming overfilled.\n"
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.5/",
	"title": "LMIG: Relational Modeling &amp; Migration",
	"tags": [],
	"description": "",
	"content": "In this module, also classified as LMIG, you will learn how to design a target data model in DynamoDB for highly normalized relational data in a relational database. The exercise also guides a step by step migration of an IMDb dataset from a self-managed MySQL database instance on EC2 to a fully managed key-value pair database Amazon DynamoDB. At the end of this lesson, you should feel confident in your ability to design and migrate an existing relational database to Amazon DynamoDB.\nSometimes data appears to be in a relational format at given point of time, though evolving business requirements cause schema changes over the project lifecycle. Every schema change is labor-intensive, costly and sometimes causes the business to reprioritize their needs due to complicated cascading impacts. Amazon DynamoDB helps IT to rethink the data model in a key-value format. Such a format has the potential to absorb disruption caused by an evolving schema. Amazon DynamoDB offers a fully managed, serverless datastore for information stored in key-value format. Schema flexibility lets DynamoDB store complex hierarchical data within an item and offers single-digit millisecond latency at scale.\nThis module will briefly discuss techniques to design a target data model and migrate relational datasets from MySQL to Amazon DynamoDB. IMDb data inside a MySQL database starts out as normalized across multiple tables. We will use denormalized/item collection modelling techniques to create a comprehensive data model for identified access patterns. There are multiple factors that will influence our decisions in building the target data model:\nAccess patterns Cardinality Overall I/O We will briefly discuss the key aspects of creating a model that will serve various access patters with ultralow latency and low I/O and cost.\n"
},
{
	"uri": "http://Handoo464.github.io/5-lmr/",
	"title": "LMR: Build and Deploy a Global Serverless Application with Amazon DynamoDB",
	"tags": [],
	"description": "",
	"content": "In this workshop you will learn how to build and deploy a globally distributed serverless application and get experience with using Amazon DynamoDB Global Tables to replicate data across AWS Regions.\nYou will use serverless components to build an API to support a video player web application. The API service is designed to store bookmark records for any video show that a customer watches, so that the application can remember where a user left off during a watch session.\nThe target audience for this workshop is any developer or application architect who needs to understand multi-region data availability architectures for their application. At the end of this workshop you should:\nFeel comfortable creating a DynamoDB Global Table in multiple regions Understand how DynamoDB Global Tables Replication works Be able to explain what eventual consistentcy means in the context of DynamoDB Global Tables replication You must bring your laptop to participate and the expected duration of this workshop is 50 minutes.\nThe workshop contains the following chapters:\nGetting started Module 1: Launch the app Module 2: Explore Global Tables Module 3: Interact with Globalflix Interface Global Tables Discussion Topics "
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.4/1.4.5/",
	"title": "Restrict backup deletion",
	"tags": [],
	"description": "",
	"content": "Customer has a common ask when they want their developer/admin should be allowed to create and delete DynamoDB tables but they shouldn’t be allowed to delete the backups.\nYou can achieve this by creating IAM policy. Following is an example of the AWS IAM policy which allow “Create Table”, “List Table”, “Create Backup” and “Delete Table” and denies “Delete Backup” of DynamoDB table.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:CreateTable\u0026#34;, \u0026#34;dynamodb:CreateBackup\u0026#34;, \u0026#34;dynamodb:DeleteTable\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:us-east-1:123456789:table/*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor1\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;dynamodb:ListTables\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor2\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;dynamodb:DeleteBackup\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:us-east-1:123456789:table/*/backup/*\u0026#34; } ] } At this documentation page you will find out more about using IAM with DynamoDB backups You can restrict in AWS backup by denying as well by denying “DeleteBackupSelection” in IAM policy.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;backup:DeleteBackupSelection\u0026#34;, \u0026#34;backup:CreateBackupSelection\u0026#34;, \u0026#34;backup:StartBackupJob\u0026#34;, \u0026#34;backup:CreateBackupPlan\u0026#34;, \u0026#34;backup:ListBackupSelections\u0026#34;, \u0026#34;backup:ListRecoveryPointsByBackupVault\u0026#34;, \u0026#34;backup:GetBackupVaultAccessPolicy\u0026#34;, \u0026#34;backup:GetBackupSelection\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:backup:us-east-1:123456789:backup-plan:*\u0026#34;, \u0026#34;arn:aws:backup:us-east-1:123456789:backup-vault:*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor1\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Deny\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;backup:DeleteBackupSelection\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:backup:us-east-1:123456789:backup-plan:*\u0026#34; } ] } You can apply the policy to role and assign the role to IAM group. Now users belonging to this IAM group will inherit the permission.\nLet’s say now the user tries to delete the backup in AWS backup.\nUser gets the access denied error due to insufficient permission to delete the backup.\n"
},
{
	"uri": "http://Handoo464.github.io/4-lcdc/4.3/4.3.5/",
	"title": "Simulate Order Updates",
	"tags": [],
	"description": "",
	"content": "After creating the Orders table, you uploaded some sample orders to the table. Lets explore the items on the Orders table before simulating any updates to any orders on the table.\nNavigate to the DynamoDB Service page using the AWS Management Console.\nSelect the Orders table then select the Explore table items button to view the orders on the table.\nThere will be 4 orders on the table all having a status of PLACED as shown in the image below.\nFeel free to explore the content of each order by selecting the id of the item using the DynamoDB Console.\nThere will be no data on the OrdersHistory table at this point beacuse no data has been written to it.\nYou can simulate updates to items on Orders table by using the AWS Management Console or by using the AWS CLI. Expand the appropriate section below for instructions on how to proceed with your preferred method.\nSimulate update using the AWS Management Console\nSelect the Orders table. Select the Explore table items button. Click on order ID 6421680 on the table. Change the status of the order from PLACED to COMPLETE. Selectthe Save and close button.\nSimulate update using the AWS CLI\nApply an update to order ID 642168\naws dynamodb update-item \\ --table-name Orders \\ --key \u0026#39;{ \u0026#34;id\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;6421680\u0026#34;} }\u0026#39; \\ --update-expression \u0026#34;SET #items = :val1, #status = :val2\u0026#34; \\ --expression-attribute-names \u0026#39;{ \u0026#34;#items\u0026#34;: \u0026#34;items\u0026#34;, \u0026#34;#status\u0026#34;: \u0026#34;status\u0026#34; }\u0026#39; \\ --expression-attribute-values \u0026#39;{ \u0026#34;:val2\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;COMPLETE\u0026#34; }, \u0026#34;:val1\u0026#34;: { \u0026#34;L\u0026#34;: [ { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;23769901\u0026#34;}, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Hydrating Face Cream\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£12.00\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;8\u0026#34;}, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34; COMPLETE\u0026#34; } } }, { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;23673445\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;EXTRA Repair Serum\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£10.00\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;5\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34; COMPLETE\u0026#34; } } } ] } }\u0026#39; \\ --return-values ALL_NEW \\ --return-item-collection-metrics SIZE \\ --return-consumed-capacity TOTAL The output should be similar to the one below.\n{ \u0026#34;Attributes\u0026#34;: { \u0026#34;orderDate\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;2023-10-01 20:39:08\u0026#34; }, \u0026#34;shipDate\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;2023-10-04 16:29:36\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;COMPLETE\u0026#34; }, \u0026#34;customer\u0026#34;: { \u0026#34;M\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Brody Dent\u0026#34; }, \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;558490551\u0026#34; }, \u0026#34;address\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;3 Bailey Lane, Clenchwarton,PE34 4AY\u0026#34; }, \u0026#34;phone\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;+441268381612\u0026#34; } } }, \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;6421680\u0026#34; }, \u0026#34;items\u0026#34;: { \u0026#34;L\u0026#34;: [ { \u0026#34;M\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Hydrating Face Cream\u0026#34; }, \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;23769901\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;8\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£12.00\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34; COMPLETE\u0026#34; } } }, { \u0026#34;M\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;EXTRA Repair Serum\u0026#34; }, \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;23673445\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;5\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£10.00\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34; COMPLETE\u0026#34; } } } ] } }, \u0026#34;ConsumedCapacity\u0026#34;: { \u0026#34;TableName\u0026#34;: \u0026#34;Orders\u0026#34;, \u0026#34;CapacityUnits\u0026#34;: 1.0 } } Now explore the Orders and OrdersHistory tables to see the effects item update you performed.\nThe status of order ID 6421680 on the Orders table should be COMPLETE as shown in the image below.\n\u0026hellip; and there should be a single record on the OrdersHistory showing the previous state of order ID 6421680.\nPerform additional updates for order ID 4514280 on the Orders table. This time first change the status of the order to COMPLETE then alter the status of some items on the same order to RETURNED using the commands below.\naws dynamodb update-item \\ --table-name Orders \\ --key \u0026#39;{ \u0026#34;id\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;4514280\u0026#34;} }\u0026#39; \\ --update-expression \u0026#34;SET #items = :val1, #status = :val2\u0026#34; \\ --expression-attribute-names \u0026#39;{ \u0026#34;#items\u0026#34;: \u0026#34;items\u0026#34;, \u0026#34;#status\u0026#34;: \u0026#34;status\u0026#34; }\u0026#39; \\ --expression-attribute-values \u0026#39;{ \u0026#34;:val1\u0026#34;: { \u0026#34;L\u0026#34;: [ { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;23884750\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Metallic Long-Wear Cream Shadow\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£15.00\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;13\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;COMPLETE\u0026#34; } } }, { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;23699354\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Eye Liner\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£9.00\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;8\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;COMPLETE\u0026#34; } } }, { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;23599030\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Bronzing Powder\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£12.00\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;10\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;COMPLETE\u0026#34; } } } ] }, \u0026#34;:val2\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;COMPLETE\u0026#34; } }\u0026#39; \\ --return-values ALL_NEW \\ --return-item-collection-metrics SIZE \\ --return-consumed-capacity TOTAL Followed by\naws dynamodb update-item \\ --table-name Orders \\ --key \u0026#39;{ \u0026#34;id\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;4514280\u0026#34;} }\u0026#39; \\ --update-expression \u0026#34;SET #items = :val1, #status = :val2\u0026#34; \\ --expression-attribute-names \u0026#39;{ \u0026#34;#items\u0026#34;: \u0026#34;items\u0026#34;, \u0026#34;#status\u0026#34;: \u0026#34;status\u0026#34; }\u0026#39; \\ --expression-attribute-values \u0026#39;{ \u0026#34;:val1\u0026#34;: { \u0026#34;L\u0026#34;: [ { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;23884750\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Metallic Long-Wear Cream Shadow\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£15.00\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;13\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;COMPLETE\u0026#34; } } }, { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;23699354\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Eye Liner\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£9.00\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;8\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;RETURNED\u0026#34; } } }, { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;23599030\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Bronzing Powder\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£12.00\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;10\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;RETURNED\u0026#34; } } } ] }, \u0026#34;:val2\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;COMPLETE\u0026#34; } }\u0026#39; \\ --return-values ALL_NEW \\ --return-item-collection-metrics SIZE \\ --return-consumed-capacity TOTAL Explore the items on the Orders and OrdersHistory tables to see the result of your updates.\nThe status of order ID 4514280 on the Orders table should be COMPLETE as shown in the image below.\n\u0026hellip; and there should be two entries for order ID 4514280 on the OrdersHistory table showing the previous states of the order.\nNote: The order of updates to the Orders table is preserved by DynamoDB streams when changes are sent to the create order history lambda function. Since items on the OrdersHistory table have a sort key - sk, that is a timestamp, items on the OrderHistory table can be sorted in the order that they were created.\n"
},
{
	"uri": "http://Handoo464.github.io/4-lcdc/4.4/4.4.5/",
	"title": "Simulate Order Updates",
	"tags": [],
	"description": "",
	"content": "Similar to the previous lab, lets perform item level updates to the Orders table and watch old copies of updated items get written to the OrdersHistory.\nNavigate to the DynamoDB Service page using the AWS Management Console.\nSelect the Orders table then select Explore table items button to view the orders on the table.\nPlease refer to the Simulate Order Updates section from the previous lab if you need a refresher on exploring table items using the AWS Management Console.\nThe items on the table should look familiar from the previous lab. The status of items on the Orders table may vary if you performed additional simulations during the previous lab on change data capture using DynamoDB streams.\nAlso explore the orders on the OrdersHistory table. The number of items on the table will depend on the number of updates you made to the Orders table during the previous lab.\nUpdate the status of order ID 9844720 from PLACED to COMPLETE using the command below.\naws dynamodb update-item \\ --table-name Orders \\ --key \u0026#39;{ \u0026#34;id\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;9844720\u0026#34;} }\u0026#39; \\ --update-expression \u0026#34;SET #items = :val1, #status = :val2\u0026#34; \\ --expression-attribute-names \u0026#39;{ \u0026#34;#items\u0026#34;: \u0026#34;items\u0026#34;, \u0026#34;#status\u0026#34;: \u0026#34;status\u0026#34; }\u0026#39; \\ --expression-attribute-values \u0026#39;{ \u0026#34;:val2\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;COMPLETE\u0026#34; }, \u0026#34;:val1\u0026#34;: { \u0026#34;L\u0026#34;: [ { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;24002126\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Shimmer Wash Eye Shadow\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£13.00\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;10\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;COMPLETE\u0026#34; } } }, { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;23607685\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Buffing Grains for Face\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£8.00\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;11\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;COMPLETE\u0026#34; } } } ] } }\u0026#39; \\ --return-values ALL_NEW \\ --return-item-collection-metrics SIZE \\ --return-consumed-capacity TOTAL The output should be similar to the one below.\n{ \u0026#34;Attributes\u0026#34;: { \u0026#34;orderDate\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;2023-10-01 01:49:13\u0026#34; }, \u0026#34;shipDate\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;2023-10-06 13:05:33\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;COMPLETE\u0026#34; }, \u0026#34;customer\u0026#34;: { \u0026#34;M\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Taylor Burnette\u0026#34; }, \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;941852721\u0026#34; }, \u0026#34;address\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;31 Walkhampton Avenue, Bradwell Common,MK13 8ND\u0026#34; }, \u0026#34;phone\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;+441663724681\u0026#34; } } }, \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;9844720\u0026#34; }, \u0026#34;items\u0026#34;: { \u0026#34;L\u0026#34;: [ { \u0026#34;M\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Shimmer Wash Eye Shadow\u0026#34; }, \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;24002126\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;10\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£13.00\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;COMPLETE\u0026#34; } } }, { \u0026#34;M\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Buffing Grains for Face\u0026#34; }, \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;23607685\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;11\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£8.00\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;COMPLETE\u0026#34; } } } ] } }, \u0026#34;ConsumedCapacity\u0026#34;: { \u0026#34;TableName\u0026#34;: \u0026#34;Orders\u0026#34;, \u0026#34;CapacityUnits\u0026#34;: 1.0 } } View the items on the Orders and OrdersHistory tables to see the effects of item update you performed.\nThe status of order ID 9844720 on the Orders table should be COMPLETE as shown in the image below.\n\u0026hellip; and there should be a single record on the OrdersHistory showing the previous state of order ID 9844720.\nPerform additional updates FOR order ID 9953371 on the Orders table. Start by changing the status of the order to ACTIVE then perform another update by setting the status of the same order to CANCELLED using the commands below.\naws dynamodb update-item \\ --table-name Orders \\ --key \u0026#39;{ \u0026#34;id\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;9953371\u0026#34;} }\u0026#39; \\ --update-expression \u0026#34;SET #items = :val1, #status = :val2\u0026#34; \\ --expression-attribute-names \u0026#39;{ \u0026#34;#items\u0026#34;: \u0026#34;items\u0026#34;, \u0026#34;#status\u0026#34;: \u0026#34;status\u0026#34; }\u0026#39; \\ --expression-attribute-values \u0026#39;{ \u0026#34;:val1\u0026#34;: { \u0026#34;L\u0026#34;: [ { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;23924636\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Protective Face Lotion\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£3.00\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;9\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;CANCELLED\u0026#34; } } }, { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;23514506\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Nail File\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£11.00\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;13\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;PLACED\u0026#34; } } }, { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;23508704\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Kitten Heels Powder Finish Foot Creme\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£11.00\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;10\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;PLACED\u0026#34; } } } ] }, \u0026#34;:val2\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;ACTIVE\u0026#34; } }\u0026#39; \\ --return-values ALL_NEW \\ --return-item-collection-metrics SIZE \\ --return-consumed-capacity TOTAL Followed by\naws dynamodb update-item \\ --table-name Orders \\ --key \u0026#39;{ \u0026#34;id\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;9953371\u0026#34;} }\u0026#39; \\ --update-expression \u0026#34;SET #items = :val1, #status = :val2\u0026#34; \\ --expression-attribute-names \u0026#39;{ \u0026#34;#items\u0026#34;: \u0026#34;items\u0026#34;, \u0026#34;#status\u0026#34;: \u0026#34;status\u0026#34; }\u0026#39; \\ --expression-attribute-values \u0026#39;{ \u0026#34;:val1\u0026#34;: { \u0026#34;L\u0026#34;: [ { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;23924636\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Protective Face Lotion\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£3.00\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;9\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;CANCELLED\u0026#34; } } }, { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;23514506\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Nail File\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£11.00\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;13\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;CANCELLED\u0026#34; } } }, { \u0026#34;M\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;23508704\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;Kitten Heels Powder Finish Foot Creme\u0026#34; }, \u0026#34;price\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;£11.00\u0026#34; }, \u0026#34;quantity\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;10\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;CANCELLED\u0026#34; } } } ] }, \u0026#34;:val2\u0026#34;: { \u0026#34;S\u0026#34;: \u0026#34;CANCELLED\u0026#34; } }\u0026#39; \\ --return-values ALL_NEW \\ --return-item-collection-metrics SIZE \\ --return-consumed-capacity TOTAL Explore the items on the Orders and OrdersHistory tables to see the result of your updates. The status for order ID 9953371 should be updated on the Orders table and there should be two items on the OrdersHistory table for order ID 9953371.\n\u0026hellip; and there should be two entries for order ID 9953371 on the OrdersHistory table showing the previous states of the order.\nThe order of updates to the Orders table is not preserved by Kinesis Data streams when changes are sent to the create order history lambda function. If you need to record the sequence that updates were made to items on the Orders table, you can configure the precision of the ApproximateCreationDateTime for your Kinesis Data stream. Once this is done, stream records written to your Kinesis Data stream will have an approximate creation date time attribute that can be used to record the sequence of updates to items on the Orders table. See How Kinesis Data Streams works with DynamoDB for more information on how it works.\n"
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.1/3.1.5/",
	"title": "Step 4 - Check the content of the workshop folder",
	"tags": [],
	"description": "",
	"content": "On the EC2 instance, go to the workshop folder and run the ls command:\ncd /home/ubuntu/workshop ls -l . The following list indicates the folder structure and the files that will be used during the workshop:\n. ├── data │ ├── employees.csv │ ├── invoice-data2.csv │ ├── invoice-data.csv │ ├── logfile_medium1.csv │ ├── logfile_medium2.csv │ ├── logfile_small1.csv │ └── logfile_stream.csv ├── ddbreplica_lambda.py ├── ddb-replication-role-arn.txt ├── gsi_city_dept.json ├── gsi_manager.json ├── iam-role-policy.json ├── iam-trust-relationship.json ├── lab_config.py ├── load_employees.py ├── load_invoice.py ├── load_logfile_parallel.py ├── load_logfile.py ├── query_city_dept.py ├── query_employees.py ├── query_index_invoiceandbilling.py ├── query_invoiceandbilling.py ├── query_responsecode.py ├── requirements.txt ├── scan_for_managers_gsi.py ├── scan_for_managers.py ├── scan_logfile_parallel.py └── scan_logfile_simple.py Python code:\nddbreplica_lambda.py load_employees.py load_invoice.py load_logfile_parallel.py load_logfile.py lab_config.py query_city_dept.py query_employees.py query_index_invoiceandbilling.py query_invoiceandbilling.py query_responsecode.py scan_for_managers_gsi.py scan_for_managers.py scan_logfile_parallel.py scan_logfile_simple.py JSON:\ngsi_city_dept.json gsi_manager.json iam-role-policy.json iam-trust-relationship.json Text (used later in the lab):\nddb-replication-role-arn.txt Run the ls command to show the sample data files:\nls -l ./data ./data contents:\nemployees.csv invoice-data2.csv invoice-data.csv logfile_medium1.csv logfile_medium2.csv logfile_small1.csv logfile_stream.csv The code provided is for instructional use only. It should not be used outside of this lab, and it is not fit for production use.\n"
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.2/3.2.5/",
	"title": "Step 5 - Increase the capacity of the table",
	"tags": [],
	"description": "",
	"content": "Run the following AWS CLI command to increase the write capacity units and read capacity units from 5 to 100.\naws dynamodb update-table --table-name logfile \\ --provisioned-throughput ReadCapacityUnits=100,WriteCapacityUnits=100 Run the command to wait until the table becomes Active.\ntime aws dynamodb wait table-exists --table-name logfile Topic for discussion: How long did it take to increase the capacity? Was it faster, or longer than you expected? Often, when you need more capacity from your provisioned throughput table it is available in only tens of seconds.\n"
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.9/3.9.5/",
	"title": "Step 5 - Map the source stream to the Lambda function",
	"tags": [],
	"description": "",
	"content": "So far, you have the source table with DynamoDB Streams enabled and the Lambda function. Now you need to map the source stream to the Lambda function. You need to copy the ARN from the previous step and paste it into the following command before running the command.\naws lambda create-event-source-mapping \\ --function-name ddbreplica_lambda --enabled --batch-size 100 --starting-position TRIM_HORIZON \\ --event-source-arn YOUR_STREAM_ARN_HERE You must copy the full stream label ARN, including the timestamp on the end\nExample:\naws lambda create-event-source-mapping \\ --function-name ddbreplica_lambda --enabled --batch-size 100 --starting-position TRIM_HORIZON \\ --event-source-arn arn:aws:dynamodb:\u0026lt;REGION\u0026gt;:\u0026lt;ACCOUNTID\u0026gt;:table/logfile/stream/2021-12-31T00:00:00.000 The following is the expected result.\n{ \u0026#34;UUID\u0026#34;: \u0026#34;0dcede66-709c-4073-a628-724d01b92095\u0026#34;, \u0026#34;BatchSize\u0026#34;: 100, \u0026#34;MaximumBatchingWindowInSeconds\u0026#34;: 0, \u0026#34;ParallelizationFactor\u0026#34;: 1, \u0026#34;EventSourceArn\u0026#34;: \u0026#34;arn:aws:dynamodb:\u0026lt;REGION\u0026gt;:\u0026lt;ACCOUNTID\u0026gt;:table/logfile/stream/2021-12-31T00:00:00.000\u0026#34;, \u0026#34;FunctionArn\u0026#34;: \u0026#34;arn:aws:lambda:\u0026lt;REGION\u0026gt;:\u0026lt;ACCOUNTID\u0026gt;:function:ddbreplica_lambda\u0026#34;, \u0026#34;LastModified\u0026#34;: 1663286115.972, \u0026#34;LastProcessingResult\u0026#34;: \u0026#34;No records processed\u0026#34;, \u0026#34;State\u0026#34;: \u0026#34;Creating\u0026#34;, \u0026#34;StateTransitionReason\u0026#34;: \u0026#34;User action\u0026#34;, \u0026#34;DestinationConfig\u0026#34;: { \u0026#34;OnFailure\u0026#34;: {} }, \u0026#34;MaximumRecordAgeInSeconds\u0026#34;: -1, \u0026#34;BisectBatchOnFunctionError\u0026#34;: false, \u0026#34;MaximumRetryAttempts\u0026#34;: -1 } "
},
{
	"uri": "http://Handoo464.github.io/4-lcdc/4.5/",
	"title": "Summary and Clean Up",
	"tags": [],
	"description": "",
	"content": "Congratulations! You have made it to the end of the workshop.\nIn this workshop you explored capturing item level changes on a DynamoDB table using DynamoDB Streams and Kinesis Data Streams. In this instance, you wrote the previous version of updated items to a different DynamoDB table. By applying these same techniques, you can build complex event driven solutions that are triggered by changes to items you have stored on DynamoDB.\nIf you used an account provided by Workshop Studio, you do not need to do any cleanup. The account terminates when the event is over.\nIf you used your own account, please remove the following resources:\nThe Lambda Function Event Source Mappings: UUID_1=`aws lambda list-event-source-mappings --function-name create-order-history-kds --query \u0026#39;EventSourceMappings[].UUID\u0026#39; --output text` UUID_2=`aws lambda list-event-source-mappings --function-name create-order-history-ddbs --query \u0026#39;EventSourceMappings[].UUID\u0026#39; --output text` aws lambda delete-event-source-mapping --uuid ${UUID_1} aws lambda delete-event-source-mapping --uuid ${UUID_2} The AWS Lambda functions created during the labs: aws lambda delete-function --function-name create-order-history-ddbs aws lambda delete-function --function-name create-order-history-kds The AWS Kinesis data stream created during the labs: aws kinesis delete-stream --stream-name Orders The Amazon DynamoDB tables created in the Getting Started section of the lab: aws dynamodb delete-table --table-name Orders aws dynamodb delete-table --table-name OrdersHistory The Amazon SQS queues created during the labs: aws sqs delete-queue --queue-url https://sqs.${REGION}.amazonaws.com/${ACCOUNT_ID}/orders-ddbs-dlq aws sqs delete-queue --queue-url https://sqs.${REGION}.amazonaws.com/${ACCOUNT_ID}/orders-kds-dlq The IAM policies attached to the IAM execution roles you created: The AWS IAM execution roles created for the lambda functions: This should wrap up the cleanup process.\n"
},
{
	"uri": "http://Handoo464.github.io/6-leda/6.5/",
	"title": "Summary: Conclusions",
	"tags": [],
	"description": "",
	"content": "Congratulations You finished both labs!\nClean up all resources If you\u0026rsquo;re running this lab as part of an AWS event on an account we provided, no further action is needed. If you\u0026rsquo;re running this lab on your own, make sure to clean up any resources that were created by deleting the CloudFormation stack.\nBlogs This workshop is based on a 2-part blog series on the AWS Database blog initially published in November 2021.\nPart 1: Build a near real-time data aggregation pipeline using a serverless, event-driven architecture Part 2: Build a fault-tolerant, serverless data aggregation pipeline with exactly-once processing Blog Source While this workshop is open-sourced, the blogs were released with a GitHub repository with much of the same code that you use in this workshop. The repo aws-serverless-realtime-aggregation is on GitHub.\nSolutions View Solutions\n"
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.4/1.4.6/",
	"title": "Cleaning Up The Resources",
	"tags": [],
	"description": "",
	"content": "Cleaning Up The Resources Step 1: Delete restored AWS resources Delete three DynamoDB restored tables using following command.\naws dynamodb delete-table \\ --table-name ProductCatalogODRestore aws dynamodb delete-table \\ --table-name ProductCatalogRestored aws dynamodb delete-table \\ --table-name ProductCatalogPITR Step 2: Delete the backup plan Follow these steps to delete a backup plan:\nIn the AWS Management Console, navigate to Services -\u0026gt; AWS Backup. In the navigation pane, choose Backup plans.On the Backup plans page, choose dbBackupPlan. This takes you to the details page. To delete the resource assignments for your plan, choose the radio button next to the dynamodbTable under Resource assignments, and then choose Delete. To delete the backup plan, choose Delete in the upper-right corner of the page. On the confirmation page, enter dbBackupPlan, and choose Delete plan. Step 3: Delete the recovery points On the AWS Backup console, in the navigation pane, choose Backup vaults.\nOn the Backup vaults page, choose the dynamodb-backup-vault. Check the recovery point and choose Delete.\nIf you are deleting more than one recovery point, follow these steps:\na. Review the list of recovery points that you are deleting.\nb. If you want to edit the list, choose Modify selection.\nc. If your list contains a continuous backup, choose whether to keep or delete your continuous backup data.\nd. To delete all the recovery points listed, enter delete, and then choose Delete recovery points.\nKeep your browser tab open until you see the green success banner at the top of the page.\nPrematurely closing this tab will end the deletion process and might leave behind some of the recovery points you wanted to delete.\nStep 4: Delete the backup vault Select the backup vault dynamodb-backup-vault and choose Delete. On the confirmation page, enter dynamodb-backup-vault, and choose Delete Backup vault. "
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.6/",
	"title": "Exercise 5: Sparse Global Secondary Indexes",
	"tags": [],
	"description": "",
	"content": "You can use a sparse global secondary index to locate table items that have an uncommon attribute. To do this, you take advantage of the fact that table items that do not contain global secondary index attribute(s) are not indexed at all.\nSuch a query for table items with an uncommon attribute can be efficient because the number of items in the index is significantly lower than the number of items in the table. In addition, the fewer table attributes you project into the index, the fewer write and read capacity units you consume from the index.\n"
},
{
	"uri": "http://Handoo464.github.io/6-leda/",
	"title": "LEDA: Build a Serverless Event Driven Architecture with DynamoDB",
	"tags": [],
	"description": "",
	"content": "In this workshop you will be presented with a serverless event-driven data aggregation pipeline. It\u0026rsquo;s built with AWS Lambda , Amazon DynamoDB , and Amazon Kinesis Data Streams .\nWhile you may be happy to partake in this workshop, be aware that the pipeline is broken and it needs your attention to get running! As you will soon discover, the workshop is designed with an input stream that contains duplicates and Lambda functions that randomly fail.\nOver the course of two labs you will have to first connect all the elements of the pipeline, and then update the Lambda functions to avoid message loss or duplication under (induced) random failures.\nHere\u0026rsquo;s what this workshop includes:\nStart here: Getting Started Overview Lab 1: Connect the pipeline Lab 2: Ensure fault tolerance and exactly once processing Summary: Conclusions Target audience The workshop is intended for anyone interested in understanding how to build serverless data processing pipelines. Basic understanding of AWS services and experience in Python programming is desirable but not required. We classify this as a 300-level workshop, which means you don\u0026rsquo;t need to be an expert on any of the three services that are the focus.\nRequirements Basic knowledge of AWS services Among other services this lab will guide you through the use of Amazon Kinesis Data Streams and AWS Lambda Basic understanding of DynamoDB If you\u0026rsquo;re not familiar with DynamoDB or are not participating in this lab as part of an AWS event, consider reviewing the documentation on \u0026ldquo;What is Amazon DynamoDB? \u0026quot; Familiarity with Python/Boto3 You will be copying as pasting code so you can focus on learning about DynamoDB. You will be able to review all code run during the exercises. Duration The workshop requires approximately 2 hours to complete.\nOutcomes Using practical examples, understand how to connect serverless AWS components into an event driven pipeline. Understand how to leverage special features of DynamoDB and Lambda to build a reliable data processing pipeline with exactly once processing semantics. Understand various error modes and retry mechanisms. "
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.5/1.5.6/",
	"title": "Load DynamoDB Table",
	"tags": [],
	"description": "",
	"content": "In this exercise, we will set up Database Migration Service (DMS) jobs to migrate data from source MySQL database (relational view, tables) to Amazon DynamoDB.\nVerify DMS creation Go to DMS Console and click on Replication Instances. You can able to see a replication instance with Class dms.c5.2xlarge in Available Status. Make sure the DMS instance is Available before you continue. If it is not Available, return to the CloudFormation console to review and troubleshoot the CloudFormation stack.\nCreate source and target endpoints Click on Endpoints and Create endpoint button Create the source endpoint. Use the following parameters to configure the endpoint:\nParameter Value Endpoint type Source endpoint Endpoint identifier mysql-endpoint Source engine MySQL Access to endpoint database Select the \u0026ldquo;Provide access information manually\u0026rdquo; radio button Server name From the EC2 dashboard , select MySQL-Instance and copy Public IPv4 DNS Port 3306 SSL mode none User name Value of DbMasterUsername added as parameter during Configure MySQL Environment Password Value of DbMasterPassword added as parameter during Configure MySQL Environment Open Test endpoint connection (optional) section, then in the VPC drop-down select DMS-VPC and click the Run test button to verify that your endpoint configuration is valid. The test will run for a minute and you should see a successful message in the Status column. Click on the Create endpoint button to create the endpoint. If you see a connection error, re-type the username and password to ensure no mistakes were made. Further, ensure you provided the IPv4 DNS name ending in amazonaws.com in the field Server name. Create the target endpoint. Repeat all steps to create the target endpoint with the following parameter values:\nParameter Value Endpoint type Target endpoint Endpoint identifier dynamodb-endpoint Target engine Amazon DynamoDB Service access role ARN CloudFormation template has created new role with full access to Amazon DynamoDB. Copy Role ARN from dynamodb-access role Open Test endpoint connection (optional) section, then in the VPC drop-down select DMS-VPC and click the Run test button to verify that your endpoint configuration is valid. The test will run for a minute and you should see a successful message in the Status column. Click on the Create endpoint button to create the endpoint.\nConfigure and Run a Replication Task Still in the AWS DMS console, go to Database migration tasks and click the Create Task button. We will create 3 replication jobs to migrate denormalized view, ratings (title_ratings) and regions/languages (title_akas) information.\nTask1: Enter the following parameter values in the Create database migration task screen: Parameter Value Task identified historical-migration01 Replication instance mysqltodynamodb-instance-* Source database endpoint mysql-endpoint Target database endpoint dynamodb-endpoint Migration type Migrate existing data Task settings: Editing mode Wizard Task settings: Target table preparation mode Do nothing Task settings: Turn on CloudWatch logs Checked Table mappings: Editing mode Select JSON editor option and follow the instructions after below screenshots Start with the JSON editor section open in your browser. In this section we will create Table mappings JSON document to replace what you see in the JSON editor. This document includes source to target mapping including any transformation on the records that will be performed during migration. To reduce the loading time during Immersion Day, we have narrowed down the migration list to selective movies. Below JSON document has list of 28 movies worked by Clint Eastwood. The remaining exercise will just focus on these movies. However, feel free to load remaining data in case you like to further explore. Some statistics around full dataset is give at the bottom of this chapter.\nCopy list of selective movies by Clint Eastwood.\n{ \u0026#34;filter-operator\u0026#34;: \u0026#34;eq\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;tt0309377\u0026#34; }, { \u0026#34;filter-operator\u0026#34;: \u0026#34;eq\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;tt12260846\u0026#34; }, { \u0026#34;filter-operator\u0026#34;: \u0026#34;eq\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;tt1212419\u0026#34; }, { \u0026#34;filter-operator\u0026#34;: \u0026#34;eq\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;tt1205489\u0026#34; }, { \u0026#34;filter-operator\u0026#34;: \u0026#34;eq\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;tt1057500\u0026#34; }, { \u0026#34;filter-operator\u0026#34;: \u0026#34;eq\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;tt0949815\u0026#34; }, { \u0026#34;filter-operator\u0026#34;: \u0026#34;eq\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;tt0824747\u0026#34; }, { \u0026#34;filter-operator\u0026#34;: \u0026#34;eq\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;tt0772168\u0026#34; }, { \u0026#34;filter-operator\u0026#34;: \u0026#34;eq\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;tt0498380\u0026#34; }, { \u0026#34;filter-operator\u0026#34;: \u0026#34;eq\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;tt0418689\u0026#34; }, { \u0026#34;filter-operator\u0026#34;: \u0026#34;eq\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;tt0405159\u0026#34; }, { \u0026#34;filter-operator\u0026#34;: \u0026#34;eq\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;tt0327056\u0026#34; }, { \u0026#34;filter-operator\u0026#34;: \u0026#34;eq\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;tt2310814\u0026#34; }, { \u0026#34;filter-operator\u0026#34;: \u0026#34;eq\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;tt2179136\u0026#34; }, { \u0026#34;filter-operator\u0026#34;: \u0026#34;eq\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;tt2083383\u0026#34; }, { \u0026#34;filter-operator\u0026#34;: \u0026#34;eq\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;tt1924245\u0026#34; }, { \u0026#34;filter-operator\u0026#34;: \u0026#34;eq\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;tt1912421\u0026#34; }, { \u0026#34;filter-operator\u0026#34;: \u0026#34;eq\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;tt1742044\u0026#34; }, { \u0026#34;filter-operator\u0026#34;: \u0026#34;eq\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;tt1616195\u0026#34; }, { \u0026#34;filter-operator\u0026#34;: \u0026#34;eq\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;tt6997426\u0026#34; }, { \u0026#34;filter-operator\u0026#34;: \u0026#34;eq\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;tt6802308\u0026#34; }, { \u0026#34;filter-operator\u0026#34;: \u0026#34;eq\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;tt3513548\u0026#34; }, { \u0026#34;filter-operator\u0026#34;: \u0026#34;eq\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;tt3263904\u0026#34; }, { \u0026#34;filter-operator\u0026#34;: \u0026#34;eq\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;tt3031654\u0026#34; }, { \u0026#34;filter-operator\u0026#34;: \u0026#34;eq\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;tt8884452\u0026#34; } Below JSON document will migrate denormalized view from imdb MySQL database (Task identified: historical-migration01). Replace the string “REPLACE THIS STRING BY MOVIES LIST” with list of movies copied earlier (Checkout following screenshot for any confusion). Then paste the resulting JSON code in to the JSON editor, replacing the existing code.\n{ \u0026#34;rules\u0026#34;: [ { \u0026#34;rule-type\u0026#34;: \u0026#34;selection\u0026#34;, \u0026#34;rule-id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;rule-name\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;object-locator\u0026#34;: { \u0026#34;schema-name\u0026#34;: \u0026#34;imdb\u0026#34;, \u0026#34;table-name\u0026#34;: \u0026#34;movies\u0026#34;, \u0026#34;table-type\u0026#34;: \u0026#34;view\u0026#34; }, \u0026#34;rule-action\u0026#34;: \u0026#34;include\u0026#34;, \u0026#34;filters\u0026#34;: [ { \u0026#34;filter-type\u0026#34;: \u0026#34;source\u0026#34;, \u0026#34;column-name\u0026#34;: \u0026#34;tconst\u0026#34;, \u0026#34;filter-conditions\u0026#34;: [\u0026#34;REPLACE THIS STRING BY MOVIES LIST\u0026#34;] } ] }, { \u0026#34;rule-type\u0026#34;: \u0026#34;object-mapping\u0026#34;, \u0026#34;rule-id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;rule-name\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;rule-action\u0026#34;: \u0026#34;map-record-to-record\u0026#34;, \u0026#34;object-locator\u0026#34;: { \u0026#34;schema-name\u0026#34;: \u0026#34;imdb\u0026#34;, \u0026#34;table-name\u0026#34;: \u0026#34;movies\u0026#34;, \u0026#34;table-type\u0026#34;: \u0026#34;view\u0026#34; }, \u0026#34;target-table-name\u0026#34;: \u0026#34;movies\u0026#34;, \u0026#34;mapping-parameters\u0026#34;: { \u0026#34;partition-key-name\u0026#34;: \u0026#34;mpkey\u0026#34;, \u0026#34;sort-key-name\u0026#34;: \u0026#34;mskey\u0026#34;, \u0026#34;exclude-columns\u0026#34;: [], \u0026#34;attribute-mappings\u0026#34;: [ { \u0026#34;target-attribute-name\u0026#34;: \u0026#34;mpkey\u0026#34;, \u0026#34;attribute-type\u0026#34;: \u0026#34;scalar\u0026#34;, \u0026#34;attribute-sub-type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;${tconst}\u0026#34; }, { \u0026#34;target-attribute-name\u0026#34;: \u0026#34;mskey\u0026#34;, \u0026#34;attribute-type\u0026#34;: \u0026#34;scalar\u0026#34;, \u0026#34;attribute-sub-type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;DETL|${category}|${ordering}\u0026#34; } ] } } ] } Go to the bottom and click on Create task. At this point the task will be created and will automatically start loading selected movies from source to target DynamoDB table. You can move forward and create two more tasks with similar steps (historical-migration02 and historical-migration03). Use the same settings as above except the Table Mappings JSON document. For historical-migration02 and historical-migration03 tasks use the JSON document mentioned below.\nBelow JSON document will migrate title_akas table from imdb MySQL database (Task identified: historical-migration02) Replace the string \u0026ldquo;REPLACE THIS STRING BY MOVIES LIST\u0026rdquo; with list of movies copied earlier.\n{ \u0026#34;rules\u0026#34;: [ { \u0026#34;rule-type\u0026#34;: \u0026#34;selection\u0026#34;, \u0026#34;rule-id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;rule-name\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;object-locator\u0026#34;: { \u0026#34;schema-name\u0026#34;: \u0026#34;imdb\u0026#34;, \u0026#34;table-name\u0026#34;: \u0026#34;title_akas\u0026#34;, \u0026#34;table-type\u0026#34;: \u0026#34;table\u0026#34; }, \u0026#34;rule-action\u0026#34;: \u0026#34;include\u0026#34;, \u0026#34;filters\u0026#34;: [ { \u0026#34;filter-type\u0026#34;: \u0026#34;source\u0026#34;, \u0026#34;column-name\u0026#34;: \u0026#34;titleId\u0026#34;, \u0026#34;filter-conditions\u0026#34;: [\u0026#34;REPLACE THIS STRING BY MOVIES LIST\u0026#34;] } ] }, { \u0026#34;rule-type\u0026#34;: \u0026#34;object-mapping\u0026#34;, \u0026#34;rule-id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;rule-name\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;rule-action\u0026#34;: \u0026#34;map-record-to-record\u0026#34;, \u0026#34;object-locator\u0026#34;: { \u0026#34;schema-name\u0026#34;: \u0026#34;imdb\u0026#34;, \u0026#34;table-name\u0026#34;: \u0026#34;title_akas\u0026#34;, \u0026#34;table-type\u0026#34;: \u0026#34;table\u0026#34; }, \u0026#34;target-table-name\u0026#34;: \u0026#34;movies\u0026#34;, \u0026#34;mapping-parameters\u0026#34;: { \u0026#34;partition-key-name\u0026#34;: \u0026#34;mpkey\u0026#34;, \u0026#34;sort-key-name\u0026#34;: \u0026#34;mskey\u0026#34;, \u0026#34;exclude-columns\u0026#34;: [], \u0026#34;attribute-mappings\u0026#34;: [ { \u0026#34;target-attribute-name\u0026#34;: \u0026#34;mpkey\u0026#34;, \u0026#34;attribute-type\u0026#34;: \u0026#34;scalar\u0026#34;, \u0026#34;attribute-sub-type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;${titleId}\u0026#34; }, { \u0026#34;target-attribute-name\u0026#34;: \u0026#34;mskey\u0026#34;, \u0026#34;attribute-type\u0026#34;: \u0026#34;scalar\u0026#34;, \u0026#34;attribute-sub-type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;REGN|${region}\u0026#34; } ] } } ] } Below JSON document will migrate title_ratings table from imdb MySQL database (Task identified: historical-migration03) Replace the string \u0026ldquo;REPLACE THIS STRING BY MOVIES LIST\u0026rdquo; with list of movies copied earlier.\n{ \u0026#34;rules\u0026#34;: [ { \u0026#34;rule-type\u0026#34;: \u0026#34;selection\u0026#34;, \u0026#34;rule-id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;rule-name\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;object-locator\u0026#34;: { \u0026#34;schema-name\u0026#34;: \u0026#34;imdb\u0026#34;, \u0026#34;table-name\u0026#34;: \u0026#34;title_ratings\u0026#34;, \u0026#34;table-type\u0026#34;: \u0026#34;table\u0026#34; }, \u0026#34;rule-action\u0026#34;: \u0026#34;include\u0026#34;, \u0026#34;filters\u0026#34;: [ { \u0026#34;filter-type\u0026#34;: \u0026#34;source\u0026#34;, \u0026#34;column-name\u0026#34;: \u0026#34;tconst\u0026#34;, \u0026#34;filter-conditions\u0026#34;: [\u0026#34;REPLACE THIS STRING BY MOVIES LIST\u0026#34;] } ] }, { \u0026#34;rule-type\u0026#34;: \u0026#34;object-mapping\u0026#34;, \u0026#34;rule-id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;rule-name\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;rule-action\u0026#34;: \u0026#34;map-record-to-record\u0026#34;, \u0026#34;object-locator\u0026#34;: { \u0026#34;schema-name\u0026#34;: \u0026#34;imdb\u0026#34;, \u0026#34;table-name\u0026#34;: \u0026#34;title_ratings\u0026#34;, \u0026#34;table-type\u0026#34;: \u0026#34;table\u0026#34; }, \u0026#34;target-table-name\u0026#34;: \u0026#34;movies\u0026#34;, \u0026#34;mapping-parameters\u0026#34;: { \u0026#34;partition-key-name\u0026#34;: \u0026#34;mpkey\u0026#34;, \u0026#34;sort-key-name\u0026#34;: \u0026#34;mskey\u0026#34;, \u0026#34;exclude-columns\u0026#34;: [], \u0026#34;attribute-mappings\u0026#34;: [ { \u0026#34;target-attribute-name\u0026#34;: \u0026#34;mpkey\u0026#34;, \u0026#34;attribute-type\u0026#34;: \u0026#34;scalar\u0026#34;, \u0026#34;attribute-sub-type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;${tconst}\u0026#34; }, { \u0026#34;target-attribute-name\u0026#34;: \u0026#34;mskey\u0026#34;, \u0026#34;attribute-type\u0026#34;: \u0026#34;scalar\u0026#34;, \u0026#34;attribute-sub-type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;RTNG\u0026#34; } ] } } ] } Solutions If you are having trouble with making the JSON documents for the tasks, expand this section to get the solutions!\nFirst Task - historical-migration01 Second Task - historical-migration02 Third Task - historical-migration03 Monitor and the restart/resume the tasks The replication task for historical migration will start moving data from MySQL imdb.movies view, title_akas and title_ratings to DynamoDB table will start in a few minutes. If you are loading selective records based on the list above, it may take 5-10 minutes to complete all three tasks.\nIf you were to run this exercise again but do a full load, the load times would be as follows:\nhistorical-migration01 task will migrate 800K+ records and normally takes 2-3 Hrs. historical-migration02 task will migrate 747K+ records and normally takes 2-3 Hrs. historical-migration03 task will migrate 79K+ records and normally takes 10-15 Minutes. You can track the status of data loading under the Table statistics of the migration task. Once loading is in progress, feel free to move to the next section of the exercise. Make sure all tasks are running or complete before you continue. If a task says Ready, check its box and choose \u0026ldquo;Restart/Resume\u0026rdquo; under the Actions button to start the task.\n"
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.1/3.1.6/",
	"title": "Step 5 - Check the files format and content",
	"tags": [],
	"description": "",
	"content": "You will be working with different data contents during this lab:\nServer Logs data Employees data Invoices and Bills data The Server Logs file has the following structure:\nrequestid (number) host (string) date (string) hourofday (number) timezone (string) method (string) url (string) responsecode (number) bytessent (number) useragent (string) To view a sample record in the file, execute:\nhead -n1 ./data/logfile_small1.csv Sample log record:\n1,66.249.67.3,2017-07-20,20,GMT-0700,GET,\u0026#34;/gallery/main.php?g2_controller=exif.SwitchDetailMode\u0026amp;g2_mode=detailed\u0026amp;g2_return=%2Fgallery%2Fmain.php%3Fg2_itemId%3D15741\u0026amp;g2_returnName=photo\u0026#34;,302,5,\u0026#34;Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\u0026#34; The Employees data file has the following structure:\nemployeeid (number) name (string) title (string) dept (string) city (string) state (string) dob (string) hire-date (string) previous title (string) previous title end date (string) is a manager (string), 1 for manager employees, non-existent for others To view a sample record in the file, execute:\nhead -n1 ./data/employees.csv Sample employee record:\n1,Onfroi Greeno,Systems Administrator,Operation,Portland,OR,1992-03-31,2014-10-24,Application Support Analyst,2014-04-12 "
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.2/3.2.6/",
	"title": "Step 6 - After increasing the table’s capacity, load more data",
	"tags": [],
	"description": "",
	"content": "After you increased the table’s capacity, run the following Python script again to populate the table using the logfile_medium2.csv input data file with the same number of rows as when you ran this command previously. Notice that the execution of the command happens more quickly this time.\npython load_logfile.py logfile ./data/logfile_medium2.csv The output will look like this:\nrow: 100 in 0.9451174736022949 row: 200 in 0.8512668609619141 ... row: 1900 in 0.8499886989593506 row: 2000 in 0.8817043304443359 RowCount: 2000, Total seconds: 17.13607406616211 With the new capacity, the total load time is lower.\n"
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.9/3.9.6/",
	"title": "Step 6 - Populate the logfile table and verify replication to logfile_replica",
	"tags": [],
	"description": "",
	"content": "Run the following Python code to load more items into the logfile table. The rows will be copied to the DynamoDB stream, procecesed by the AWS Lambda function, and then writen into the logfile_replica table at the end.\npython load_logfile.py logfile ./data/logfile_stream.csv The output will look like the following.\nRowCount: 2000, Total seconds: 15.808809518814087 Verify replication You can scan the logfile_replica table to verify that the records have been replicated. It takes a few seconds, so you may need to repeat the following AWS CLI command until you get the records. Once again, use the up-arrow to repeat the previous command.\naws dynamodb scan --table-name \u0026#39;logfile_replica\u0026#39; --max-items 2 --output text You will see the first two items of the replica table as follows.\nNone 723 723 BYTESSENT 2969 DATE 2009-07-21 HOST 64.233.172.17 HOUROFDAY 8 METHOD GET REQUESTID 4666 RESPONSECODE 200 TIMEZONE GMT-0700 URL /gwidgets/alexa.xml USERAGENT Mozilla/5.0 (compatible) Feedfetcher-Google; (+http://www.google.com/feedfetcher.html) BYTESSENT 1160 DATE 2009-07-21 HOST 64.233.172.17 HOUROFDAY 6 METHOD GET REQUESTID 4119 RESPONSECODE 200 TIMEZONE GMT-0700 URL /gadgets/adpowers/AlexaRank/ALL_ALL.xml USERAGENT Mozilla/5.0 (compatible) Feedfetcher-Google; (+http://www.google.com/feedfetcher.html) NEXTTOKEN eyJFeGNsdXNpdmVTdGFydEtleSI6IG51bGwsICJib3RvX3RydW5jYXRlX2Ftb3VudCI6IDJ9 Note: Your log entries may differ. As long as you have two log entries, you\u0026rsquo;ve verified successful replication. If you don\u0026rsquo;t see any entries, rerun the load_logfile.py command because you might have run the inserts too soon after creating the Lambda function.\nCongratulations, you have successfully completed all the exercises in the workshop! If you ran the lab on your own AWS account, you should delete all the tables made during these exercises. If you are at an AWS event using the AWS Workshop platform (Workshop Studio), you do not need to delete your tables.\nDuring the course of the lab, you created DynamoDB tables that will incur a cost that could approach tens or hundreds of dollars per day. You must delete the DynamoDB tables using the DynamoDB console to clean up the lab. In addition, if you are not part of an AWS event or you are running this lab in your own account, make sure you delete the CloudFormation stack as soon as the lab is complete. If you\u0026rsquo;re using Workshop Studio Event Delivery, you don\u0026rsquo;t need to delete the CloudFormation stack.\n"
},
{
	"uri": "http://Handoo464.github.io/5-lmr/5.6/",
	"title": "Summary and Clean up",
	"tags": [],
	"description": "",
	"content": "We hope you enjoyed this workshop and learning about how to build and deploy global applications! Please feel free to send change requests to our GitHub issues .\nIf you are running this workshop as part of an AWS sponsored event, your temporary account will be destroyed at the end of the event and all the resources will be deleted, so there is nothing else for you to do.\nIf you are running this workshop in your own AWS account, you may delete the DynamoDBID CloudFormation Stack and the Chalice created components to avoid incurring any charges afterwards. The DynamoDB table was created in On Demand mode so there are no charges for provisioned capacity levels, only for actual usage. However there will be small charges for storage if the table is not cleaned up.\n"
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.2/1.2.6/",
	"title": "Transactions",
	"tags": [],
	"description": "",
	"content": "The DynamoDB TransactWriteItems API is a synchronous write operation that groups up to 100 action requests (subject to an aggregate 4MB size limit for the transaction). It is invoked using the transact-write-items CLI command .\nThese actions can target items in different tables, but not in different AWS accounts or Regions, and no two actions can target the same item. The actions are completed atomically so that either all of them succeed, or all of them fail. For a greater discussion on Isolation Levels for Transactions see the Developer Guide .\nYou\u0026rsquo;ll recall from previous modules that the sample data contains multiple related tables: Forum, Thread, and Reply. When a new Reply item is added, we also need to increase the Messages count in the related Forum item. This should be done in a transaction so that both changes succeed or both changes fail at the same time, and someone reading this data should see both changes or neither change at the same time.\nTransactions in DynamoDB respect the concept of idempotency. Idempotency gives you the ability to send the same transaction more than once, but DynamoDB will only execute that transaction once. This is especially useful when using APIs that aren\u0026rsquo;t themselves idempotent, like using UpdateItem to increment or decrement a number field for example. When executing a transaction you will specify a string to represent the ClientRequestToken (aka Idempotency Token). For more discussion of idempotency please see the Developer Guide That command in the CLI would be:\naws dynamodb transact-write-items --client-request-token TRANSACTION1 --transact-items \u0026#39;[ { \u0026#34;Put\u0026#34;: { \u0026#34;TableName\u0026#34; : \u0026#34;Reply\u0026#34;, \u0026#34;Item\u0026#34; : { \u0026#34;Id\u0026#34; : {\u0026#34;S\u0026#34;: \u0026#34;Amazon DynamoDB#DynamoDB Thread 2\u0026#34;}, \u0026#34;ReplyDateTime\u0026#34; : {\u0026#34;S\u0026#34;: \u0026#34;2021-04-27T17:47:30Z\u0026#34;}, \u0026#34;Message\u0026#34; : {\u0026#34;S\u0026#34;: \u0026#34;DynamoDB Thread 2 Reply 3 text\u0026#34;}, \u0026#34;PostedBy\u0026#34; : {\u0026#34;S\u0026#34;: \u0026#34;User C\u0026#34;} } } }, { \u0026#34;Update\u0026#34;: { \u0026#34;TableName\u0026#34; : \u0026#34;Forum\u0026#34;, \u0026#34;Key\u0026#34; : {\u0026#34;Name\u0026#34; : {\u0026#34;S\u0026#34;: \u0026#34;Amazon DynamoDB\u0026#34;}}, \u0026#34;UpdateExpression\u0026#34;: \u0026#34;ADD Messages :inc\u0026#34;, \u0026#34;ExpressionAttributeValues\u0026#34; : { \u0026#34;:inc\u0026#34;: {\u0026#34;N\u0026#34; : \u0026#34;1\u0026#34;} } } } ]\u0026#39; Look at the Forum item and you\u0026rsquo;ll see that the Messages count was incremented by 1, from 4 to 5.\naws dynamodb get-item \\ --table-name Forum \\ --key \u0026#39;{\u0026#34;Name\u0026#34; : {\u0026#34;S\u0026#34;: \u0026#34;Amazon DynamoDB\u0026#34;}}\u0026#39; ... \u0026#34;Messages\u0026#34;: { \u0026#34;N\u0026#34;: \u0026#34;5\u0026#34; } ... If you run the same transaction command again, with the same client-request-token value, you can verify that the other invocations of the transaction are essentially ignored and the Messages attributed remains at 5.\nNow we need to do another transaction to reverse the above operation and clean up the table:\naws dynamodb transact-write-items --client-request-token TRANSACTION2 --transact-items \u0026#39;[ { \u0026#34;Delete\u0026#34;: { \u0026#34;TableName\u0026#34; : \u0026#34;Reply\u0026#34;, \u0026#34;Key\u0026#34; : { \u0026#34;Id\u0026#34; : {\u0026#34;S\u0026#34;: \u0026#34;Amazon DynamoDB#DynamoDB Thread 2\u0026#34;}, \u0026#34;ReplyDateTime\u0026#34; : {\u0026#34;S\u0026#34;: \u0026#34;2021-04-27T17:47:30Z\u0026#34;} } } }, { \u0026#34;Update\u0026#34;: { \u0026#34;TableName\u0026#34; : \u0026#34;Forum\u0026#34;, \u0026#34;Key\u0026#34; : {\u0026#34;Name\u0026#34; : {\u0026#34;S\u0026#34;: \u0026#34;Amazon DynamoDB\u0026#34;}}, \u0026#34;UpdateExpression\u0026#34;: \u0026#34;ADD Messages :inc\u0026#34;, \u0026#34;ExpressionAttributeValues\u0026#34; : { \u0026#34;:inc\u0026#34;: {\u0026#34;N\u0026#34; : \u0026#34;-1\u0026#34;} } } } ]\u0026#39; "
},
{
	"uri": "http://Handoo464.github.io/7-lgme/7.6/",
	"title": "View past games",
	"tags": [],
	"description": "",
	"content": "In this module, you handle the final access pattern — find all past games for a user. Users in the application might want to view games they’ve played to watch replays, or they might want to view their friends’ games.\nInverted index pattern You might recall that there is a many-to-many relationship between the Game entity and the associated User entities, and the relationship is represented by a UserGameMapping entity.\nOften, you want to query both sides of a relationship. With the primary key setup, you can find all the User entities in a Game. You can enable querying all Game entities for a User by using an inverted index.\nIn DynamoDB, an inverted index is a global secondary index (GSI) that is the inverse of your primary key. The sort key becomes your partition key and vice versa. This pattern flips your table and allows you to query on the other side of your many-to-many relationships.\nIn the following steps, you add an inverted index to the table and see how to use it to retrieve all Game entities for a specific User.\n"
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.5/1.5.7/",
	"title": "Access DynamoDB Table",
	"tags": [],
	"description": "",
	"content": "Amazon DynamoDB supports PartiQL , a SQL-compatible query language, to select, insert, update, and delete data in Amazon DynamoDB. Using PartiQL, you can easily interact with DynamoDB tables and run ad hoc queries using the AWS Management Console. In this exercise, we will hands-on a few access patterns using PartiQL statements.\nLogin to DynamoDB console and select PartiQL editor from left navigation. Select movies table that was created and loaded by the Database Migration Service job. Select ellipsis next to the table name and click on the scan table. We will use PartiQL scripts to demonstrate all 6 access patterns discussed at previous chapter. For our example we will provide you the partition key values, but in real life you will need to make an index of keys perhaps using a GSI. Get details by the movie: Each IMDB movie has a unique tconst. The denormalized table is created with each row representing a unique combination of movie and crew i.e. tconst and nconst. Since tconst is part of the partition key for the base table, it can use under WHERE conditions to select the details. Copy below command to run inside PartiQL query editor. Find all the cast and crew worked in a movie. Below query will include actor, actress, producer, cinematographer etc. worked in a given movie. SELECT * FROM \u0026#34;movies\u0026#34; WHERE \u0026#34;mpkey\u0026#34; = \u0026#39;tt0309377\u0026#39; and begins_with(\u0026#34;mskey\u0026#34;,\u0026#39;DETL|\u0026#39;) Find only actors worked in a movie. SELECT * FROM \u0026#34;movies\u0026#34; WHERE \u0026#34;mpkey\u0026#34; = \u0026#39;tt0309377\u0026#39; and begins_with(\u0026#34;mskey\u0026#34;,\u0026#39;DETL|actor\u0026#39;) Find only details of a movie. SELECT * FROM \u0026#34;movies\u0026#34; WHERE \u0026#34;mpkey\u0026#34; = \u0026#39;tt0309377\u0026#39; and begins_with(\u0026#34;mskey\u0026#34;,\u0026#39;DETL|\u0026#39;) and \u0026#34;ordering\u0026#34; = \u0026#39;1\u0026#39; Find all the regions, languages and title for a movie. SELECT * FROM \u0026#34;movies\u0026#34; WHERE \u0026#34;mpkey\u0026#34; = \u0026#39;tt0309377\u0026#39; and begins_with(\u0026#34;mskey\u0026#34;,\u0026#39;REGN|\u0026#39;) Find movie title for a specific region of a movie. SELECT * FROM \u0026#34;movies\u0026#34; WHERE \u0026#34;mpkey\u0026#34; = \u0026#39;tt0309377\u0026#39; and begins_with(\u0026#34;mskey\u0026#34;,\u0026#39;REGN|NZ\u0026#39;) Find original title of a movie. SELECT * FROM \u0026#34;movies\u0026#34; WHERE \u0026#34;mpkey\u0026#34; = \u0026#39;tt0309377\u0026#39; and begins_with(\u0026#34;mskey\u0026#34;,\u0026#39;REGN|\u0026#39;) and \u0026#34;types\u0026#34; = \u0026#39;original\u0026#39; To access information at the crew member level (#6 in the access pattern), we need to create an additional Global Secondary Index (GSI) with a new partition key nconst (unique for crew member). This will allow querying on the new partition key for GSI vs scan on the base table.\nSelect the Tables from the left navigation, choose movies table and click on the Index tab. Click on Create Index and add the following details. Parameter Value Partition key nconst Data type String Sort key - optional startYear Data type String Attribute projections All Finally, click on Create Index. This may take an hour depending on the number of records in the base table. Once the GSI status columns change from Pending to Available, go back to the PartiQL editor to execute a query on GSI. Find all movies by a crew (as actor, director etc.) SELECT * FROM \u0026#34;movies\u0026#34;.\u0026#34;nconst-startYear-index\u0026#34; WHERE \u0026#34;nconst\u0026#34; = \u0026#39;nm0000142\u0026#39; Find all movies by a crew as actor since 2002 and order by year ascending SELECT * FROM \u0026#34;movies\u0026#34;.\u0026#34;nconst-startYear-index\u0026#34; WHERE \u0026#34;nconst\u0026#34; = \u0026#39;nm0000142\u0026#39; and \u0026#34;startYear\u0026#34; \u0026gt;= \u0026#39;2002\u0026#39; ORDER BY \u0026#34;startYear\u0026#34; Congratulations! you have completed the RDBMS migration exercise.\n"
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.7/",
	"title": "Exercise 6: Composite Keys",
	"tags": [],
	"description": "",
	"content": "Carefully choosing the sort key attribute is important because it can significantly improve the selectivity of the items retrieved by a query. Let\u0026rsquo;s say you need to create an application to query the employees by geographic location (state and city) and by department. You have the attributes: state, city, and dept. You can create a global secondary index that will combine these attributes to allow queries by location/dept. In DynamoDB you can query the items using a combination of the partition key and the sort key. In this case, your query criteria need to use more than two attributes, so you will create a composite-key structure that allows you to query with more than two attributes.\nPreviously (Exercise 4, Step 1) you ran commands to create the employees table and load it with sample records. One of the attributes in this data is called state, that stores two-letter state abbreviations for US states. In addition, the attribute value of state is prefixed with state# and stored under the attribute name GSI_3_PK. The script also created the attribute city_dept which represents a composite attribute using the city and dept attributes delimited by a # between the values. The attribute value uses the format city#dept (for example Seattle#Development). This attribute value is duplicated and stored under the GSI_3_SK key.\nGSI_3 Attribute Name (Type) Special Attribute? Attribute Use Case Sample Attribute Value GSI_3_PK (STRING) GSI_3 partition key The state of the employee state#WA GSI_3_SK (STRING) GSI_3 sort key The city and department of the employee, concatenated Seattle#Development Note: Though you’re making a new global secondary index for this query, you can still overload this global secondary index in the future. Global secondary index overloading gives you the flexibility to put different entity types in the same index (for example, employees and buildings). To support future growth, the GSI_3 partition key is suffixed with the entity type, which allows you to insert rows in the same global secondary index later without comingling data.\n"
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.8/",
	"title": "Exercise 7: Adjacency Lists",
	"tags": [],
	"description": "",
	"content": "When different entities of an application have a many-to-many relationship between them, it is easier to model the relationship as an adjacency list. In this model, all top-level entities (synonymous with nodes in the graph model) are represented as the partition key. Any relationship with other entities (edges in a graph) are represented as an item within the partition by setting the value of the sort key to the target entity ID (target node).\nThis example uses an InvoiceAndBills table to demonstrate this design pattern. In this scenario, a customer can have multiple invoices, so there is a 1-to-many relationship between a customer ID and an invoice ID. An invoice contains many bills, and a bill can be broken up and associated with multiple invoices. So there is a many-to-many relationship between invoice ID and bill ID. The partition key attribute is either an invoice ID, bill ID, or customer ID.\nYou will model the table to execute the following queries:\nUsing the invoice ID, retrieve the top-level invoice details, customer, and associated bill details. Retrieve all invoice IDs for a customer. Using the bill ID, retrieve the top-level bill details and the associated invoice details. Table: InvoiceAndBills Key schema: HASH, RANGE (partition and sort key) Table read capacity units (RCUs) = 100 Table write capacity units (WCUS) = 100 Global secondary index (GSI): GSI_1 (100 RCUs, 100 WCUs) - Allows for reverse lookup to the related entity. Attribute Name (Type) Special Attribute? Attribute Use Case Sample Attribute Value PK (STRING) Partition key Holds the ID of the entity, either a bill, invoice, or customer B#3392 or I#506 or C#1317 SK (STRING) Sort key, GSI_1 partition key Holds the related ID: either a bill, invoice, or customer B#1721 or C#506 or I#1317 "
},
{
	"uri": "http://Handoo464.github.io/1-lhol/1.2/1.2.7/",
	"title": "Global Secondary Indexes",
	"tags": [],
	"description": "",
	"content": "We have concerned ourself hitherto with accessing data based on the key attributes. If we wanted to look for items based on non-key attributes we had to do a full table scan and use filter conditions to find what we wanted, which would be both very slow and very expensive for systems operating at large scale.\nDynamoDB provides a feature called Global Secondary Indexes (GSIs) which will automatically pivot your data around different Partition and Sort Keys. Data can be re-grouped and re-sorted to allow for more access patterns to be quickly served with the Query and Scan APIs.\nRemember the previous example where we wanted to find all the replies in the Reply table that were posted by User A:\naws dynamodb scan \\ --table-name Reply \\ --filter-expression \u0026#39;PostedBy = :user\u0026#39; \\ --expression-attribute-values \u0026#39;{ \u0026#34;:user\u0026#34; : {\u0026#34;S\u0026#34;: \u0026#34;User A\u0026#34;} }\u0026#39; \\ --return-consumed-capacity TOTAL When running that scan operation we could see that the Count returned was different than the ScannedCount. If there had been a billion Reply items but only three of them were posted by User A, we would have to pay (both in time and money) to scan through a billion items just to find the three we wanted.\nArmed with this knowledge of GSIs, we can now create a GSI on the Reply table to service this new access pattern. GSIs can be created and removed at any time, even if the table has data in it already! This new GSI will use the PostedBy attribute as the Partition (HASH) key and we will still keep the messages sorted by ReplyDateTime as the Sort (RANGE) key. We want all the attributes from the table copied (projected) into the GSI so we will use the ALL ProjectionType. Note that the name of the index we create is PostedBy-ReplyDateTime-gsi.\naws dynamodb update-table \\ --table-name Reply \\ --attribute-definitions AttributeName=PostedBy,AttributeType=S AttributeName=ReplyDateTime,AttributeType=S \\ --global-secondary-index-updates \u0026#39;[{ \u0026#34;Create\u0026#34;:{ \u0026#34;IndexName\u0026#34;: \u0026#34;PostedBy-ReplyDateTime-gsi\u0026#34;, \u0026#34;KeySchema\u0026#34;: [ { \u0026#34;AttributeName\u0026#34; : \u0026#34;PostedBy\u0026#34;, \u0026#34;KeyType\u0026#34;: \u0026#34;HASH\u0026#34; }, { \u0026#34;AttributeName\u0026#34; : \u0026#34;ReplyDateTime\u0026#34;, \u0026#34;KeyType\u0026#34; : \u0026#34;RANGE\u0026#34; } ], \u0026#34;ProvisionedThroughput\u0026#34;: { \u0026#34;ReadCapacityUnits\u0026#34;: 5, \u0026#34;WriteCapacityUnits\u0026#34;: 5 }, \u0026#34;Projection\u0026#34;: { \u0026#34;ProjectionType\u0026#34;: \u0026#34;ALL\u0026#34; } } } ]\u0026#39; It can take a little time while DynamoDB creates the GSI and backfills data from the table into the index. We can watch this from the command line and wait until the IndexStatus goes ACTIVE:\n#Get initial status aws dynamodb describe-table --table-name Reply --query \u0026#34;Table.GlobalSecondaryIndexes[0].IndexStatus\u0026#34; #Watch the status with the wait command (use Ctrl+C to exit): watch -n 5 \u0026#34;aws dynamodb describe-table --table-name Reply --query \u0026#34;Table.GlobalSecondaryIndexes[0].IndexStatus\u0026#34;\u0026#34; Once the GSI has become ACTIVE, continue on to the exercise below. Use Ctrl+C to exit the watch command.\n"
},
{
	"uri": "http://Handoo464.github.io/7-lgme/",
	"title": "LGME: Modeling Game Player Data with Amazon DynamoDB",
	"tags": [],
	"description": "",
	"content": "In this workshop, you will learn advanced data modeling patterns in Amazon DynamoDB . When using DynamoDB, it is important to consider how you will access your data (your access patterns) before you model your data. You will go through an example multiplayer gaming application, learn about the access patterns in the gaming application, and see how to design a DynamoDB table to handle the access patterns by using secondary indexes and transactions.\nRead up more on how DynamoDB is used by existing customers in GameTech particularly https://aws.amazon.com/dynamodb/gaming/ Here\u0026rsquo;s what this workshop includes:\nGetting Started Plan your data model Core usage: user profiles and games Find open games Join and close games View past games Summary \u0026amp; Cleanup Target audience This workshop is designed for developers, engineers, and database administrators who are involved in designing and maintaining DynamoDB applications.\n"
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.1/3.1.7/",
	"title": "Step 6 - Preload the items for the table Scan exercise",
	"tags": [],
	"description": "",
	"content": " Reminder: All commands are executed in the shell console connected to the EC2 instance, not your local machine. (If you are not sure you can always validate going back to step 1)\nIn this step, let\u0026rsquo;s populate the table with 1 million items in preparation for that exercise.\nRun the command to create a new table:\naws dynamodb create-table --table-name logfile_scan \\ --attribute-definitions AttributeName=PK,AttributeType=S AttributeName=GSI_1_PK,AttributeType=S AttributeName=GSI_1_SK,AttributeType=S \\ --key-schema AttributeName=PK,KeyType=HASH \\ --provisioned-throughput ReadCapacityUnits=5000,WriteCapacityUnits=5000 \\ --tags Key=workshop-design-patterns,Value=targeted-for-cleanup \\ --global-secondary-indexes \u0026#34;IndexName=GSI_1,\\ KeySchema=[{AttributeName=GSI_1_PK,KeyType=HASH},{AttributeName=GSI_1_SK,KeyType=RANGE}],\\ Projection={ProjectionType=KEYS_ONLY},\\ ProvisionedThroughput={ReadCapacityUnits=3000,WriteCapacityUnits=5000}\u0026#34; This command will create a new table and one GSI with the following definition:\nTable: logfile_scan Key schema: HASH Table RCU = 5000 Table WCU = 5000 GSI(s): GSI_1 (3000 RCU, 5000 WCU) - Allows for parallel or sequential scans of the access logs. Sorted by status code and timestamp. Attribute name (Type) Special attribute? Attribute use case Sample attribute value PK (STRING) Hash key Holds the request id for the access log request#104009 GSI_1_PK (STRING) GSI 1 hash key A shard key, with values 0-N, to allow log searches shard#3 GSI_1_SK (STRING) GSI 1 sort key Sorts the logs hierarchically, from status code -\u0026gt; date -\u0026gt; hour 200#2019-09-21#01 Run the command to wait until the table becomes Active:\naws dynamodb wait table-exists --table-name logfile_scan Populate the table Run the following command to load the server logs data into the logfile_scan table. It will load 1,000,000 rows to the table.\nnohup python load_logfile_parallel.py logfile_scan \u0026amp; disown nohup is used to run the process in the background, and disown allows the load to continue in case you are disconnected.\nThe following command will take about ten minutes to complete. It will run in the background.\nRun pgrep -l python to verify the script is loading data in the background.\npgrep -l python Output:\n3257 python The process id - the 4 digit number in the above example - will be different for everyone.\nThe script will continue to run in the background while you work on the next exercise.\nYou have completed the SETUP!\n"
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.2/3.2.7/",
	"title": "Step 7 - Create a new table with a low-capacity global secondary index",
	"tags": [],
	"description": "",
	"content": "Now, create a new table with different capacity units. The new table’s global secondary index has only 1 write capacity unit (WCU) and 1 read capacity unit (RCU).\nTo create the new table, run the following AWS CLI command.\naws dynamodb create-table --table-name logfile_gsi_low \\ --attribute-definitions AttributeName=PK,AttributeType=S AttributeName=GSI_1_PK,AttributeType=S \\ --key-schema AttributeName=PK,KeyType=HASH \\ --provisioned-throughput ReadCapacityUnits=1000,WriteCapacityUnits=1000 \\ --tags Key=workshop-design-patterns,Value=targeted-for-cleanup \\ --global-secondary-indexes \u0026#34;IndexName=GSI_1,\\ KeySchema=[{AttributeName=GSI_1_PK,KeyType=HASH}],\\ Projection={ProjectionType=INCLUDE,NonKeyAttributes=[\u0026#39;bytessent\u0026#39;]},\\ ProvisionedThroughput={ReadCapacityUnits=1,WriteCapacityUnits=1}\u0026#34; Run the following AWS CLI command to wait until the table becomes ACTIVE:\naws dynamodb wait table-exists --table-name logfile_gsi_low The initial command creates a new table and one global secondary index with the following definition:\nTable: logfile_gsi_low Key schema: HASH (partition key) Table read capacity units (RCUs) = 1000 Table write capacity units (WCUs) = 1000 Global secondary index: GSI_1 (1 RCU, 1 WCU) - Allows for querying by host IP address Attribute Name (Type) Special Attribute? Attribute Use Case Sample Attribute Value PK (STRING) Partition key Holds the request ID for the access log request#104009 GSI_1_PK (STRING) GSI 1 partition key The host for the request, an IPv4 address host#66.249.67.3 Let\u0026rsquo;s populate this table with a large dataset. You will use a multi-threaded version of the Python load script to simulate more writes per second to the DynamoDB table. This will create contention for provisioned capacity to simulate a surge of traffic on an under-provisioned table.\npython load_logfile_parallel.py logfile_gsi_low After a few minutes, the execution of this script will be throttled and show an error message similar to the following error. This indicates you should increase the provisioned capacity of the DynamoDB table, or enable DynamoDB auto scaling if you have not already (read more about DynamoDB auto scaling in the AWS documentation ).\nProvisionedThroughputExceededException: An error occurred (ProvisionedThroughputExceededException) when calling the BatchWriteItem operation (reached max retries: 9): The level of configured provisioned throughput for one or more global secondary indexes of the table was exceeded. Consider increasing your provisioning level for the under-provisioned global secondary indexes with the UpdateTable API You can pause the operation by typing Ctrl+Z (Ctrl+C if you are Mac user). This new table has more RCUs (1,000) and WCUs (1,000), but you still got an error and the load time increased.\nTopic for discussion: Can you explain the behavior of the test? An exception named ProvisionedThroughputExceededException was returned by DynamoDB with an exception message suggesting the provisioned capacity of the GSI be increased. This is a telling error, and one that needs to be acted upon. In short, if you want 100% of the writes on the DynamoDB base table to be copied into the GSI, then the GSI should be provisioned with 100% (the same amount) of the capacity on the base table, which should be 1,000 WCU in this example. Simply put, the GSI was under-provisioned.\nReview the table in the AWS Console Let\u0026rsquo;s review the Amazon CloudWatch metrics for this test in the AWS management console for Amazon DynamoDB. We will need to see which metrics were emitted to CloudWatch during this bout of write throttling.\nOpen the AWS console, or switch to your browser tab with the AWS console, to view the metrics for the logfile_gsi_low table. These are found under the DynamoDB section of the AWS management console in the tables view. If you don\u0026rsquo;t see the table, remember to click the refresh button on the top right of the DynamoDB console.\nThe following image shows the write capacity metric for the logfile_gsi_low table. Note that the consumed writes (the blue line) were lower than the provisioned writes (red line) for the table during the test. This tells us the base table had sufficient write capacity for the surge of requests.\nIt may take a few minutes for the provisioned capacity (red line) to show up in the graphs. The provisioned capacity metrics are synthetic and there can be delays of five to ten minutes until they show a change.\nThe following image shows the write capacity metric for the global secondary index. Note that the consumed writes (the blue line) were higher than the provisioned writes (red line) for the global secondary index during the test. This tells us the GSI was woefully under-provisioned for the requests it received.\nThe following image shows the throttled write requests for the logfile_gsi_low table. Note that the table has throttled write requests, even though the base table was provisioned with sufficient WCUs. Each throttled API request on DynamoDB generates one datapoint for the ThrottledRequests metric. In this picture, about 20 API requests were throttled by DynamoDB. However, the table has a GSI and we do not yet know if it, or the base table was the source of the throttle. We must continue investigating. To identify the source of these throttled write requests, review the throttled write events metric. If the DynamoDB base table is the throttle source, it will have WriteThrottleEvents. However, if the GSI has insufficient write capacity, it will have WriteThrottleEvents.\nWhen you review the throttle events for the GSI, you will see the source of our throttles! Only the GSI has \u0026lsquo;Throttled write events\u0026rsquo;, which means it is the source of throttling on the table, and the cause of the throttled Batch write requests.\nIt may take some time for the write throttle events to appear on the GSI throttled write events graph. If you don\u0026rsquo;t immediately see metrics, re-run the command above to load data into DynamoDB and let it continue for several minutes so that many throttling events are created.\nWhen a DynamoDB global secondary index\u0026rsquo;s write throttles are sufficient enough to create throttled requests, the behavior is called GSI back pressure. Throttled requests are ProvisionedThroughputExceededException errors in the AWS SDKs, generate ThrottledRequests metrics in CloudWatch, and appear as \u0026rsquo;throttled write requests\u0026rsquo; on the base table in the AWS console. When GSI back pressure occurs, all writes to the DynamoDB table are rejected until space in the buffer between the DynamoDB base table and GSI opens up. Regardless of whether a new row is destined for a GSI, writes for a time will be rejected on the base table until space is available - DynamoDB does not have time to determine if a row to be written will be in the GSI or not. This is a troubling situation, but it\u0026rsquo;s an unavoidable constraint from DynamoDB because the service cannot create a buffer of unlimited size between your base table and GSI; there must be a limit to the number of items waiting to be copied from the base table into a GSI. In order to be aware of this behavior early, it\u0026rsquo;s important to monitor throttled requests and events on your DynamoDB table and GSI.\nReview Remember that a DynamoDB table is provisioned separately from a global secondary index. If you underprovision a global secondary index, it might start to apply back pressure on your tables in the form of throttles. Back pressure is problematic because it will cause all write requests to the base table to be rejected until the buffer between the base table and GSI has enough space for new data. Remember to monitor the CloudWatch metrics on both your tables and global secondary indexes, and set monitoring alarms based on your business requirements.\n"
},
{
	"uri": "http://Handoo464.github.io/7-lgme/7.7/",
	"title": "Summary &amp; Cleanup",
	"tags": [],
	"description": "",
	"content": "In the previous modules, the following access patterns in the gaming application were handled:\nCreate user profile (Write) Update user profile (Write) Get user profile (Read) Create game (Write) Find open games (Read) View game (Read) Join game for a user (Write) Start game (Write) Update game for a user (Write) Update game (Write) Find games for user (Read) The strategies used to satisfy these patterns include:\nA single-table design that combines multiple entity types in one table. A composite primary key that allows for many-to-many relationships. A sparse global secondary index (GSI) to filter on one of the fields. DynamoDB transactions to handle complex write patterns across multiple entities. An inverted index (GSI) to allow reverse lookups on the many-to-many entity. Congratulations! You made it to the end of this workshop. Please take a few moments to share your feedback with us using the link that you received from the lab facilitator.\nCleanup If you were running this lab in your own AWS Account (not an AWS run event), don\u0026rsquo;t forget to cleanup the resources, by deleting the CloudFormation stack or the resources themselves (incase of no CloudFormation stack) you used during setup.\nIf following the lab in your own AWS Account, you will create DynamoDB tables that will incur a cost that could approach tens or hundreds of dollars per day. Ensure you delete the DynamoDB tables using the DynamoDB console, and make sure you delete the Cloud9 environment as soon as the lab is complete.\n"
},
{
	"uri": "http://Handoo464.github.io/8-ldc/",
	"title": "LDC: Design Challenges",
	"tags": [],
	"description": "",
	"content": "Design Challenges This is a collection of data model design challenge scenarios to help you understand the decisions you make when building efficient data models. While not required for this section, the NoSQL Workbench for Amazon DynamoDB , is an excellent tool to help build, visualize, and manipulate data models for DynamoDB.\n"
},
{
	"uri": "http://Handoo464.github.io/3-ladv/3.9/",
	"title": "Exercise 8: Amazon DynamoDB Streams and AWS Lambda",
	"tags": [],
	"description": "",
	"content": "The combination of DynamoDB Streams (documentation explaining the service ) with AWS Lambda enables many powerful design patterns. In this exercise, you replicate items from one DynamoDB table to another table by using DynamoDB Streams and Lambda functions.\nDynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near-real time. DynamoDB Streams provides for the following use cases:\nA global multi-player game has a multi-leader database topology, storing data in multiple AWS Regions. Each region stays in sync by consuming and replaying the changes that occur in the remote Regions. (In fact, DynamoDB Global Tables relies on DynamoDB Streams for global replication.) A new customer adds data to a DynamoDB table. This event invokes an AWS Lambda function to copy the data to a separate DynamoDB table for long term retention (This is very similar to this exercise, where we copy data from one DynamoDB table to another using DynamoDB Streams.) You will reuse the logfile table that you created in Exercise 1. You will enable DynamoDB Streams on the logfile table. Whenever a change is made to the logfile table, this change appears immediately in a stream. Next, you attach a Lambda function to the stream. The purpose of the Lambda function is to query DynamoDB Streams for updates to the logfile table and write the updates to a newly created table named logfile_replica. The following diagram shows an overview of this implementation.\n"
},
{
	"uri": "http://Handoo464.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://Handoo464.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]